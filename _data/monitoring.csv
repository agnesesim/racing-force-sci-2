patent_id,filing_date,publication_date,priority_date,assignee,inventors,docdb_family_id,tags,title,abstract,claims,filename,ipc_classes,ipc_classes_list,ipc_classes_list_,title_abstract,ipc_classes_string
US10275662_B1,2016-09-30,2019-04-30,2016-09-30,ZOOX,"ASKELAND, JACOB LEE",66248243,road monitoring,estimating friction based on image data,"A friction estimation system for estimating friction-related data associated with a surface on which a vehicle travels, may include a camera array including a plurality of imagers configured to capture image data associated with a surface on which a vehicle travels. The image data may include light data associated with the surface. The friction estimation system may also include an image interpreter in communication with the camera array and configured to receive the image data from the camera array and determine friction-related data associated with the surface based, at least in part, on the image data. The image interpreter may be configured to be in communication with a vehicle control system and provide the friction-related data to the vehicle control system.","1. A friction estimation system for estimating friction-related data associated with a surface on which a vehicle travels, the friction estimation system comprising: a camera array comprising a plurality of imagers configured to capture image data associated with a surface on which a vehicle travels; an image interpreter in communication with the camera array and configured to receive the image data from the camera array and determine friction-related data associated with the surface based, at least in part, on the image data, the image interpreter being configured to be in communication with a vehicle control system and provide the friction-related data to the vehicle control system; a location sensor, an output of which comprises a location of the vehicle; and one or more processors configured to associate, based at least in part on the location, the friction-related data with a map accessible to the vehicle.2. The friction estimation system of claim 1, wherein the image data comprises at least one of image depth profile data associated with the surface or texture data associated with the surface.3. The friction estimation system of claim 1, wherein the friction-related data comprises a friction coefficient between the surface and one or more tires of the vehicle.4. The friction estimation system of claim 1, wherein the one or more processors are configured to provide a correlation between the image data and the friction-related data, and wherein the correlation between image data and friction-related data comprises at least one of a correlation table correlating image data and friction-related data, or a mathematical relationship between image data and friction-related data.5. The friction estimation system of claim 1, wherein the one or more processors are configured to provide a correlation between the image data and the friction-related data, and wherein the one or more processors are configured to update the correlation between the image data and the friction-related data based on measured correlations between the image data and sensed traction events between the vehicle tires and surfaces on which the vehicle has travelled.6. The friction estimation system of claim 1, wherein the one or more processors are configured to provide a correlation between the image data and the friction-related data, and wherein the correlation comprises a correlation between the image data and a material from which the surface is made, and a correlation between the material and the friction-related data.7. The friction estimation system of claim 1, wherein the one or more processors are configured to provide a correlation between the image data and the friction-related data, and wherein the one or more processors are configured to receive the location of the vehicle and update the correlation between the image data and the friction-related data based, at least in part, on the location of the vehicle.8. The friction estimation system of claim 1, further comprising a transmitter in communication with a network and configured to communicate data correlating the location of the vehicle and the friction-related data via the network.9. The friction estimation system of claim 1, further comprising a receiver in communication with a network and configured to receive updates relating to a correlation between the image data and friction-related data.10. A vehicle control system for controlling a vehicle, the vehicle control system comprising: a friction estimation system for estimating friction-related data associated with a surface on which a vehicle travels, the friction estimation system comprising: a camera array configured to capture image data associated with the surface on which the vehicle travels; and an image interpreter in communication with the camera array and configured to receive the image data from the camera array and to determine friction-related data associated with the surface based, at least in part, on the image data; a transmitter in communication with a network; and a drive system in communication with the friction estimation system and configured to control maneuvering of the vehicle, the drive system comprising at least one of: a vehicle propulsion system configured to propel the vehicle on the surface, a vehicle steering system configured to alter a direction of travel of the vehicle, or a vehicle braking system configured to reduce a speed of travel of the vehicle, wherein the drive system is configured to receive the friction-related data from one or more of the friction estimation system or the network and control maneuvering of the vehicle based, at least in part, on the friction-related data.11. The vehicle control system of claim 10, wherein the image data comprises at least one of image depth profile data associated with the surface or texture data associated with the surface.12. The vehicle control system of claim 10, wherein the friction-related data comprises a friction coefficient between the surface and one or more tires of the vehicle.13. The vehicle control system of claim 10, wherein the drive system comprises a vehicle propulsion system configured to propel the vehicle on the surface, and wherein the drive system is configured to receive the friction-related data from the friction estimation system and alter propulsion of the vehicle based, at least in part, on the friction-related data.14. The vehicle control system of claim 10, wherein the drive system comprises a vehicle steering system configured to alter a direction of travel of the vehicle, and wherein the drive system is configured to receive the friction-related data from the friction estimation system and alter a steering angle of the steering system based, at least in part, on the friction-related data.15. The vehicle control system of claim 10, wherein the drive system comprises a vehicle braking system configured to reduce a speed of travel of the vehicle, and wherein the drive system is configured to receive the friction-related data from the friction estimation system and alter a rate of deceleration of the vehicle based, at least in part, on the friction-related data.16. The vehicle control system of claim 10, wherein the drive system further comprises a ride control system configured to control compliance of suspension of the vehicle, and wherein the ride control system is configured to control the compliance based, at least in part, on at least one of the image data or friction-related data.17. The vehicle control system of claim 10, wherein the friction estimation system further comprises one or more processors in communication with the image interpreter and configured to provide a correlation between image data and friction-related data.18. The vehicle control system of claim 17, wherein the drive system further comprises at least one of: a traction control system configured to alter output of the propulsion system based, at least in part, on sensing slip between one or more tires of the vehicle and the surface; a stability control system configured to selectively control rotation of one or more tires of the vehicle based, at least in part, on sensing slip between one or more of the tires during turning of the vehicle; or an anti-lock braking control system configured to prevent one or more tires of the vehicle from ceasing rotation during braking of the vehicle, wherein the one or more processors are configured to: update the correlation between the image data and the friction-related data based on correlations between the image data and traction events sensed by at least one of the traction control system, the stability control system, or the anti-lock braking control system; and transmit, via the network, the updated correlation data to one or more of a second vehicle or a server.19. The vehicle control system of claim 10, further comprising a safety system configured to protect at least one of objects outside the vehicle from collisions with the vehicle, or occupants inside the vehicle during collisions, and wherein the vehicle control system is configured to alter operation of the safety system based, at least in part, on the friction-related data.20. A method for estimating friction-related data associated with a surface on which a vehicle travels, the method comprising: obtaining image data associated with the surface via a camera array comprising a plurality of imagers; correlating the image data with material data associated with a material of the surface; correlating the material data with friction-related data; estimating the friction-related data based, at least in part, on the correlation between the material data and the friction-related data; and transmitting, via a network, data correlating a location of the vehicle and the friction-related data to one or more of a second vehicle or a server.21. The method of claim 20, wherein the image data comprises at least one of image depth profile data associated with the surface or texture data associated with the surface, and wherein the method further comprises estimating the friction-related data by at least one of analyzing the image depth profile data using machine learning procedures or analyzing the texture data using multi-channel filtering techniques.22. The method of claim 20, wherein the friction-related data comprises a friction coefficient between the surface and one or more tires of the vehicle.23. The method of claim 20, wherein correlating the image data with the material data associated with the material of the surface comprises obtaining the correlation from at least one of a correlation table correlating the image data and the material data, or a mathematical relationship between the image data and the material data.24. The method of claim 20, further comprising updating the correlation between the image data and the friction-related data based on measured correlations between the image data and sensed traction events between the vehicle tires and surfaces on which the vehicle has travelled.25. The method of claim 20, further comprising updating the correlation between the image data and the friction-related data based, at least in part, on location data associated with a location of the vehicle.26. The method of claim 20, further comprising receiving updates relating to correlations between the image data and the friction-related data via the network.27. The method of claim 20, further comprising: receiving, via the network, second friction-related data associated with a second location; and controlling maneuvering of the vehicle at the second location based, at least in part, on the second friction-related data, wherein controlling maneuvering of the vehicle comprises at least one of: altering propulsion of the vehicle based, at least in part, on the second friction-related data, altering a steering angle of a steering system associated with the vehicle based, at least in part, on the second friction-related data, altering a rate of deceleration of the vehicle based, at least in part, on the second friction-related data, or altering operation of a safety system based, at least in part, on the second friction-related data, wherein the safety system is configured to protect at least one of objects outside the vehicle from collisions with the vehicle, or occupants inside the vehicle during collisions.28. The method of claim 20, further comprising updating the correlation between the image data and the friction-related data based on correlations between the image data and traction events sensed by at least one of a traction control system associated with the vehicle, a stability control system associated with the vehicle, or an anti-lock braking control system associated with the vehicle.",US10275662_B1.txt,"G06K9/00,G06K9/52,G06T7/00,G06T7/40,H04N5/247","{'electric communication technique', 'computing; calculating; counting'}","['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general', 'image data processing or generation, in general', 'pictorial communication, e.g. television']","estimating friction based on image data A friction estimation system for estimating friction-related data associated with a surface on which a vehicle travels, may include a camera array including a plurality of imagers configured to capture image data associated with a surface on which a vehicle travels. The image data may include light data associated with the surface. The friction estimation system may also include an image interpreter in communication with the camera array and configured to receive the image data from the camera array and determine friction-related data associated with the surface based, at least in part, on the image data. The image interpreter may be configured to be in communication with a vehicle control system and provide the friction-related data to the vehicle control system.",electric communication technique computing; calculating; counting
US10825183_B2,2018-08-21,2020-11-03,2017-09-26,BOE TECHNOLOGY GROUP COMPANY,"HAN YANGDU, HUIYANG QIJIN, HAILAN",61047740,road monitoring,analyzing and processing method and device for a road,"The present disclosure provides an analyzing and processing method and device for a road. In the analyzing and processing method for a road provided by the present disclosure, a slope value of a ground surface is obtained and image data of the ground surface is collected, and then the image data is analyzed and processed by using a pre-trained landform analysis model, so as to determine a landform type corresponding to the image data. A traveling condition is then determined based on the slope value of the ground surface and the landform type, and command information is generated according to the traveling conditions.","1. An analyzing and processing method for a road, comprising: obtaining a slope value of a ground surface and collecting image data of the ground surface; determining a landform type corresponding to the image data, wherein determining the landform type corresponding to the image data comprises analyzing and processing the image data by using a pre-trained landform analysis model, wherein training the pre-trained landform analysis model comprises: selecting a set of image data containing a landform type label as a training sample; using an edge-based image segmentation method to segment the image data in the training sample into sky and earth; using a region-based image segmentation method to divide the earth in the image data into a plurality of regions; extracting texture information of each of the plurality of regions to determine the landform type; and establishing a correspondence relationship between the image data and the landform type by machine learning; determining a traveling condition according to the slope value of the ground surface and the landform type, wherein the traveling condition comprises a traveling condition suitable for running a horse, a traveling condition suitable for riding the horse slowly, and a traveling condition suitable for dismounting from the horse and leading the horse; and generating command information according to the traveling condition, wherein the command information is configured to adjust a movement status of the horse.2. The method according to claim 1, wherein determining the traveling condition according to the slope value of the ground surface and the landform type comprises: determining a slope type of the ground surface according to the slope value, wherein the slope type comprises downhill, flat road and uphill; and determining the traveling condition according to the slope type and the landform type.3. The method according to claim 2, wherein the determining the slope type of the ground surface according to the slope value comprises: determining that the slope type of the ground surface is downhill, when the slope value is less than a first threshold slope; and determining that the slope type of the ground surface is the flat road, when the slope value is greater than or equal to the first threshold slope, and less than or equal to a second threshold slope; and determining that the slope type of the ground surface is uphill, when the slope value is greater than the second threshold slope.4. The method according to claim 2, wherein the determining the traveling condition according to the slope type and the landform type comprises: determining that the traveling condition is suitable for dismounting from the horse and leading the horse, when the slope type is downhill; determining that the traveling condition is suitable for riding the horse slowly, when the slope type is uphill; determining that the travelling condition is suitable for running the horse, when the slope type is the flat road and the landform type is grassland or sand land; and determining that the traveling condition is suitable for riding the horse slowly, when the slope type is the flat road, and the landform type is not grassland or sand land.5. The method according to claim 1, wherein generating the command information according to the traveling condition comprises: displaying an indicator lamp having a corresponding color according to the traveling condition, to prompt a rider to adjust the movement status of the horse; and issuing a standard voice command to the horse by a speaker, and applying an electric stimulus command to the horse to adjust the movement status of the horse, according to the traveling condition.6. The method according to claim 1, wherein generating the command information according to the traveling condition comprises: displaying an indicator lamp having a corresponding color according to the traveling condition, to prompt a rider to adjust the movement status of the horse; and applying an electric stimulus command to the horse to adjust the movement status of the horse, according to the traveling condition.7. The method according to claim 1, wherein generating the command information according to the traveling condition comprises: issuing a standard voice command to the horse by a speaker, and applying an electric stimulus command to the horse to adjust the movement status of the horse, according to the traveling condition.8. The method according to claim 1, wherein generating the command information according to the traveling condition comprises: applying an electric stimulus command to the horse to adjust the movement status of the horse, according to the traveling condition.9. The method according to claim 1, wherein before the command information is generated according to the traveling condition, the method further comprises: collecting a human command given by a rider to the horse, wherein the human command comprises a human voice command, and the human voice command comprises a standard voice command and a non-standard voice command; generating the command information according to the traveling condition comprises: when the human voice command is the non-standard voice command, and the human voice command is appropriate for the traveling condition, transforming the non-standard voice command into the standard voice command having the same meaning by using a pre-trained voice understanding model, and playing the standard voice command by a speaker; when the human voice command is the standard voice command, and the human voice command is not appropriate for the traveling condition, regenerating command information appropriate for the traveling condition according to the traveling condition.10. The method according to claim 1, wherein obtaining the slope value of the ground surface comprises: determining the slope value of the ground surface according to a distance between an image collector and the ground surface; or determining the slope value of the ground surface according to acceleration of the image collector in a direction perpendicular to the ground surface; wherein the image collector is provided on a helmet of a rider and is facing a moving direction.11. The method according to claim 1, wherein obtaining the slope value of the ground surface comprises: determining the slope value of the ground surface according to acceleration of a image collector in a direction perpendicular to the ground surface; wherein the image collector is provided on a helmet of a rider and is facing a moving direction.12. An analyzing and processing device for a road, applied to horse riding, the device being provided on a helmet of a rider, and comprising: a data obtaining module comprising an image collector and configured to obtain a slope value of a ground surface and collect image data of the ground surface; an analysis and processing module comprising a processor and configured to analyze and process the image data by using a pre-trained landform analysis model, to determine a landform type corresponding to the image data, wherein training the pre-trained landform analysis model comprises: selecting a set of image data containing a landform type label as a training sample; using an edge-based image segmentation method to segment the image data in the training sample into sky and earth; using a region-based image segmentation method to divide the earth in the image data into a plurality of regions; extracting texture information of each of the plurality of regions to determine the landform type; and establishing a correspondence relationship between the image data and the landform type by machine learning; a road condition recognition module comprising an information processor configured to determine a traveling condition according to the slope value of the ground surface and the landform type, wherein the traveling condition comprises a traveling condition suitable for running a horse, a traveling condition suitable for riding the horse slowly, and a traveling condition suitable for dismounting from the horse and leading the horse; and a main control module comprising a computer chip and configured to generate command information according to the traveling condition, wherein the command information is configured to adjust a movement status of the horse.13. An analyzing and processing system for a road, applied to horse riding, comprising a helmet, a speaker, and an electric stimulus device, wherein the helmet is worn on a head of a rider, and the speaker and the electric stimulus device are disposed on a body of a horse, the helmet comprises an image collector, a pickup, and an information processing device, the image collector is configured to obtain a slope value of a ground surface and collect image data of the ground surface, the pickup is configured to receive a human voice command given by the rider, the information processing device is configured to analyze and process the image data by using a pre-trained landform analysis model, to determine a landform type corresponding to the image data, and determine a traveling condition according to the slope value of the ground surface and the landform type, generate command information according to the traveling condition, wherein the command information comprises a standard voice command and an electric stimulus command for adjusting a movement status of the horse; the speaker is configured to perform the standard voice command on the horse; the electric stimulus device is configured to execute the electric stimulus command on horse; and the image collector, the pickup, the speaker, and the electric stimulus device are all in communication connection with the information processing device.",US10825183_B2.txt,"G06K9/00,G06T7/11,G06T7/12,G06T7/13,G06T7/149,G06T7/64",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general', 'image data processing or generation, in general', 'image data processing or generation, in general', 'image data processing or generation, in general', 'image data processing or generation, in general']","analyzing and processing method and device for a road The present disclosure provides an analyzing and processing method and device for a road. In the analyzing and processing method for a road provided by the present disclosure, a slope value of a ground surface is obtained and image data of the ground surface is collected, and then the image data is analyzed and processed by using a pre-trained landform analysis model, so as to determine a landform type corresponding to the image data. A traveling condition is then determined based on the slope value of the ground surface and the landform type, and command information is generated according to the traveling conditions.",computing; calculating; counting
WO2019043406_A1,2018-08-31,2019-03-07,2017-08-31,CALIPSA,"GAO, JIAMENGPLOIX, BORIS",60050552,anomaly detection,anomaly detection from video data from surveillance cameras,"The present invention relates to object orientated data analysis. More particularly, the present invention relates to analysis of objects within video data from surveillance cameras. According to a first aspect, there is provided a method of detecting anomalous behaviour, the method comprising the steps of: receiving a first set of input data, comprising one or more digital image frames; generating a statistical model based on the first set of input data, the statistical model operable to detect one or more objects within the first set of input data; analysing a second set of input data with respect to the statistical model; and detecting one or more objects within the second set of input data.","A method of detecting anomalous behaviour, the method comprising the steps of: receiving a first set of input data, comprising one or more digital image frames;generating a statistical model based on the first set of input data, the statistical model operable to detect one or more objects within the first set of input data;analysing a second set of input data with respect to the statistical model; and detecting one or more objects within the second set of input data.A method as claimed in claim 1 , wherein the first and/or second set of input data comprises one or more digital videos, formed from the one or more digital image frames.A method as claimed in any one of claims 1 or 2, wherein the one or more digital videos are recorded from one or more surveillance cameras.A method as claimed in any preceding claim, wherein the generation of the statistical model is performed using one or more of: Convolutional Neural Networks (CNNs); Deep Convolutional Networks; Recurrent Neural Networks; Reinforced Learning; Scale-Invariant Feature Transform (SIFT) features, and/or optical flow feature vectors.A method as claimed in any preceding claim, further comprising the steps of:analysing the first set of input data through one or more filters; andobtaining one or more filter outputs.A method as claimed in claim 5, wherein the generation of the statistical model comprises the use of the one or more filter outputs.A method as claimed in any one of claims 5 or 6, wherein the one or more filters comprise one or more of: Convolutional Neural Networks (CNNs); Deep Convolutional Networks; Recurrent Neural Networks; Reinforced Learning; ScaleInvariant Feature Transform (SIFT) features, and/or optical flow feature vectors. A method as claimed in any preceding claim, wherein the one or more objects comprise one or more of: vehicles; human beings; animals; plants; buildings; and/or weather formations.A method as claimed in any preceding claim, wherein the statistical model is operable to track one or more objects in the first and/or second set of input data.A method as claimed in any preceding claim, wherein the statistical model is operable detect anomalous objects in the first and/or second set of input data.A method as claimed in any preceding claim, wherein the analysis of the second set of input data is unsupervised.A method as claimed in any preceding claim, wherein the analysis of the second set of input data occurs in real time.An apparatus for detecting anomalous behaviour, comprising:means for receiving a first set of input data, comprising one or more digital image frames;means for generating a statistical model based on the first set of input data, the statistical model operable to detect one or more objects within the first set of input data;means for analysing a second set of input data with respect to the statistical model; andmeans for detecting one or more objects within the second set of input data.A system operable to perform the method of any one of claims 1 to 12.A computer program product operable to perform the method and/or apparatus and/or system of any preceding claim.",WO2019043406_A1.txt,G06K9/00,{'computing; calculating; counting'},['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers'],"anomaly detection from video data from surveillance cameras The present invention relates to object orientated data analysis. More particularly, the present invention relates to analysis of objects within video data from surveillance cameras. According to a first aspect, there is provided a method of detecting anomalous behaviour, the method comprising the steps of: receiving a first set of input data, comprising one or more digital image frames; generating a statistical model based on the first set of input data, the statistical model operable to detect one or more objects within the first set of input data; analysing a second set of input data with respect to the statistical model; and detecting one or more objects within the second set of input data.",computing; calculating; counting
US2009268947_A1,2009-03-31,2009-10-29,2008-03-31,"HARMAN BECKER AUTOMOTIVE SYSTEMSSCHAUFLER, ROLAND","SCHAUFLER, ROLAND",40225267,road monitoring,real time environment model generation system,"A vehicle environment monitoring system is provided that is based on a three-dimensional vector model. The three-dimensional vector model of the vehicle's environment is generated on the basis of the image data captured by at least one three-dimensional camera. Out of the image data, particular data are extracted for generating the three-dimensional vector model in order to reduce the data volume. For data extraction, a data extraction algorithm is applied that is determined in accordance with at least one parameter that relates to the situation of the vehicle. Therefore, targeted data extraction is performed for generating a three-dimensional model that is particularly adapted for an application that is desired in the current vehicle situation. The applications of the vector model include driver assistance, external monitoring and vehicle control, as well as recording in an event data recorder. In one implementation, a sequence of three-dimensional vector models, representing a three-dimensional space-and-time model, is generated.","1. A method of monitoring the environment of a camera-equipped vehicle, comprising the steps of: capturing a 3D-image of the environment of the vehicle, the 3D-image representing a predetermined area of the vehicle environment; reducing the amount of information acquired in said image capturing step by extracting data from said 3D-image employing a data extraction algorithm; and generating a 3D-vector model of the vehicle environment from said data extracted from said 3D-image characterized by the step of: determining the data extraction algorithm to be employed in said reducing step based on at least one parameter characterizing the vehicle situation.2. The method of claim 1, where said extraction algorithm determining step determines the data extraction algorithm to be adapted according to the processing time available for generating the 3D-vector model.3. The method of claim 1, where said extraction algorithm determining step determines the data extraction algorithm to be adapted according to information reflected in the 3D-vector model.4. The method of claim 1, where said data extraction algorithm includes object recognition.5. (canceled)6. The method of claim 1, where said data extraction algorithm extracts data only from a part of the 3D-image corresponding to a certain visual field.7. The method of claim 1, further comprising the step of further processing said 3D-vector model for assisting a driver in controlling the vehicle based on the processing result.8. The method of claim 7, where said extraction algorithm determining step includes the step of determining a maximum available amount of time for performing said steps of generating and further processing said 3D-vector model based on the vehicle situation and further takes into account said maximum available amount of time to determine said data extraction algorithm.9. (canceled)10. The method of claim 9, where said extraction algorithm determining step includes the step of determining a maximum amount of data to be extracted in correspondence with said maximum amount of time.11. The method of claim 10, where said extraction algorithm determining step further includes the step of determining priorities for different kinds of data that can be extracted from said 3D-image based on the particular relevance of different kinds of data in the vehicle situation characterized by said at least one parameter.12. The method of claim 1, where said extraction algorithm determining step further includes the step of analyzing the vehicle situation based on the captured 3D-image, where a value of said at least one parameter is obtained from said analyzing step.13. The method of claim 1, further comprising the step of detecting vehicle status information, where the detected vehicle status information includes a value of said at least one parameter.14. The method of claim 13, where said vehicle status information includes velocity information of said vehicle, and where the size of the visual field from which said data extraction algorithm extracts data decreases as the velocity of the vehicle increases.15. (canceled)16. (canceled)17. The method of claim 1, where said extraction algorithm determining step includes the step of selecting a particular data extraction algorithm out of a plurality of data extraction algorithms based on a parameter describing the vehicle situation.18. The method of claim 1, where said 3D-vector model represents objects in the environment of the vehicle by vectors and object classification information, said object classification information associates an object with standard object classes of an object model.19. The method of claim 18, where said object model is pre-stored in a database containing standard information of such objects that are most likely to occur and most relevant to the vehicle's environment.20. (canceled)21. The method of claim 1, further comprising the step of: generating a 3D-space-and-time model, where said 3D-space-and-time model includes a sequence of 3D-vector models.22. (canceled)23. The method of claim 21, further comprising the step of generating a response to the information of the vehicle environment represented by said 3D-space-and-time model.24. The method of claim 23, where said response beg is directed towards avoiding an emergency situation to be expected by a driver.25. (canceled)26. (canceled)27. The method of claim 24, where said response includes an automatic interference with the vehicle control.28. The method of claim 23, where said response is directed towards assisting a driver in parking the vehicle.29. The method of claim 23, where said response includes taking countermeasures to minimize the consequences of a threatening accident.30. (canceled)31. The method of claim 1, where said image capturing step captures a plurality of 3D-images from various camera positions, where said plurality of 3D-images re-present an environment area including a predetermined range around the vehice.32. (canceled)33. The method of claim 1, where said 3D-vector model generating step further employs data from sensors.34. The method of claim 33, further comprising the step of sensor fusion to transform sensor data into a unique format and remove inconsistent and/or redundant data.35. (canceled)36. The method of claim 1, further comprising the step of transmitting the 3D vector model to an external location with respect to said vehicle.37. (canceled)38. The method of claim 36, where said 3D-vector model is transmitted via a wireless communication.39. The method of claim 36, where said 3D-vector model is transmitted to and visualized on an external computer.40. The method of claim 36, further comprising the step of evaluating said 3D-vector model at an external computer.41. The method of claim 40, further comprising the step of re-transmitting results of said evaluation back to the vehicle.42. The method of claim 40, further comprising the step of remotely controlling the vehicle by employing the evaluation results.43. (canceled)44. The method of claim 36, where said transmitting step transmits a sequence of 3D-vector models representing a 3D-space-and-time model.45. The method of claim 1, further comprising the step of saving the 3D-vector model in an event data recorder.46. The method of claim 45, further comprising the steps of: generating a sequence of 3D-vector models during a predetermined period of time; and saving said sequence of 3D-vector models into the event data recorder.47. The method of claim 46, further comprising the step of associating the 3D-vector models with time stamps before saving into the event data recorder.48. The method of claim 47, where the 3D-vector models are saved for a predetermined storing time; the method further comprising the step of detecting an impact caused by an extraordinary event, where the 3D-vector models remain permanently saved upon said detection of an impact.49. The method of claim 48, where said 3D-vector models are stored in a cyclic storage having a predetermined storage capacity; the method further comprising the step of cyclically overwriting stored 3D-vector models having the earliest time stamp, until said detection of au impact.50. An environment monitoring device for monitoring the environment of a vehicle, comprising: a 3D-camera for capturing an image of the environment of the vehicle, the image representing a predetermined area of the vehicle environment; and a first processing unit for processing the information acquired by said 3D-camera, said first processing unit including: a data extracting unit for extracting data from said image by employing a data extraction algorithm; and a model generation unit for generating a 3D-vector model of the vehicle environment from said data extracted from said image; characterized in that said first processing unit further includes a determining unit for determining the data extraction algorithm to be employed by said data extracting unit based on at least one parameter characterizing the vehicle situation.51. The environment monitoring device of claim 50, where said determining unit determines the data extraction algorithm to be adapted according to the processing time available for generating the 3D-vector model.52. The environment monitoring device of claim 50, where said determining unit determines the data extraction algorithm to be adapted according to information reflected in the 3D-vector model.53. The environment monitoring device of claim 50, where said data extraction algorithm includes object recognition.54. (canceled)55. The environment monitoring device of claim 50, where said data extraction algorithm extract data only from a part of the image corresponding to a certain visual field.56. The environment monitoring device of claim 50, further comprising a second processing unit for further processing said 3D-vector model for assisting a driver in controlling the vehicle based on the processing result.57. The environment monitoring device of claim 56, where said determining units determines a maximum available amount of time for generating and further processing said 3D-vector model based on the vehicle situation, and further takes into account said maximum available amount of time for determining said data extraction algorithm.58. (canceled)59. The environment monitoring device of claim 58, where said determining unit determines a maximum amount of data to be extracted in correspondence with said maximum amount of time.60. The environment monitoring device of claim 59, where said determining unit further determines priorities for different kinds of data that can be extracted from said image based on the particular relevance of different kinds of data in the vehicle situation characterized by said at least one parameter.61. The environment monitoring device of claim 50, where said determining unit further analyzes the vehicle situation based on the captured image for obtaining a value of said at least one parameter.62. The environment monitoring device of claim 50, further comprising a detector for detecting vehicle status information, where the detected vehicle status information includes a value of said at least one parameter.63. The environment monitoring device of claim 62, where said vehicle status information includes velocity information of said vehicle, and where the size of the visual field from which said data extraction algorithm extracts data decreases as the velocity of the vehicle is increases.64. (canceled)65. (canceled)66. The environment monitoring device of claim 50, where said determining unit selects a particular data extraction algorithm out of a plurality of data extraction algorithms based on a parameter describing the vehicle situation.67. The environment monitoring device of claim 50, where said 3D-vector model represents objects in the environment of the vehicle by vectors and object classification information, said object classification information associates an object with standard object classes of an object model.68. The environment monitoring device of claim 67, further comprising a database for pre-storing said object model.69. The environment monitoring device of claim 67, where said extraction algorithm obtains object classification information from the image.70. The environment monitoring device of claim 50, where said model generation unit generates a 3D-space-and-time model that includes a sequence of 3D-vector models.71. (canceled)72. The environment monitoring device of claim 70, further comprising a second processing unit for generating a response to the information of the vehicle environment represented by said 3D-space-and-time model.73. The environment monitoring device of claim 72, where said response is directed towards avoiding an emergency situation to be expected by a driver.74. The environment monitoring device of claim 73, further comprising a device for notifying a driver of an expected emergency situation based on the response generated by said second processing unit.75. The environment monitoring device of claim 73, further comprising a device for assisting a driver to avoid an emergency situation based on the response generated by said second processing unit.76. The environment monitoring device of claim 75, where said device is a controller for automatically interfering with the vehicle control based on the response generated by said second processing unit.77. The environment monitoring device of claim 72, where said response is directed towards assisting a driver in parking the vehicle.78. The environment monitoring device of claim 72, further comprising a controller for taking countermeasures to minimize the consequences of a threatening accident based on the response generated by said second processing unit.79. (canceled)80. The environment monitoring device of claim 50, comprising a plurality of 3D cameras for capturing images from various camera positions, where the images captured by said plurality of 3D-cameras representing an environment area include a predetermined range around the vehicle.81. (canceled)82. The environment monitoring device of claim 50, further comprising sensors, where said model generation unit further employs data from said sensors for generating said 3D-vector model.83. The environment monitoring device of claim 82, further comprising a sensor fusion unit for transforming sensor data into a unique format and removing inconsistent and/or redundant data.84. (canceled)85. The environment monitoring device of claim 50, further comprising a transmitter for transmitting the 3D-vector model to an external location with respect to said vehicle.86. (canceled)87. The environment monitoring device of claim 85, where said transmitter transmits said 3D-vector model via wireless communication.88. The environment monitoring device of claim 85, where said transmitter transmits said 3D-vector model for visualizing on an external computer.89. The environment monitoring device of claim 85, where said transmitter transmits said 3D-vector model for evaluating at an external computer.90. The environment monitoring device of claim 89, further comprising receiving unit for receiving results of said evaluation transmitted back to the vehicle.91. The environment monitoring device of claim 89, further comprising a controller for controlling the vehicle by employing the evaluation results received from the external computer.92. (canceled)93. The environment monitoring device of claim 85, where said transmitter transmits a sequence of 3D-vector models representing a 3D-space-and-time model.94. The environment monitoring device of claim 50, further comprising an event data recorder for saving the 3D-vector model.95. The environment monitoring device of claim 94, where said model generation unit generates a sequence of 3D-vector models during a predetermined period of time, and where said sequence of 3D-vector models is saved into the event data recorder.96. The environment monitoring device of claim 95, where the 3D-vector models are associated with time stamps before saving.97. The environment monitoring device of claim 96, further comprising an impact detector for detecting an impact caused by an extraordinary event, where the 3D-vector models are saved in the event data recorder for a predetermined storing time and the 3D-vector models remain permanently saved when an impact is detected by said impact detector.98. (canceled)",US2009268947_A1.txt,G06K9/00,{'computing; calculating; counting'},['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers'],"real time environment model generation system A vehicle environment monitoring system is provided that is based on a three-dimensional vector model. The three-dimensional vector model of the vehicle's environment is generated on the basis of the image data captured by at least one three-dimensional camera. Out of the image data, particular data are extracted for generating the three-dimensional vector model in order to reduce the data volume. For data extraction, a data extraction algorithm is applied that is determined in accordance with at least one parameter that relates to the situation of the vehicle. Therefore, targeted data extraction is performed for generating a three-dimensional model that is particularly adapted for an application that is desired in the current vehicle situation. The applications of the vector model include driver assistance, external monitoring and vehicle control, as well as recording in an event data recorder. In one implementation, a sequence of three-dimensional vector models, representing a three-dimensional space-and-time model, is generated.",computing; calculating; counting
US10795380_B1,2020-01-27,2020-10-06,2020-01-27,"BANJO CORPORATIONsafeXai, Inc.","MEHTA, RISHPATTON, DAMIEN MICHAEL",72664186,road monitoring,system and method for event-based vehicle operation,"Embodiments of a method and/or system for facilitating event-based vehicle operation can include routing a vehicle along a route in a three-dimensional space; detecting an event based on the external signal; determining that the event is of interest based on a parameter associated with the vehicle; and re-routing the vehicle towards the event, including modifying a vehicle locomotion component to move the vehicle toward the event along a shortened route.","We claim:1. A system comprising: a processor; and system memory coupled to the processor and storing instructions configured to cause the processor to: route a vehicle along a route that defines coordinates of travel in a three-dimensional space; while the vehicle is traveling along the route, detect an event based on an external signal including detecting coordinates of the event; determine that the event is an event of interest based on a parameter associated with the vehicle; and re-route the vehicle toward the coordinates of the event of interest along a shortened route that defines adjusted coordinates of travel in the three-dimensional space, including dynamically modifying operation of a locomotion component to depart from the route to move the vehicle toward the coordinates of the event of interest along the shortened route.2. The system of claim 1, wherein instructions configured to re-route the vehicle comprise instructions configured to dynamically modify operation of a locomotion component selected from among: a vehicle propulsion component or a vehicle steering component.3. The system of claim 1, wherein instructions configured to re-route the vehicle comprise instructions configured to: determine control instructions for the vehicle based on the shortened route; and send the control instructions to the vehicle.4. The system of claim 1, wherein instructions configured to detect an event comprise instructions configured to: collect a plurality of social networking system posts; compute a keyword frequency associated with the plurality of social networking system posts; and determine that the computed keyword frequency exceeds an historic keyword frequency.5. The system of claim 1, wherein instructions configured to detect an event comprises instructions configured to detect a ground-based event; and wherein instructions configured determine that the event is an event of interest comprise instructions configured to determine that the ground-based event is of interest based on a vehicle parameter received from an airborne Unmanned Aerial Vehicle (UAV).6. The system of claim 1, wherein instructions configured to route a vehicle comprise instructions configure to route an Unmanned Aerial Vehicle (UAV) along a series of waypoints; and wherein instructions configured to re-route the vehicle along the shortened route comprise instructions configured to re-route the UAV along a second different series of waypoints.7. The system of claim 1, further comprising instructions configured to determine an estimated time of arrival (ETA) to the event based on the route; and compute vehicle arrival at the event prior to the ETA via travel on the shortened route.8. The system of claim 1, wherein instructions configured to route a vehicle comprise instructions configure to route an aquatic vehicle along a series of waypoints; and wherein instructions configured to re-route the vehicle along the shortened route comprise instructions configured to re-route the aquatic vehicle along a second different series of waypoints.9. The system of claim 1, further comprising instructions configured to collect sensor data; and wherein instructions configured to detect an event comprise instructions configured to detect the event based on the sensor data.10. The system of claim 1, wherein instructions configured to detect an event comprise instructions configured to determine that an event category probability exceeds a category probability threshold.11. The system of claim 1, wherein instructions configured to detect an event comprise instructions configured to: determine an event category probability based on the external signal; compute that the event category probability exceeds a category probability threshold; and categorize the event into the event category.12. The system of claim 1, wherein instructions configured to re-route the vehicle comprise instructions configured to modify a vertical coordinate of travel.13. The system of claim 1, further comprising instructions configured to identify a region on the route; and wherein instructions configured to detect an event comprise instructions configured to detect an event associated with the region.14. The system of claim 1, wherein instructions configured to detect an event comprise instructions configured to detect the event using an artificial intelligence approach.15. The system of claim 1, wherein instructions configured to determine that the event is an event of interest comprise instructions configured to anticipate a physical proximity of the vehicle to the event.16. The system of claim 1, wherein instructions configured to determine that the event is an event of interest based on a parameter associated with the vehicle comprise instructions configured to determine that the event is an event of interest based on a vehicle parameter selected from among: a vehicle identifier, a vehicle class, a vehicle model, a vehicle component type, a vehicle emission parameter, or a vehicle operation parameter.17. The system of claim 1, wherein instructions configured to route a vehicle along a route comprise instructions configured to route a fleet vehicle along the route; and wherein instructions configured to re-route the vehicle comprise instructions configured to re-route the fleet vehicle.18. The system of claim 1, further comprising instructions configured to ingest the external signal, the external signal selected from among: weather data, emergency response data, flight tracking data, audio data, image data, calendar data, or accident data.",US10795380_B1.txt,"B64C39/02,G05D1/10,G06Q50/00,G08G5/00","{'controlling; regulating', 'signalling', 'computing; calculating; counting', 'aircraft; aviation; cosmonautics'}","['aeroplanes; helicopters (air-cushion vehicles b60v)', 'systems for controlling or regulating non-electric variables (for continuous casting of metals b22d11/16; valves per se f16k; sensing non-electric variables, see the relevant subclasses of g01; for regulating electric or magnetic variables g05f)', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})']","system and method for event-based vehicle operation Embodiments of a method and/or system for facilitating event-based vehicle operation can include routing a vehicle along a route in a three-dimensional space; detecting an event based on the external signal; determining that the event is of interest based on a parameter associated with the vehicle; and re-routing the vehicle towards the event, including modifying a vehicle locomotion component to move the vehicle toward the event along a shortened route.",controlling; regulating signalling computing; calculating; counting aircraft; aviation; cosmonautics
US10108866_B2,2016-05-26,2018-10-23,2016-05-26,CARNEGIE MELLON UNIVERSITYGM GLOBAL TECHNOLOGY OPERATIONSGM GLOBAL TECHNOLOGY OPERATIONS,"WANG JINSONGPRINET, VERONIQUE C.WETTERGREEN, DAVIDJONGHO, LEE",60418041,road monitoring,method and system for robust curb and bump detection from front or rear monocular cameras,A method of detecting a curb. An image of a path of travel is captured by a monocular image capture device mounted to a vehicle. A feature extraction technique is applied by a processor to the captured image. A classifier is applied to the extracted features to identify a candidate region in the image. Curb edges are localized by the processor in the candidate region of the image by extracting edge points. Candidate curbs are identified as a function of the extracted edge points. A pair of parallel curves is selected representing the candidate curb. A range from image capture device to the candidate curb is determined. A height of the candidate curb is determined. A vehicle application is enabled to assist a driver in maneuvering a vehicle utilizing the determined range and depth of the candidate curb.,"1. A method of detecting a curb comprising: capturing an image of a path of travel by an image capture device mounted to a vehicle, wherein an optical axis of the image capture device is parallel to a plane of a surface having the curb, and the image capture device has an XYZ camera-center reference system; applying a feature extraction technique by a processor to the captured image to generate extracted features of the captured image; applying a classifier to the extracted features to identify a candidate region in the image; localizing curb edges by the processor in the candidate region of the captured image by extracting edge points; identifying at least one candidate curb as a function of the extracted edge points; selecting a pair of parallel curves representing the candidate curb; determining a range from the image capture device to the candidate curb via the processor; and enabling a vehicle application to assist a driver in maneuvering a vehicle utilizing the determined range to the candidate curb; wherein determining the range includes using the following formula: where Dg is the range, {dot over (w)}g and {dot over (u)}g are the coordinates along respective axes Z and X, in the XYZ camera-center reference system, of a bearing vector b=({dot over (u)},{dot over (v)},{dot over (w)}) of a point on the image, and h is a height of the image capture device above a road surface having the candidate curb.2. The method of claim 1 wherein applying a feature extraction technique to the captured image includes applying a descriptor for dividing the captured image into cells.3. The method of claim 2 wherein applying the descriptor includes applying a Histogram of Gradient for dividing the captured image into cells, and wherein a histogram of gradient directions is compiled for each cell as a function of the pixels in each cell.4. The method of claim 3 wherein the cells have patches defined as small connected regions, and wherein applying a classifier to the extracted features includes applying a binary classifier to classify each of the patches of each cell to identify candidate regions.5. The method of claim 3 wherein the cells have patches defined as small connected regions, and applying a classifier to the extracted features includes applying a support vector machine to classify each of the patches of each cell to identify candidate regions.6. The method of claim 1 wherein localized curb edges in the candidate regions includes extracting edge points from the candidate regions.7. The method of claim 6 wherein a Gaussian filter is applied to smooth the image prior to extracting the edge points.8. The method of claim 1 wherein identifying candidate curb includes identifying a plurality of candidate curbs that are spaced within a predetermined range of one another.9. The method of claim 8 wherein the plurality of candidate curbs include five curb lines.10. The method of claim 8 wherein second order polynomials are used to fit curb lines in the candidate regions.11. The method of claim 1 wherein the selected pair of parallel curves represent an upper boundary and a lower boundary of the candidate curb.12. The method of claim 1 wherein determining a range from the image capture device to the candidate curb includes determining a range from a camera plane of the image capture device to the candidate curb.13. The method of claim 1 further comprising applying temporal smoothing via the processor as a post-processing step to eliminate false positives.14. The method of claim 13 wherein applying temporal smoothing includes capturing a next image at a next time frame, via the image capture device, and applying a tracking filter to the processed image and the next image via the processor, wherein the temporal smoothing observes redundancies between consecutive image frames.15. The method of claim 1 wherein enabling a vehicle application includes displaying the image on a display device to the driver and highlighting the identified curb in the display device.16. The method of claim 1 wherein enabling a vehicle application includes applying the range the candidate curb to an autonomous parking application, the autonomous parking application actuating vehicle devices to park the vehicle.17. The method of claim 1 wherein enabling a vehicle application includes applying the range to the candidate curb to a collision avoidance application, the collision avoidance application actuating vehicle devices to avoid a collision with the curb.18. The method of claim 1 wherein enabling a vehicle application includes applying the range to the candidate curb to a clear path detection system, the clear path detection system actuating vehicle devices to maintain the vehicle along the path of travel.19. The method of claim 1 wherein enabling a vehicle application includes applying the range to the candidate curb to a lane centering application, the lane centering application actuating vehicle devices to center the vehicle within the lane.20. A method of detecting a curb from a vehicle using an image capture device mounted to the vehicle and having an XYZ camera-center reference system, the method comprising: capturing an image of a path of travel via the image capture device; applying a feature extraction technique to the captured image using a processor to thereby generate extracted features of the image; applying a classifier to the extracted features to identify a candidate region in the image; localizing curb edges in the identified candidate region of the image by extracting edge points via the processor; identifying a candidate curb in the identified candidate region as a function of the extracted edge points; selecting a pair of parallel curves representing the candidate curb; determining a range from the image capture device to the candidate curb via the processor; and enabling a vehicle application to assist a driver in maneuvering a vehicle utilizing the determined range of the candidate curb; wherein determining the range includes using the following formula: where Dg is the determined range, {dot over (u)}g is the coordinate along axis X within the XYZ camera-center reference system of a bearing vector b=({dot over (u)},{dot over (v)},{dot over (w)}) of a point on the image, h is a height of the camera above a road surface having the candidate curb, and f is the camera focal length provided by camera intrinsic parameter calibration.21. The method of claim 1, further comprising estimating a height (hi) of the candidate curb using the following equation: where Di is the determined range (Dg); and enabling the vehicle application using the estimated height of the candidate curb.",US10108866_B2.txt,"G06K9/00,G06K9/46,G06K9/62",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']",method and system for robust curb and bump detection from front or rear monocular cameras A method of detecting a curb. An image of a path of travel is captured by a monocular image capture device mounted to a vehicle. A feature extraction technique is applied by a processor to the captured image. A classifier is applied to the extracted features to identify a candidate region in the image. Curb edges are localized by the processor in the candidate region of the image by extracting edge points. Candidate curbs are identified as a function of the extracted edge points. A pair of parallel curves is selected representing the candidate curb. A range from image capture device to the candidate curb is determined. A height of the candidate curb is determined. A vehicle application is enabled to assist a driver in maneuvering a vehicle utilizing the determined range and depth of the candidate curb.,computing; calculating; counting
US2020327337_A1,2019-09-12,2020-10-15,2019-04-15,HYUNDAI MOTOR COMPANYKIA MOTORS CORPORATIONCHUNG-ANG UNIVERSITY,"PAIK, JOON KIKIM, HYUN SANGJang, Jin BeumAnn, Yun SupChoi, Nak EunChoo, Yeon Seung",72748049,road monitoring,apparatus and method for detecting object of a vehicle,"An apparatus for detecting an object of a vehicle is provided. The apparatus includes a camera that acquires an image from front of a vehicle and a controller that generates feature pyramid images based on a plurality of feature images extracted from the image. The controller also generates feature aggregation images by filtering the feature pyramid images, detects a pedestrian area from the feature aggregation images, and detects face regions from the feature pyramid images. At least one of the face regions that overlaps the pedestrian area is then determined as a face of a pedestrian.","1. An apparatus for detecting an object of a vehicle, comprising: a camera configured to acquire an image of an area in front of the vehicle; and a controller configured to generate feature pyramid images based on a plurality of feature images extracted from the image, generate feature aggregation images by filtering the feature pyramid images, detect a pedestrian area from the feature aggregation images, detect face regions from the feature pyramid images, and determine at least one of the face regions that overlaps the pedestrian area as a face of a pedestrian.2. The apparatus of claim 1, wherein the feature pyramid images are generated by down-scaling the feature image from an original size at a predetermined ratio.3. The apparatus of claim 1, wherein the controller is configured to generate a filter bank by performing a convolution operation or a correlation operation on a first feature filter and a second feature filter and generate the feature aggregation images by filtering the feature pyramid images using the filter bank.4. The apparatus of claim 3, wherein the controller is configured to generate a training feature image based on feature information extracted from a training image and generates the first feature filter from the training feature image based on a Local Binary Pattern method.5. The apparatus of claim 4, wherein the controller is configured to generate a first feature aggregation image by filtering the training feature image using the first feature filter and generate a second feature filter from the first feature aggregation image based on a Locally De-correction Channel Feature (LDCF) method.6. The apparatus of claim 5, wherein the controller is configured to generate a training feature aggregation image by filtering the training feature image using the filter bank and generate a pedestrian classifier for classifying the pedestrian area in the training feature aggregation image.7. The apparatus of claim 6, wherein the controller is configured to detect the pedestrian area from the feature aggregation images using the pedestrian classifier.8. The apparatus of claim 1, wherein the controller is configured to synthesize a region, having a highest score among the face regions that overlap the pedestrian area, with the pedestrian area.9. The apparatus of claim 8, wherein the controller is configured to determine the region having the highest score as the face of the pedestrian.10. The apparatus of claim 9, wherein the controller is configured to calculate a probability that an object of the image in the face region that overlaps the pedestrian area is a face of the pedestrian, and generate the score based on the probability.11. A method for detecting an object of a vehicle, comprising: acquiring, by a controller, an image of an area in front of the vehicle; generating, by the controller, feature pyramid images based on a plurality of feature images extracted from the image; generating, by the controller, feature aggregation images by filtering the feature pyramid images; detecting, by the controller, a pedestrian area from the feature aggregation images; detecting, by the controller, face regions from the feature pyramid images; and determining, by the controller, at least one of the face regions overlapping the pedestrian area as a face of a pedestrian.12. The method of claim 11, wherein the feature pyramid images are generated by down-scaling the feature image from an original size at a predetermined ratio.13. The method of claim 11, further comprising: generating, by the controller, a filter bank by performing a convolution operation or a correlation operation on a first feature filter and a second feature filter; and generating, by the controller, the feature aggregation images by filtering the feature pyramid images using the filter bank.14. The method of claim 13, wherein the first feature filter generates a training feature image based on feature information extracted from a training image, and generates the first feature filter from the training feature image based on a Local Binary Pattern method.15. The method of claim 14, wherein the second feature filter generates a first feature aggregation image by filtering the training feature image using the first feature filter and generates a second feature filter from the first feature aggregation image based on a Locally De-correction Channel Feature (LDCF) method.16. The method of claim 15, further comprising: generating, by the controller, a training feature aggregation image by filtering the training feature image using the filter bank; and generating, by the controller, a pedestrian classifier for classifying the pedestrian area in the training feature aggregation image.17. The method of claim 16, further comprising: detecting, by the controller, the pedestrian area from the feature aggregation images using the pedestrian classifier.18. The method of claim 11, further comprising: synthesizing, by the controller, a region, having a highest score among the face regions overlapping the pedestrian area, with the pedestrian area.19. The method of claim 18, further comprising: determining, by the controller, the region having the highest score as the face of the pedestrian.20. The method of claim 19, further comprising: calculating, by the controller, a probability that an object of the image in the face region overlapping the pedestrian area is a face of the pedestrian, and generating the score based on the probability.",US2020327337_A1.txt,"B60W30/095,G06K9/00,G06K9/46","{'computing; calculating; counting', 'vehicles in general'}","['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']","apparatus and method for detecting object of a vehicle An apparatus for detecting an object of a vehicle is provided. The apparatus includes a camera that acquires an image from front of a vehicle and a controller that generates feature pyramid images based on a plurality of feature images extracted from the image. The controller also generates feature aggregation images by filtering the feature pyramid images, detects a pedestrian area from the feature aggregation images, and detects face regions from the feature pyramid images. At least one of the face regions that overlaps the pedestrian area is then determined as a face of a pedestrian.",computing; calculating; counting vehicles in general
US9466000_B2,2012-05-21,2016-10-11,2011-05-19,"UNIVERSITY OF CALIFORNIABHANU, BIRKAFAI, MEHRAN","BHANU, BIRKAFAI, MEHRAN",47177666,vehicle classification,dynamic bayesian networks for vehicle classification in video,"A system and method for classification of passenger vehicles and measuring their properties, and more particularly to a stochastic multi-class vehicle classification system, which classifies a vehicle (given its direct rear-side view) into one of four classes Sedan, Pickup truck, SUV/Minivan, and unknown, and wherein a feature pool of tail light and vehicle dimensions is extracted which feeds a feature selection algorithm to define a low-dimensional feature vector, and the feature vector is then processed by a Hybrid Dynamic Bayesian Network (HDBN) to classify each vehicle.","1. A method for vehicle classification comprising: detecting at least three subcomponents, the at least three subcomponents comprising vehicle detection, license plate extraction, and tail light extraction; using a Gaussian mixture model approach for detection of a moving object, wherein the Gaussian mixture model comprises Gaussian distributions to determine if a pixel is more likely to belong to a background model or not, and an AND approach, which determines a pixel as background only if the pixel falls within three standard deviations for all the components in all three R, G, and B color channels; validating detected moving objects by using a simple frame differencing approach; removing shadows and erroneous pixels by finding a vertical axis of symmetry using an accelerated version of Loy's symmetry and readjusting a bounding box containing a mask with respect to an axis of symmetry, wherein if the shadow is behind the vehicle, removing the shadow using geometrical assumptions such as camera location, object geometry, and ground surface geometry, and wherein given the vehicle rear mask, measuring a height and width of the bounding box, and area of the mask; inputting the license plate corner coordinates into an algorithm for license plate extraction, and adding a Gaussian noise with constant mean 0 and variance 0.2 times width to the license plate width measurement; and performing a Bayesian network analysis on the at least three detected subcomponents for a plurality of vehicle classifications, wherein the Bayesian network analysis is defined as a directed acyclic graph G=(V, E), where nodes represent random variables from a domain of interest and arcs symbolize direct dependencies between the random variables.2. The method of claim 1, wherein for a Bayesian network with n nodes X1, X2, . . . Xn, a full joint distribution is defined as: but a node in a Bayesian network is only conditional on its parent's values so where p(x1, x2, . . . , xn) is an abbreviation for p(X1=x1 {circumflex over ( )} . . . {circumflex over ( )} Xn=xn).3. The method of claim 2, further comprising using a K2 algorithm to determine a sub-optimal structure.4. The method of claim 3, comprising: generating a manually structured network and comparing the sub-optimal structure to the manually structured network.5. The method of claim 4, comprising: adding a temporal dimension to the Bayesian network to create a Dynamic Bayesian Network (DBN structure).6. The method of claim 5, comprising: replicating the Bayesian network with time-dependent random variables over T time slices, and wherein a new set of arcs defining the transition model is used to determine how various random variables are related between time slices.7. The method of claim 6, wherein the DBN structure is defined as following: for each time slice ti, i=1, 2, . . . , 5; each feature xit is the parent of xit+1; Ct is the parent of Ct+1; and all intra slice dependencies also hold as inter time slices except for arcs from time slice t hidden nodes to time slice t+1 observed nodes.8. The method of claim 7, comprising: generating a Hybrid Dynamic Bayesian Network (HDBN), which consists of discrete and continuous nodes, and wherein parameters of the HDBN are required before classification is performed.9. The method of claim 8, wherein the plurality of vehicle classifications include at least one or more of the following: Sedan, Pickup, and SUV or Minivan.10. The method of claim 1, comprising: generating a complete system framework, which includes data collection, feature extraction, feature selection, and classification.11. The method of claim 10, comprising: extracting features from images of a rear view of a vehicle.12. The method of claim 11, comprising: extracting tail light features, wherein the tail light features include separately for each tail light a width, a distance from the license plate, and an angle between the tail light and the license plate.13. The method of claim 12, comprising: extracting license plate features, the license plate features including the license plate location and a size thereof is used as a reference to enable comparison and normalize the tail light features and the vehicle size values.14. The method of claim 1, comprising: performing a tail light extraction by fusing two methods, a first method wherein an image is converted to hue, saturation and value (HSV) color space and classifying pixels into three main color groups red, green, and blue, and a second method, which defines the red level of each pixel as in red, green and blue (RGB) color space.15. The method of claim 14, comprising: generating a bounding box surrounding each tail light by combining results from both methods and checking if the regions with high redness can be a tail light.16. The method of claim 15, wherein if the vehicle body color is red itself, estimating a vehicle color using a hue, saturation and value (HSV) color space histogram analysis approach, which determines if the vehicle is red or not, and if a red vehicle is detected, the tail light detection component is enhanced by adding an extra level of post-processing which includes thresholding, color segmentation, removing large and small regions, and symmetry analysis, and after the tail lights are detected, separately computing width, centroid, and distance and angle with the license plate for both left and right tail lights.17. The method of claim 16, comprising: extracting one or more features for each image frame, and normalizing each distance with respect to the license plate width, the one or more features comprising: perpendicular distance from license plate centroid to a line connecting two tail light centroids, right tail light width, left tail light width, right tail light license plate angle, left tail light license plate angle, right tail light license plate distance, left tail light license plate distance, bounding box width, bounding box height, license plate distance to bounding box bottom side, and/or vehicle mask area.18. The method of claim 17, comprising: optimizing an evaluation criterion to improve classification accuracy, shorten computational time, reduce measurements costs, and/or eliminate use of dimensionality.19. The method of claim 18, comprising: performing a Sequential Floating Forward Selection (SFFS), which is a deterministic statistical pattern recognition (SPR) feature selection method and returning a single suboptimal solution.20. The method of claim 19, wherein SFFS starts from an empty set and adding features that increase accuracy and removing the least significant features during a conditional exclusion.21. The method of claim 20, comprising: a stopping condition, which halts the SFFS algorithm to limit the number of feature selection iterative steps to 2n-1, wherein n is the number of features and defining a correct classification rate (CCR) threshold of b %, where b is greater than the CCR of the case when all features are used.22. The method of claim 21, wherein the classification is a two stage approach, which consists of estimating the Gaussian distribution parameters of the distance to the nearest neighbor for all vehicles in the training dataset and determining if a vehicle test case is known or unknown by initially computing the distance to its nearest neighbor.23. The method of claim 22, comprising: if the distance does not lie within 4 standard deviations of a mean (4), classifying it as unknown, and if the vehicle is classified as known it is a candidate for a second stage of classification.24. A system for classification of vehicles comprising: a camera configured to capture images of at least one moving object; and a computer processing unit configured to detecting at least three subcomponents from the captured images of the at least one moving object, the at least three subcomponents comprising vehicle detection, license plate extraction, and tail light extraction; use a Gaussian mixture model approach for detection of a moving object, wherein the Gaussian mixture model comprises Gaussian distributions to determine if a pixel is more likely to belong to a background model or not, and an AND approach, which determines a pixel as background only if the pixel falls within three standard deviations for all the components in all three R, G, and B color channels; validating detected moving objects by using a simple frame differencinq approach; remove shadows and erroneous pixels by finding a vertical axis of symmetry using an accelerated version of Loy's symmetry and readiusting a bounding box containing a mask with respect to an axis of symmetry, wherein if the shadow is behind the vehicle, removing the shadow using geometrical assumptions such as camera location, object geometry, and ground surface geometry, and wherein given the vehicle rear mask, measuring a height and width of the bounding box, and area of the mask; input the license plate corner coordinates into an algorithm for license plate extraction, and adding a Gaussian noise with constant mean 0 and variance 0.2 times width to the license plate width measurement; and perform a Bayesian network analysis on the at least three detected subcomponents for a plurality of vehicle classifications, wherein the Bayesian network is defined as a directed acyclic graph G=(V, E), where nodes represent random variables from a domain of interest and arcs symbolize direct dependencies between random variables.25. The system of claim 24, wherein the computer processing unit further comprises: a memory arrangement, a processing unit and an optional display unit configured to display data and/or classification of the distinct structures in the class.26. The system of claim 24, wherein the camera is configured to capture video images.27. A computer program product comprising a non-transitory computer usable medium having a computer readable code embodied therein for classification of passenger vehicles and measuring their properties from a rear view video frame, the computer readable program code is configured to execute a process, which: detects at least three subcomponents from the captured images of the at least one moving object, the at least three subcomponents comprising vehicle detection, license plate extraction, and tail light extraction; uses a Gaussian mixture model approach for detection of a moving object, wherein the Gaussian mixture model comprises Gaussian distributions to determine if a pixel is more likely to belong to a background model or not, and an AND approach, which determines a pixel as background only if the pixel falls within three standard deviations for all the components in all three R, G, and B color channels; validates detected moving objects by using a simple frame differencing approach; removes shadows and erroneous pixels by finding a vertical axis of symmetry using an accelerated version of Loy's symmetry and readjusting a bounding box containing a mask with respect to an axis of symmetry, wherein if the shadow is behind the vehicle, removing the shadow using geometrical assumptions such as camera location, object geometry, and ground surface geometry, and wherein given the vehicle rear mask, measuring a height and width of the bounding box, and area of the mask; inputs the license plate corner coordinates into an algorithm for license plate extraction, and adding a Gaussian noise with constant mean 0 and variance 0.2 times width to the license plate width measurement; and performs a Bayesian network analysis on the at least three detected subcomponents for a plurality of vehicle classifications, which are known, wherein the Bayesian network is defined as a directed acyclic graph G=(V, E), where nodes represent random variables from a domain of interest and arcs symbolize direct dependencies between random variables.28. The system of claim 24, wherein for a Bayesian network with n nodes X1, X2, . . . Xn, a full joint distribution is defined as: but a node in a Bayesian network is only conditional on its parent's values so where p(x1, x2, . . . , xn) is an abbreviation for p(X1=x1 {circumflex over ( )} . . . {circumflex over ( )} Xn=xn).29. The system of claim 28, further comprising using a K2 algorithm to determine a sub-optimal structure; generating a manually structured network and comparing the sub-optimal structure to the manually structured network; and adding a temporal dimension to the Bayesian network to create a Dynamic Bayesian Network (DBN structure).30. The system of claim 29, comprising: replicating the Bayesian network with time-dependent random variables over T time slices, and wherein a new set of arcs defining the transition model is used to determine how various random variables are related between time slices; and wherein the DBN structure is defined as following: for each time slice ti, i=1, 2, . . . , 5; each feature xit is the parent of xit+1; Ct is the parent of Ct+1; and all intra slice dependencies also hold as inter time slices except for arcs from time slice t hidden nodes to time slice t+1 observed nodes.31. The system of claim 30, comprising: generating a Hybrid Dynamic Bayesian Network (HDBN), which consists of discrete and continuous nodes, and wherein parameters of the HDBN are required before classification is performed, and wherein the plurality of vehicle classifications include at least one or more of the following: Sedan, Pickup, and SUV or Minivan.32. The system of claim 24, comprising: generating a complete system framework, which includes data collection, feature extraction, feature selection, and classification; extracting features from images of a rear view of a vehicle; extracting tail light features, wherein the tail light features include separately for each tail light a width, a distance from the license plate, and an angle between the tail light and the license plate; and extracting license plate features, the license plate features including the license plate location and a size thereof is used as a reference to enable comparison and normalize the tail light features and the vehicle size values.",US9466000_B2.txt,"G06K9/00,G06K9/46,G06K9/62",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']","dynamic bayesian networks for vehicle classification in video A system and method for classification of passenger vehicles and measuring their properties, and more particularly to a stochastic multi-class vehicle classification system, which classifies a vehicle (given its direct rear-side view) into one of four classes Sedan, Pickup truck, SUV/Minivan, and unknown, and wherein a feature pool of tail light and vehicle dimensions is extracted which feeds a feature selection algorithm to define a low-dimensional feature vector, and the feature vector is then processed by a Hybrid Dynamic Bayesian Network (HDBN) to classify each vehicle.",computing; calculating; counting
US10740927_B2,2018-01-31,2020-08-11,2017-02-16,PING AN TECHNOLOGY COMPANY,"WANG, JIANZONGXIAO, JING",61152346,vehicle classification,method and device for vehicle identification,"The present application provides a method and device for vehicle identification, the method comprises: obtaining a first vehicle image and a second vehicle image, wherein the first vehicle image comprises a first vehicle, and the second vehicle image comprises a second vehicle; extracting a first color feature and a first vehicle type feature of the first vehicle, and extracting a second color feature and a second vehicle type feature of the second vehicle based on a convolutional neural network model; combining the first color feature and the first vehicle type feature into a first vehicle feature, and combining the second color feature and the second vehicle type feature into a second vehicle feature; calculating a similarity parameter between the first vehicle feature and the second vehicle feature; and determining whether the first vehicle is the same one as the second vehicle according to the similarity parameter.","1. A vehicle identification method, comprising: obtaining a first vehicle image and a second vehicle image, wherein the first vehicle image comprises a first vehicle, and the second vehicle image comprises a second vehicle; extracting a first color feature and a first vehicle type feature of the first vehicle, and extracting a second color feature and a second vehicle type feature of the second vehicle based on a convolutional neural network model; combining the first color feature and the first vehicle type feature into a first vehicle feature, and combining the second color feature and the second vehicle type feature into a second vehicle feature; calculating a similarity parameter between the first vehicle feature and the second vehicle feature; and determining whether the first vehicle is the same one as the second vehicle according to the similarity parameter; wherein the step of extracting a first color feature and a first vehicle type feature of the first vehicle, and a second color feature and a second vehicle type feature of the second vehicle based on a convolutional neural network model, comprises: pre-training a first convolutional neural network model through a preset image database; training the first convolution neural network model through vehicle images of a plurality of colors; inputting the first vehicle image and the second vehicle image into the first convolution neural network model respectively, wherein the first convolution neural network model comprises a multi-layer convolution layer and a multi-layer full-connection layer, wherein an output of a previous layer is an input of a current layer; and extracting the first color feature from the first vehicle image and extracting the second color feature from the second vehicle image, after the processing through the convolution layer and the full-connection layer.2. The method according to claim 1, wherein the step of extracting a first color feature and a first vehicle type feature of the first vehicle and a second color feature and a second vehicle type feature of the second vehicle based on a convolutional neural network model further comprises: pre-training a second convolutional neural network model through a preset image database; training the second convolution neural network model through vehicle images of a plurality of visual angles; inputting the first vehicle image and the second vehicle image into the second convolution neural network model respectively, wherein the second convolution neural network model comprises a multi-layer convolution layer and a multi-layer full-connection layer, and an output of a previous layer is an input of a current layer; and extracting the first vehicle type feature from the first vehicle image, and extracting the second vehicle type feature from the second vehicle image after the processing through the convolution layer and the full-connection layer.3. The method according to claim 1, wherein the step of combining the first color feature and the first vehicle type feature into a first vehicle feature and combining the second color feature and the second vehicle type feature into a second vehicle feature comprises: obtaining a color weight of the first color feature and the second color feature, and obtaining a vehicle type weight of the first vehicle type feature and the second vehicle type feature, wherein the vehicle type weight is larger than the color weight; summating a multiplication of the first color feature and the color weight with a multiplication of the first vehicle type feature and the vehicle type weight to obtain the first vehicle feature; and summating a multiplication of the second color feature and the color weight with a multiplication of the second vehicle type feature and the vehicle type weight to obtain the second vehicle feature.4. The method according to claim 1, wherein the step of determining whether the first vehicle is the same one as the second vehicle according to the similarity parameter comprises: obtaining a preset threshold value; determining that the first vehicle is the same one as the second vehicle when the similarity parameter is larger than or equal to the preset threshold value; or determining that the first vehicle is not the same one as the second vehicle when the similarity parameter is smaller than the preset threshold value.5. A terminal device comprising a memory, a processor, and a computer readable instruction stored in the memory and executable by the processor, wherein when executing the computer readable instruction, the processor implements following steps of: obtaining a first vehicle image and a second vehicle image, wherein the first vehicle image comprises a first vehicle, and the second vehicle image comprises a second vehicle; extracting a first color feature and a first vehicle type feature of the first vehicle, and extracting a second color feature and a second vehicle type feature of the second vehicle based on a convolutional neural network model; combining the first color feature and the first vehicle type feature into a first vehicle feature, and combining the second color feature and the second vehicle type feature into a second vehicle feature; calculating a similarity parameter between the first vehicle feature and the second vehicle feature; and determining whether the first vehicle is the same one as the second vehicle according to the similarity parameter; wherein the step of extracting a first color feature and a first vehicle type feature of the first vehicle and a second color feature and a second vehicle type feature of the second vehicle based on a convolutional neural network model comprises: pre-training a first convolutional neural network model through a preset image database; training the first convolution neural network model through vehicle images of a plurality of colors; inputting the first vehicle image and the second vehicle image into the first convolution neural network model respectively, wherein the first convolution neural network model comprises a multi-layer convolution layer and a multi-layer full-connection layer, and an output of a previous layer is an input of a current layer; and extracting the first color feature from the first vehicle image, and extracting the second color feature from the second vehicle image after the processing through the convolution layer and the full-connection layer.6. The terminal device according to claim 5, wherein the step of extracting a first color feature and a first vehicle type feature of the first vehicle and a second color feature and a second vehicle type feature of the second vehicle based on a convolutional neural network model further comprises: pre-training a second convolutional neural network model through a preset image database; training the second convolution neural network model through vehicle images of a plurality of visual angles; inputting the first vehicle image and the second vehicle image into the second convolution neural network model respectively, wherein the second convolution neural network model comprises a multi-layer convolution layer and a multi-layer full-connection layer, and an output of a previous layer is an input of a current layer; and extracting the first vehicle type feature from the first vehicle image, and extracting the second vehicle type feature from the second vehicle image after the processing through the convolution layer and the full-connection layer.7. The terminal device according to claim 5, wherein the step of combining the first color feature and the first vehicle type feature into a first vehicle feature and combining the second color feature and the second vehicle type feature into a second vehicle feature comprises: obtaining a color weight of the first color feature and the second color feature, and obtaining a vehicle type weight of the first vehicle type feature and the second vehicle type feature, wherein the vehicle type weight is larger than the color weight; summating a multiplication of the first color feature and the color weight with a multiplication of the first vehicle type feature and the vehicle type weight to obtain the first vehicle feature; and summating a multiplication of the second color feature and the color weight with a multiplication of the second vehicle type feature and the vehicle type weight to obtain the second vehicle feature.8. The terminal device according to claim 5, wherein the step of determining whether the first vehicle is the same one as the second vehicle according to the similarity parameter comprises: obtaining a preset threshold value; determining that the first vehicle is the same one as the second vehicle when the similarity parameter is larger than or equal to the preset threshold value; or determining that the first vehicle is not the same one as the second vehicle when the similarity parameter is smaller than the preset threshold value.",US10740927_B2.txt,"G06K9/00,G06K9/62,G06N3/04,G06Q40/08,G06T7/90",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for', 'image data processing or generation, in general']","method and device for vehicle identification The present application provides a method and device for vehicle identification, the method comprises: obtaining a first vehicle image and a second vehicle image, wherein the first vehicle image comprises a first vehicle, and the second vehicle image comprises a second vehicle; extracting a first color feature and a first vehicle type feature of the first vehicle, and extracting a second color feature and a second vehicle type feature of the second vehicle based on a convolutional neural network model; combining the first color feature and the first vehicle type feature into a first vehicle feature, and combining the second color feature and the second vehicle type feature into a second vehicle feature; calculating a similarity parameter between the first vehicle feature and the second vehicle feature; and determining whether the first vehicle is the same one as the second vehicle according to the similarity parameter.",computing; calculating; counting
CN106156768_B,2016-07-01,2019-03-12,2016-07-01,JIAO WENHUAQU ZHENSHEN,BAI HEGAO SHANLU JUNYIQU ZHENSHEN,57349782,vehicle classification,motor vehicle registration certificate detection method based on vision,"The invention mainly relates to a visual detection method, in particular to a motor vehicle registration certificate detection method based on vision. The method includes the following steps of 1, image acquisition, wherein a motor vehicle registration certificate image containing the information of a number plate, an engine code, a vehicle identification number (VIN), the registration date, the issue date and a red seal is acquired; 2, information localization, wherein the five pieces of information of the number plate, the engine code, the VIN, the registration date and the issue date on the motor vehicle driving license are localized through image processing; 3, character segmentation, wherein character segmentation is carried out on the number plate, the engine code, the VIN, the registration date and the issue date which are localized in the step 2, and single character segments obtained through segmentation are stored in a classified mode; 4, character recognition, wherein through a trained artificial neural network classifier, single character recognition is carried out on the single character segments obtained through segmentation in the step 3, and the recognition result is output. The method is used for registration certificate information recognition.","1. The vision-based motor vehicle driving license detection method is characterized by comprising the following steps of: (1) and image acquisition: collecting a motor vehicle running license image containing number plate, engine code, vehicle identification number VIN, registration date, certificate issuing date and red seal information; (2) and information positioning: positioning five items of information of a number plate, an engine code, a vehicle identification number VIN, a registration date and a certificate issuing date on a motor vehicle running certificate by utilizing image processing; (3) and character segmentation: respectively carrying out character segmentation on the number plate, the engine code, the vehicle identification number VIN, the registration date and the certificate issuing date positioned in the step (2) and storing the segmented single-character image blocks in a classified manner; (4) and character recognition: and (4) carrying out single character recognition on the single character image block obtained by segmentation in the step (3) by using a trained artificial neural network classifier, and outputting a recognition result. 2. The vision-based vehicle license detection method of claim 1, wherein the image processing for locating the information of the number plate, the engine code and the vehicle identification number VIN on the vehicle license comprises the following steps: (21) gaussian filtering, gray level conversion, self-adaptive threshold binarization, filtering in the horizontal direction of sobel to highlight vertical features and closed operation are carried out on the forward original image to obtain a connected region containing certificate information; (22) and extracting the outline, extracting an external rectangle, screening the external rectangle by using the relative area, the relative height and the central coordinate based on the size of the whole picture to obtain an external rectangle block of the number plate, the engine code and the vehicle identification number VIN, and respectively positioning the three parts. 3. The vision-based vehicle license detection method of claim 1, wherein the image processing for locating the registration date and the certification date information on the vehicle license comprises the steps of: (23) and converting the original RGB color space image into an LAB color space, extracting an A channel image, performing histogram equalization on the A channel image, selecting a higher threshold value for binarization processing, and obtaining an image containing a red seal part on the motor vehicle running license. (24) Analyzing the connected region of the binarized image obtained in the step (23), finding out the connected region with the maximum external rectangle, traversing other connected regions in the image by taking the connected region as a reference, finding out a new connected region with the external rectangle overlapped with the reference connected region, and fusing the new connected region with the reference connected region into a new reference connected region; traversing the rest of the non-overlapped connected regions again, checking whether the connected regions are overlapped with the external rectangles of the new reference connected region, if so, fusing the overlapped connected regions into the new reference connected region, continuously traversing the rest of the connected regions, and performing circulating operation until all the overlapped parts of all the external rectangles are fused together to obtain the position information of the red seal on the motor vehicle license; (25) acquiring an external polygon of the fusion connected region according to the binarized image in the step (23), and judging and selecting an included angle between the rightmost side of the external polygon and the vertical direction, wherein the included angle is less than five degrees and is neglected, and the included angle is higher than the standard and is used as the basis for correcting the image skew; (26) positioning a registration date by using a relative position relationship for red seal position information of the image after the direction is adjusted and the deflection angle obtained in the step (25), taking the height of the red seal as a unit reference, rightwards measuring 3/5 unit reference lengths as the upper left-hand abscissa of the registration date information by taking the rightmost abscissa of the red seal as a reference, upwards measuring 1/3 unit references as the ordinate of the upper left-hand ordinate of the registration date by taking the bottommost ordinate of the red seal as a reference, and respectively taking 3 and 0.4 unit references as the width and height of the registration date, wherein the height of the enlarged intercepted date when the image is inclined is the sine value of the width multiplied by the inclination angle; (27) positioning the date of issuance of certificate by using the relative position relationship for the red seal position information of the image after the direction adjustment and the deflection angle obtained in the step (25), taking the height of the red seal as a unit reference, taking the rightmost abscissa of the red seal as a reference, measuring 3/5 unit reference lengths rightwards as the upper left-hand abscissa of the date of issuance information, taking the rightmost ordinate of the red seal as a reference, measuring 1/3 unit references upwards as the ordinate of the upper left-hand ordinate of the date of issuance of certificate, taking 3 and 0.4 unit references as the width and height of the date of issuance of certificate respectively, and enlarging the height of the date of interception when the image is inclined as the sine value of the width multiplied by the inclination angle. 4. The vision-based vehicle license detection method of claim 3 wherein the number of other connected regions in step (24) does not exceed 20. 5. The vision-based vehicle running license detection method of claim 1, wherein the following steps are included for character segmentation of a number plate, an engine code, a vehicle identification number VIN, a registration date and a certification date on a vehicle running license: (31) according to the number plate, the engine code, the vehicle identification code VIN, the registration date and the certificate issuing date image blocks obtained by positioning, respectively carrying out gray level image conversion and self-adaptive threshold value binaryzation, extracting a contour and an external rectangle, filtering out the external rectangle with the area smaller than the area 1/50 of the segmented image and the height smaller than the segmented image 1/2, and obtaining the residual image blocks which are binary single character image blocks; (32) and (4) respectively sorting the binarized single-character image blocks obtained in the step (31) according to positions, and unifying the size of the image blocks to be 20 pixels by 20 pixels. 6. The vision-based vehicle running license detection method of claim 1, wherein for character recognition of a number plate, an engine code, a vehicle identification number VIN, a registration date and a certification date on a vehicle running license, the steps of: (41) performing feature extraction by using the binaryzation single character image blocks obtained in the steps (31) and (32); (42) and (4) taking the features of the binaryzation single character image block extracted in the step (41) as the input of a neural network, carrying out character recognition by using the trained neural network, and outputting the recognized characters in a character array form after the recognized characters are arranged in sequence. 7. The vision-based vehicle license detection method of claim 6 wherein the extracted features include vertical histograms, horizontal histograms and 5 x 5 low resolution images of single character binarized pictures. 8. The vision-based vehicle license detection method of claim 6 wherein the artificial neural network is a multilayer perceptron, selecting a BP neural network.",CN106156768_B.txt,G06K9/20,{'computing; calculating; counting'},['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers'],"motor vehicle registration certificate detection method based on vision The invention mainly relates to a visual detection method, in particular to a motor vehicle registration certificate detection method based on vision. The method includes the following steps of 1, image acquisition, wherein a motor vehicle registration certificate image containing the information of a number plate, an engine code, a vehicle identification number (VIN), the registration date, the issue date and a red seal is acquired; 2, information localization, wherein the five pieces of information of the number plate, the engine code, the VIN, the registration date and the issue date on the motor vehicle driving license are localized through image processing; 3, character segmentation, wherein character segmentation is carried out on the number plate, the engine code, the VIN, the registration date and the issue date which are localized in the step 2, and single character segments obtained through segmentation are stored in a classified mode; 4, character recognition, wherein through a trained artificial neural network classifier, single character recognition is carried out on the single character segments obtained through segmentation in the step 3, and the recognition result is output. The method is used for registration certificate information recognition.",computing; calculating; counting
US2020174490_A1,2018-07-27,2020-06-04,2017-07-27,WAYMO,"KRIZHEVSKY, ALEXANDEROGALE, ABHIJITBANSAL, MAYANK",70849125,road monitoring,neural networks for vehicle trajectory planning,"Systems, methods, devices, and other techniques for planning a trajectory of a vehicle. A computing system can implement a trajectory planning neural network configured to, at each time step of multiple time steps: obtain a first neural network input and a second neural network input. The first neural network input can characterize a set of waypoints indicated by the waypoint data, and the second neural network input can characterize (a) environmental data that represents a current state of an environment of the vehicle and (b) navigation data that represents a planned navigation route for the vehicle. The trajectory planning neural network may process the first neural network input and the second neural network input to generate a set of output scores, where each output score in the set of output scores corresponds to a different location of a set of possible locations in a vicinity of the vehicle.","1. A computing system for planning a trajectory of a vehicle, the system comprising: a memory configured to store waypoint data indicating one or more waypoints, each waypoint representing a previously traveled location of the vehicle or a location in a planned trajectory for the vehicle; and one or more computers and one or more storage devices storing instructions that when executed cause the one or more computers to implement: a trajectory planning neural network configured to, at each time step of a plurality of time steps: obtain a first neural network input and a second neural network input, wherein (i) the first neural network input characterizes a set of waypoints indicated by the waypoint data, and (ii) the second neural network input characterizes (a) environmental data that represents a current state of an environment of the vehicle and (b) navigation data that represents a planned navigation route for the vehicle; and process the first neural network input and the second neural network input to generate a set of output scores, wherein each output score in the set of output scores corresponds to a different location of a set of possible locations in a vicinity of the vehicle and indicates a likelihood that the respective location is an optimal location for a next waypoint in the planned trajectory for the vehicle to follow the planned navigation route; and a trajectory management system configured to, at each time step of the plurality of time steps: select, based on the set of output scores generated by the trajectory planning neural network at the time step, one of the set of possible locations as the waypoint for the planned trajectory of the vehicle at the time step; and update the waypoint data by writing to the memory an indication of the selected one of the set of possible locations as the waypoint for the planned trajectory of the vehicle at the time step.2. The computing system of claim 1, wherein for an initial time step of the plurality of time steps, the set of waypoints characterized by the first neural network input includes at least one waypoint that represents a previously traveled location of the vehicle at a time that precedes the plurality of time steps.3. The computing system of claim 2, wherein for each time step of the plurality of time steps after the initial time step, the set of waypoints characterized by the first neural network input includes the waypoints that were determined at each preceding time step of the plurality of time steps.4. The computing system of claim 2, wherein for at least one time step of the plurality of time steps after the initial time step, the set of waypoints characterized by the first neural network input includes: (i) one or more first waypoints that represent previously traveled locations of the vehicle at times that precede the plurality of time steps, and (ii) one or more second waypoints that were determined at preceding time steps of the plurality of time steps.5. The computing system of claim 1, wherein the trajectory planning neural network is a feedforward neural network.6. The computing system of claim 1, wherein the second neural network input at each time step of the plurality of time steps characterizes the environmental data that represents the current state of the environment of the vehicle at the time step.7. The computing system of claim 6, wherein: the second neural network input characterizes multiple channels of environmental data, and the multiple channels of environmental data include two or more of roadgraph data representing one or more roads in the vicinity of the vehicle, perception object data representing locations of objects that have been detected as being in the vicinity of the vehicle, speed limit data representing speed limits associated with the one or more roads in the vicinity of the vehicle, light detection and ranging (LIDAR) data representing a LIDAR image of the vicinity of the vehicle, radio detection and ranging (RADAR) data representing a RADAR image of the vicinity of the vehicle, camera data representing an optical image of the vicinity of the vehicle, or traffic artifacts data representing identified traffic artifacts in the vicinity of the vehicle.8. The computing system of claim 1, wherein the second neural network input at each time step of the plurality of time steps characterizes the navigation data that represents the planned navigation route for the vehicle.9. The computing system of claim 1, wherein the second neural network input at each time step of the plurality of time steps characterizes both the environmental data that represents the current state of the environment of the vehicle at the time step and the navigation data that represents the planned navigation route for the vehicle.10. The computing system of claim 1, wherein each successive pair of time steps in the plurality of time steps represents a successive pair of real-world times that are separated by a fixed interval in the range 100 milliseconds to 500 milliseconds.11. The computing system of claim 1, further comprising a vehicle control subsystem configured to determine control actions for the vehicle to take to cause the vehicle to maneuver along a planned trajectory defined by the waypoints for at least some of the plurality of time steps.12. The computing system of claim 11, further comprising maneuvering the vehicle along the planned trajectory as a result of executing at least some of the control actions determined by the vehicle control subsystem, wherein the control actions include at least one of steering, braking, or accelerating the vehicle.13. A computer-implemented method for planning a trajectory of a vehicle, the method comprising: for each time step in a series of time steps: obtaining a first neural network input that characterizes a set of waypoints that each represent a previous location of the vehicle or a location in a planned trajectory for the vehicle; obtaining a second neural network input that characterizes (i) environmental data that represents a current state of an environment of the vehicle and (ii) navigation data that represents a planned navigation route for the vehicle; providing the first neural network input and the second neural network input to a trajectory planning neural network and, in response, obtaining a set of output scores from the trajectory planning neural network, each output score corresponding to a respective location of a set of possible locations in a vicinity of the vehicle and indicating a likelihood that the respective location is an optimal location for a next waypoint in the planned trajectory for the vehicle to follow the planned navigation route; and selecting, based on the set of output scores, one of the possible locations in the vicinity of the vehicle as a waypoint for the planned trajectory of the vehicle at the time step.14. The method of claim 13, wherein, for each time step in the series of time steps after an initial time step, the set of waypoints characterized by the first neural network input at the time step represents the locations of the vehicle that were selected at each preceding time step in the series of time steps.15. The method of claim 14, wherein the set of waypoints characterized by the first neural network input at the initial time step represents locations that the vehicle has traversed at particular times that precede the series of time steps.16. The method of claim 13, wherein for at least one of the series of time steps: the second neural network input characterizes multiple channels of environmental data, and the multiple channels of environmental data include two or more of roadgraph data representing one or more roads in the vicinity of the vehicle, perception object data representing locations of objects that have been detected as being in the vicinity of the vehicle, speed limit data representing speed limits associated with the one or more roads in the vicinity of the vehicle, light detection and ranging (LIDAR) data representing a LIDAR image of the vicinity of the vehicle, radio detection and ranging (RADAR) data representing a RADAR image of the vicinity of the vehicle, camera data representing an optical image of the vicinity of the vehicle, or traffic artifacts data representing identified traffic artifacts in the vicinity of the vehicle.17. The method of claim 13, further comprising, at each time step in the series of time steps, writing an indication in memory of the selected one of the possible locations in the vicinity of the vehicle as the waypoint for the planned trajectory of the vehicle at the time step.18. The method of claim 13, further comprising determining control actions for the vehicle to take to cause the vehicle to maneuver along a planned trajectory defined by the waypoints for at least some of the series of time steps.19. The method of claim 13, wherein each successive pair of time steps in the series of time steps represents a successive pair of real-world times that are separated by a fixed interval in the range 100 milliseconds to 500 milliseconds.20. One or more non-transitory computer-readable media having instructions stored thereon that, when executed by data processing apparatus, cause the data processing apparatus to perform operations comprising: for each time step in a series of time steps: obtaining a first neural network input that characterizes a set of waypoints that each represent a previous location of the vehicle or a location in a planned trajectory for the vehicle; obtaining a second neural network input that characterizes at least one of (i) environmental data that represents a current state of an environment of the vehicle or (ii) navigation data that represents a planned navigation route for the vehicle; providing the first neural network input and the second neural network input to a trajectory planning neural network and, in response, obtaining a set of output scores from the trajectory planning neural network, each output score corresponding to a respective location of a set of possible locations in a vicinity of the vehicle and indicating a likelihood that the respective location is an optimal location for a next waypoint in the planned trajectory for the vehicle to follow the planned navigation route; and selecting, based on the set of output scores, one of the possible locations in the vicinity of the vehicle as a waypoint for the planned trajectory of the vehicle at the time step.21. 21-44. (canceled)",US2020174490_A1.txt,"G05D1/02,G06N3/04","{'controlling; regulating', 'computing; calculating; counting'}","['systems for controlling or regulating non-electric variables (for continuous casting of metals b22d11/16; valves per se f16k; sensing non-electric variables, see the relevant subclasses of g01; for regulating electric or magnetic variables g05f)', 'computing arrangements based on specific computational models']","neural networks for vehicle trajectory planning Systems, methods, devices, and other techniques for planning a trajectory of a vehicle. A computing system can implement a trajectory planning neural network configured to, at each time step of multiple time steps: obtain a first neural network input and a second neural network input. The first neural network input can characterize a set of waypoints indicated by the waypoint data, and the second neural network input can characterize (a) environmental data that represents a current state of an environment of the vehicle and (b) navigation data that represents a planned navigation route for the vehicle. The trajectory planning neural network may process the first neural network input and the second neural network input to generate a set of output scores, where each output score in the set of output scores corresponds to a different location of a set of possible locations in a vicinity of the vehicle.",controlling; regulating computing; calculating; counting
US2020293822_A1,2020-03-24,2020-09-17,2017-07-05,Perceptive Automata Inc.,"ANTHONY, SAMUEL ENGLISHFALLER, AVERY WAGNERMISRA, KSHITIJ",64903265,road monitoring,system and method of predicting human interaction with vehicles,"Systems and methods for predicting user interaction with vehicles. A computing device receives an image and a video segment of a road scene, the first at least one of an image and a video segment being taken from a perspective of a participant in the road scene and then generates stimulus data based on the image and the video segment. Stimulus data is transmitted to a user interface and response data is received, which includes at least one of an action and a likelihood of the action corresponding to another participant in the road scene. The computing device aggregates a subset of the plurality of response data to form statistical data and a model is created based on the statistical data. The model is applied to another image or video segment and a prediction of user behavior in the another image or video segment is generated.","1. (canceled)2. A computer-implemented method for controlling an autonomous vehicle based on predicted state of mind of users in a scene captured by a camera of the autonomous vehicle, the method comprising: receiving a plurality of images displaying road scenes captured by one or more vehicles; receiving a plurality of user responses, each user response describing a state of mind of a road user displayed in one or more images; generating training dataset comprising summary statistics of uses responses describing the state of minds of road users displayed in the plurality of images; training, using the training dataset, a supervised learning based model configured to predict summary statistics describing a state of mind of a road user displayed in an input image; receiving, by an autonomous vehicle, a new image captured by a camera of the autonomous vehicle, the new image of a scene including a road user; predicting, by the autonomous vehicle, using the supervised learning based model, summary statistics describing a state of mind of the road user in the new image; controlling the autonomous vehicle based on the prediction of the supervised learning based model.3. The computer-implemented method of claim 2, wherein the road user in the input image is a pedestrian and the state of mind predicted by the supervised learning based model indicates whether the pedestrian is likely to perform an action, the action including one of: staying in place, or crossing a street.4. The computer-implemented method of claim 2, wherein the road user in the input image is a cyclist and the state of mind predicted by the supervised learning based model indicates whether the cyclist is likely to cross a lane.5. The computer-implemented method of claim 2, wherein the statistical summary comprises one ore more of: a central tendency, a variance, a skew, a kurtosis, a scale, or a histogram.6. The computer-implemented method of claim 2, wherein receiving a user response comprises: sending one or more images for display to a viewer via a user interface, the one or more images including a road user, wherein the user interface is configured to prompt the viewer to describe how the road user will act.7. The computer-implemented method of claim 2, wherein receiving a user response comprises: generating a stimulus comprising a set of images; sending the generated stimulus for display via a user interface; and receiving, via the user interface, a user response describing the stimulus.8. The computer-implemented method of claim 7, further comprising: modifying an image to generate a stimulus, the modifying comprising extracting a subset of the image contained in a bounding box around the road user displayed in the image.9. The computer-implemented method of claim 8, wherein the supervised learning based model uses one or more features comprising: dimensions of the bounding box; location of the bounding box; shape of the bounding box; or a change in size or position of the bounding box across two images representing video frames.10. The computer-implemented method of claim 2, wherein the user response is determined based on: an explicit response to a question asked via the user interface; and an implicit data representing one or more of: time taken to respond; whether user deleted keystrokes; or whether user moved the mouse anywhere other than the location corresponding to the selected response.11. The computer-implemented method of claim 2, wherein the supervised learning based model is one of: a random forest regressor, a support vector regressor, a simple neural network, a deep convolutional neural network, a recurrent neural network, or a long-short-term memory (LSTM) neural network.12. A non-transitory computer readable storage medium storing instructions that when executed by a computer processor cause the computer processer to perform steps of a computer-implemented method for controlling an autonomous vehicle based on predicted state of mind of users in a scene captured by a camera of the autonomous vehicle, the steps comprising: receiving a plurality of images displaying road scenes captured by one or more vehicles; receiving a plurality of user responses, each user response describing a state of mind of a road user displayed in one or more images; generating training dataset comprising summary statistics of uses responses describing the state of minds of road users displayed in the plurality of images; training, using the training dataset, a supervised learning based model configured to predict summary statistics describing a state of mind of a road user displayed in an input image; receiving, by an autonomous vehicle, a new image captured by a camera of the autonomous vehicle, the new image of a scene including a road user; predicting, by the autonomous vehicle, using the supervised learning based model, summary statistics describing a state of mind of the road user in the new image; controlling the autonomous vehicle based on the prediction of the supervised learning based model.13. The non-transitory computer readable storage medium of claim 12, wherein the road user in the input image is a pedestrian and the state of mind predicted by the supervised learning based model indicates whether the pedestrian is likely to perform an action, the action including one of: staying in place, or crossing a street.14. The non-transitory computer readable storage medium of claim 12, wherein the road user in the input image is a cyclist and the state of mind predicted by the supervised learning based model indicates whether the cyclist is likely to cross a lane.15. The non-transitory computer readable storage medium of claim 12, wherein the statistical summary comprises one or more of: a central tendency, a variance, a skew, a kurtosis, a scale, or a histogram.16. The non-transitory computer readable storage medium of claim 12, wherein receiving a user response comprises: sending one or more images for display to a viewer via a user interface, the one or more images including a road user, wherein the user interface is configured to prompt the viewer to describe how the road user will act.17. The non-transitory computer readable storage medium of claim 12, wherein receiving a user response comprises: generating a stimulus comprising a set of images; sending the generated stimulus for display via a user interface; and receiving, via the user interface, a user response describing the stimulus.18. The non-transitory computer readable storage medium of claim 17, further comprising: modifying an image to generate a stimulus, the modifying comprising extracting a subset of the image contained in a bounding box around the road user displayed in the image.19. The non-transitory computer readable storage medium of claim 18, wherein the supervised learning based model uses one or more features comprising: dimensions of the bounding box; location of the bounding box; shape of the bounding box; or a change in size or position of the bounding box across two images representing video frames.20. The non-transitory computer readable storage medium of claim 12, wherein the user response is determined based on: an explicit response to a question asked via the user interface; and an implicit data representing one or more of: time taken to respond; whether user deleted keystrokes; or whether user moved the mouse anywhere other than the location corresponding to the selected response.21. A computer system comprising: a computer processor; and a non-transitory computer readable storage medium storing instructions that when executed by a computer processor cause the computer processer to perform steps of a computer-implemented method for controlling an autonomous vehicle based on predicted state of mind of users in a scene captured by a camera of the autonomous vehicle, the steps comprising: receiving a plurality of images displaying road scenes captured by one or more vehicles; receiving a plurality of user responses, each user response describing a state of mind of a road user displayed in one or more images; generating training dataset comprising summary statistics of uses responses describing the state of minds of road users displayed in the plurality of images; training, using the training dataset, a supervised learning based model configured to predict summary statistics describing a state of mind of a road user displayed in an input image; receiving, by an autonomous vehicle, a new image captured by a camera of the autonomous vehicle, the new image of a scene including a road user; predicting, by the autonomous vehicle, using the supervised learning based model, summary statistics describing a state of mind of the road user in the new image; controlling the autonomous vehicle based on the prediction of the supervised learning based model.",US2020293822_A1.txt,"B60W30/00,G05D1/00,G06K9/00,G06K9/62,G06N3/04,G06N3/08,G08G1/04,G08G1/16","{'controlling; regulating', 'signalling', 'computing; calculating; counting', 'vehicles in general'}","['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'systems for controlling or regulating non-electric variables (for continuous casting of metals b22d11/16; valves per se f16k; sensing non-electric variables, see the relevant subclasses of g01; for regulating electric or magnetic variables g05f)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})']","system and method of predicting human interaction with vehicles Systems and methods for predicting user interaction with vehicles. A computing device receives an image and a video segment of a road scene, the first at least one of an image and a video segment being taken from a perspective of a participant in the road scene and then generates stimulus data based on the image and the video segment. Stimulus data is transmitted to a user interface and response data is received, which includes at least one of an action and a likelihood of the action corresponding to another participant in the road scene. The computing device aggregates a subset of the plurality of response data to form statistical data and a model is created based on the statistical data. The model is applied to another image or video segment and a prediction of user behavior in the another image or video segment is generated.",controlling; regulating signalling computing; calculating; counting vehicles in general
US10339391_B2,2016-08-24,2019-07-02,2016-08-24,GM GLOBAL TECHNOLOGY OPERATIONS,"LITKOUHI, BAKHTIAR B.WANG JINSONGZHANG, QIZHAO, QINGRONG",61242978,road monitoring,fusion-based wet road surface detection,"A method for determining wetness on a path of travel. A surface of the path of travel is captured by at least one image capture device. A plurality of wet surface detection techniques is applied to the at least one image. An analysis for each wet surface detection technique is determined in real-time of whether the surface of the path of travel is wet. Each analysis independently determines whether the path of travel is wet. Each analysis by each wet surface detection technique is input to a fusion and decision making module. Each analysis determined by each wet surface detection technique is weighted within the fusion and decision making module by comprehensive analysis of weather information, geology information, and vehicle motions. A wet surface detection signal is provided to a control device. The control device applies the wet surface detection signal to mitigate the wet surface condition.","1. A method for determining wetness on a path of travel of a motor vehicle, the method comprising: capturing an image of a surface of the path of travel by an image capture device, the image capture device focusing at the surface where water is expected as the motor vehicle travels along the path of travel; applying a plurality of wet surface detection techniques, by a processor, to the image, each of the wet surface detection techniques independently assessing whether the path of travel is wet or not wet, the plurality of wet surface detection techniques including a rearward tire splash analysis technique, a side tire splash analysis technique, a tire track analysis technique, and/or a mirrored light image analysis technique; determining, in real-time by the processor, a respective analysis result for each of the wet surface detection techniques indicating the surface of the path of travel is wet or not wet; inputting the analysis results determined by the wet surface detection techniques into a fusion and decision-making module; weighting the analysis results input into the fusion and decision-making module using a condition assessment module, the weighting including dynamically determining and applying a respective weight factor to each of the analysis results determined by each of the wet surface detection techniques within the fusion and decision-making module; fusing the weighted analysis results using the fusion and decision-making module, the fusing including calculating a summation of the weighted analysis results and determining if the summation of the weighted analysis results exceeds a predetermined threshold; and providing, responsive to the summation of the weighted analysis results exceeding the predetermined threshold, a wet surface detection signal indicating a wet surface condition on the path of travel to a control device of the motor vehicle, the control device applying the wet surface detection signal to mitigate the wet surface condition.2. The method of claim 1, wherein the rearward tire splash analysis technique includes detecting within the captured image a rearward tire splash of water off of the path of travel.3. The method of claim 1, wherein the side tire splash analysis technique includes detecting within the captured image a side tire splash of water off of the path of travel.4. The method of claim 1, wherein the tire track analysis technique includes detecting within the captured image a tire track generated by a tire of the motor vehicle on the path of travel when the surface is wet.5. The method of claim 1, wherein the mirrored light image analysis technique includes detecting within the captured image a mirrored surface where ice or water is present on the path of travel.6. The method of claim 1, wherein each of the analysis results determined by each of the wet surface detection techniques is normalized to represent one of a wet surface or a non-wet surface.7. The method of claim 1, wherein each of the weigh factors is determined as a function of rain condition data, path of travel topology data, and vehicle speed data.8. The method of claim 7, wherein the weighting as determined by the condition assessment module includes determining a water depth level on the path of travel as a function of the rain condition data and the road topology data.9. The method of claim 8, wherein the weighting as determined by the condition assessment module includes determining the respective weight factor for each of the respective wet surface detection techniques as a function of the water depth level on the path of travel and the vehicle speed.10. The method of claim 8, wherein the condition assessment module utilizes a Neural Network to determine the water depth level on the path of travel and the weight factors.11. The method of claim 8, wherein the condition assessment module utilizes a Bayesian Network to determine the water depth level on the path of travel and the weight factors.12. The method of claim 8, wherein the condition assessment module utilizes logic deduction to determine the water depth level on the path of travel.13. The method of claim 9, wherein the respective weight factors are dynamically determined by the following equation:description=""In-line Formulae"" end=""lead""?W=P(Ti|D,V)description=""In-line Formulae"" end=""tail""? where W is a weight factor, P is a conditional probability, Ti is a respective wet road surface detection technique, D is the water depth level, and V is the velocity of the vehicle.14. The method of claim 13, wherein fusing the weighted analysis results using the fusion and decision-making module includes calculating a voting number as follows:description=""In-line Formulae"" end=""lead""?Voting number=w1*T1+w2*T2+w3*T3+w4*T4description=""In-line Formulae"" end=""tail""? where Ti (I=1, 2, 3, 4) is the respective analysis results for one of the wet surface detection techniques and takes on either 1 (wet) or 0 (non-wet); and wi (I=1, 2, 3, 4) is the respective weight factor for one of the respective analysis results and takes on a corresponding percentage value.15. The method of claim 1, wherein the control device uses the wet surface detection signal to determine a vehicle braking strategy to autonomously actuate a braking system of the motor vehicle.16. The method of claim 1, wherein the control device uses the wet surface detection signal to determine a traction control strategy to autonomously actuate a traction control system of the motor vehicle.17. The method of claim 1, further comprising transmitting the wet surface detection signal to a wireless communication system to alert other vehicles of the wet surface condition on the path of travel.18. The method of claim 1, wherein the control device uses the wet surface detection signal to alert a driver of the motor vehicle of a potential reduced traction between vehicle tires of the motor vehicle and the surface as a result of the identified wet surface condition on the path of travel.19. The method of claim 1, wherein the control device uses the wet surface detection signal to alert a driver of the motor vehicle against a use of a driver assistance system of the motor vehicle.20. The method of claim 1, wherein the control device uses the wet surface detection signal to autonomously modify a control setting of an automated control feature of the motor vehicle in response to the identified wet surface condition on the path of travel.21. The method of claim 1, wherein the control device uses the wet surface detection signal to alert a driver of the motor vehicle to reduce a vehicle speed of the motor vehicle in response to the identified wet surface condition on the path of travel.22. The method of claim 1, wherein the control device uses the wet surface detection signal to shut baffles on an air intake scoop of the motor vehicle for preventing water ingestion in response to the identified wet surface condition on the path of travel.23. A motor vehicle comprising: a vehicle body; a plurality of wheels rotatably mounted to the vehicle body; an image capture device mounted to the vehicle body; and a processor mounted to the vehicle body and communicatively connected to the image capture device, the processor being programmed to: capture, via the image capture device, an image of a surface of a path of travel of the motor vehicle; apply to the captured image a plurality of wet surface detection techniques each operable to independently assess whether the surface of the path of travel is wet or not wet, the wet surface detection techniques including at least two of a rearward tire splash analysis technique, a side tire splash analysis technique, a tire track analysis technique, or a mirrored light image analysis technique; determine, in real-time for each of the applied wet surface detection techniques, a respective analysis result indicating the surface is wet or not wet; input the determined analysis results of the wet surface detection techniques into a fusion and decision-making module; weight, via a condition assessment module, the analysis results of the wet surface detection techniques, including dynamically determining and applying a respective weight factor to each of the analysis results; fuse, via the fusion and decision-making module, the weighted analysis results, including calculating a summation of the weighted analysis results and determining if the summation of the weighted analysis results exceeds a predetermined threshold; and responsive to the summation of the weighted analysis results exceeding the predetermined threshold, commanding a control device of the motor vehicle to mitigate a wet surface condition on the surface of the path of travel based on a wet surface detection signal indicative of the wet surface condition.",US10339391_B2.txt,"B60W40/06,G06K9/00,G06K9/20,G06K9/46","{'computing; calculating; counting', 'vehicles in general'}","['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']","fusion-based wet road surface detection A method for determining wetness on a path of travel. A surface of the path of travel is captured by at least one image capture device. A plurality of wet surface detection techniques is applied to the at least one image. An analysis for each wet surface detection technique is determined in real-time of whether the surface of the path of travel is wet. Each analysis independently determines whether the path of travel is wet. Each analysis by each wet surface detection technique is input to a fusion and decision making module. Each analysis determined by each wet surface detection technique is weighted within the fusion and decision making module by comprehensive analysis of weather information, geology information, and vehicle motions. A wet surface detection signal is provided to a control device. The control device applies the wet surface detection signal to mitigate the wet surface condition.",computing; calculating; counting vehicles in general
US10082795_B2,2016-08-24,2018-09-25,2016-08-24,GM GLOBAL TECHNOLOGY OPERATIONS,"LITKOUHI, BAKHTIAR B.WANG JINSONGZHANG, QIZHAO, QINGRONG",61242424,road monitoring,vision-based on-board real-time estimation of water film thickness,A method for determining a thickness of water on a path of travel. A plurality of images of a surface of the path of travel is captured by an image capture device over a predetermined sampling period. A plurality of wet surface detection techniques are applied to each of the images. A detection rate is determined in real-time for each wet surface detection technique. A detection rate trigger condition is determined as a function of a velocity of the vehicle for each detection rate. The real-time determined detection rate trigger conditions are compared to predetermined detection rate trigger conditions in a classification module to identify matching results pattern. A water film thickness associated with the matching results pattern is identified in the classification module. A water film thickness signal is provided to a control device. The control device applies the water film thickness signal to mitigate the wet surface condition.,"1. A method for determining a thickness of water on a path of travel of a vehicle having a subsystem, the subsystem including air baffles on an air intake scoop of the vehicle, a traction control system, a cruise or adaptive cruise control system, a steering assist system, and/or a brake system, the method comprising: capturing a plurality of images of a surface of the path of travel by an image capture device over a predetermined sampling period, the image capture device being focused at the surface where water splash is expected as the vehicle travels along the path of travel; applying a plurality of wet surface detection techniques, by a processor of a controller of the vehicle, to each of the images; determining in real-time, by the processor, a respective detection rate for each of the wet surface detection techniques; determining, by the processor, a respective detection rate trigger condition as a function of a velocity of the vehicle for each of the respective detection rates; comparing each of the respective detection rate trigger conditions to predetermined detection rate trigger conditions in a classification module of the controller to identify a matching results pattern, the predetermined detection rate trigger conditions representing various water film thickness levels; identifying a water film thickness associated with the matching results pattern in the classification module; providing a water film thickness signal to the controller indicative of the identified water film thickness associated with the matching results pattern; and in response to receipt of the water film thickness signal, executing a control action of the subsystem aboard the vehicle, via the controller, to thereby mitigate an effect of the wet surface condition on operation of the vehicle; wherein executing a control action includes shutting the air baffles, actuating the traction control system, disabling the cruise or adaptive cruise control system, disabling the steering assist system, and/or applying brakes of the brake system.2. The method of claim 1 wherein the plurality of wet surface detection techniques includes at least a rearward tire splash analysis technique.3. The method of claim 1 wherein the plurality of wet surface techniques includes at least a side tire splash analysis technique.4. The method of claim 1 wherein the plurality of wet surface techniques includes at least a tire track analysis technique.5. The method of claim 1 wherein the plurality of wet surface techniques includes at least a mirrored light image analysis technique.6. The method of claim 1 wherein the detection rate for each applied wet surface detection technique is determined using the following formula: where the sample numbers of wet road decision represents a number of samples that positively identifies water on the surface by a respective technique during a window period, N represents the total number of images sampled for that respective technique during the window period, and Ti represents the respective technique used to identify the surface condition.7. The method of claim 1 wherein each detection rate trigger condition is determined as function of comparing each detection rate to a respective predetermined velocity-based threshold value.8. The method of claim 7 wherein each detection rate trigger condition is normalized.9. The method of claim 8 wherein each normalized detection rate trigger condition represents a degree of certainty of water present on the surface.10. The method of claim 7 wherein utilizing the classification module includes utilizing a Dempster-Shafer Evidential theory.11. The method of claim 7 wherein utilizing the classification module includes utilizing a Bayesian Network theory.12. The method of claim 7 wherein utilizing the classification module includes utilizing a lookup table.13. The method of claim 12 wherein the normalized detection rate trigger condition includes one of a positive determination of water present on the surface, a positive indication of water present on the surface, an uncertainty of whether water is present on the surface, and no detection of water on the surface, wherein the positive determination of water present on the surface is representative of the detection rate substantially equal to a value of 1, wherein the positive indication of water present on the road surface is representative of the detection rate being greater than the velocity-based threshold but less than 1, the uncertainty of water present on the surface is representative of the detection rate being less than the velocity-based threshold but greater than 0, and no detection of water on the surface is represented by the detection rate substantially equal to 0.14. The method of claim 13 wherein the lookup table is generated in an offline process and is stored in a vehicle memory.15. The method of claim 14 wherein the lookup table includes groups of detection rate trigger conditions, wherein each group includes a respective detection rate trigger condition from each wet surface technique.16. The method of claim 15 wherein each group in the lookup table is associated with a respective water film thickness range.17. The method of claim 1 wherein the control action includes using the identified water film thickness to determine and execute an autonomous actuating vehicle braking strategy aboard the vehicle by applying the brakes of the brake system at a braking force sufficient for removing water from the brakes.18. The method of claim 1 wherein the subsystem is the traction control system of the vehicle, and the control action includes autonomously actuating the traction control system.19. The method of claim 1 wherein the subsystem is a wireless communication system of the vehicle, the method further comprising providing the identified water film thickness signal to the wireless communication system to thereby alert other vehicles of the identified water film thickness on the surface.20. The method of claim 1 further comprising using the identified water film thickness signal alerts to alert a driver of the vehicle of a potential reduced traction between vehicle tires and the surface as a result of the identified water film thickness or against a use of a driver assistance system.21. The method of claim 1 further comprising using the identified water film thickness signal to alert a driver of the vehicle against a use of a driver assistance system.22. The method of claim 1 wherein the subsystem includes the cruise control system, and the control action includes autonomously disabling the cruise control system in response to the identified water film thickness.23. The method of claim 1 further comprising using the identified water film thickness signal to alert a driver of the vehicle to reduce a vehicle speed in response to the identified water film thickness.24. The method of claim 1 wherein the subsystem includes the air baffles, and the control action includes shutting the air baffles on the air intake scoop of the vehicle in response to the identified water film thickness to thereby prevent water ingestion into an engine of the vehicle.25. A method for determining a thickness of water on a path of travel of a vehicle having air baffles on an air intake scoop to an engine of the vehicle, the method comprising: capturing a plurality of images of a surface of the path of travel by an image capture device over a predetermined sampling period, the image capture device being focused at the surface where water splash is expected as the vehicle travels along the path of travel; applying a plurality of wet surface detection techniques, by a processor of a controller of the vehicle, to each of the images; determining in real-time, by the processor, a respective detection rate for each of the wet surface detection techniques; determining, by the processor, a respective detection rate trigger condition as a function of a velocity of the vehicle for each of the respective detection rates; comparing each of the respective detection rate trigger conditions to predetermined detection rate trigger conditions in a classification module of the controller to identify a matching results pattern, the predetermined detection rate trigger conditions representing various water film thickness levels; identifying a water film thickness associated with the matching results pattern in the classification module; providing a water film thickness signal to the controller indicative of the identified water film thickness associated with the matching results pattern; and in response to receipt of the water film thickness signal by the controller, autonomously shutting the air baffles.",US10082795_B2.txt,"G01B11/06,G05D1/02,G06K9/00,G06K9/78","{'measuring; testing', 'controlling; regulating', 'computing; calculating; counting'}","['measuring length, thickness or similar linear dimensions; measuring angles; measuring areas; measuring irregularities of surfaces or contours', 'systems for controlling or regulating non-electric variables (for continuous casting of metals b22d11/16; valves per se f16k; sensing non-electric variables, see the relevant subclasses of g01; for regulating electric or magnetic variables g05f)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']",vision-based on-board real-time estimation of water film thickness A method for determining a thickness of water on a path of travel. A plurality of images of a surface of the path of travel is captured by an image capture device over a predetermined sampling period. A plurality of wet surface detection techniques are applied to each of the images. A detection rate is determined in real-time for each wet surface detection technique. A detection rate trigger condition is determined as a function of a velocity of the vehicle for each detection rate. The real-time determined detection rate trigger conditions are compared to predetermined detection rate trigger conditions in a classification module to identify matching results pattern. A water film thickness associated with the matching results pattern is identified in the classification module. A water film thickness signal is provided to a control device. The control device applies the water film thickness signal to mitigate the wet surface condition.,measuring; testing controlling; regulating computing; calculating; counting
CN109444912_A,2018-10-31,2019-03-08,2018-10-31,UNIVERSITY OF ELECTRONIC SCIENCE AND TECHNOLOGY OF CHINA,CHEN MINGLI WEIMA HUMINPENG LINGBINGZHANG PENGFEIPENG ZHENMINGZHAO XUEGONGYANG CHUNPINGZHANG MINGZHU,65550017,road monitoring,driving environment sensing system and method based on cooperative control and deep learning,"The invention discloses a driving environment sensing system and method based on cooperative control and deep learning, and relates to the technical field of intelligent transportation. The system comprises a laser transmitting module, an optical receiving module, a laser ranging module, a main controller, a cooperative control module and a data processing center. The laser transmitting module covers an area to be monitored, and visible light and infrared video images are collected by the optical receiving module; the cooperative control module processes the data returned by the a multi-sensorto complete the high-precision speed measurement and ranging measurement of a moving object and control the data acquisition of the multi-sensor and communication with the data processing center at the same time; and the data processing center performs three-dimensional reconstruction of the target, and then transmits the three-dimensional data to the deep learning network to perform target classification and identification, and finally locates and tracks the target in real time in the monitoring area to achieve the perception and monitoring of the driving environment. The driving environmentsensing system and method overcome the defects of the conventional system with low resolution and short detection distance, and have the characteristics of all-weather, long-distance and high-precision measurement.","1. A driving environment perception system based on cooperative control and deep learning is characterized by comprising: the laser emission module is used for emitting infrared laser beams to the monitoring area; the laser ranging module (2) is used for ranging the automobile through the received laser; the optical receiving module is used for receiving infrared light and visible light in a monitoring area; the optical receiving module comprises a receiving antenna (6), a beam splitter (7), a narrow-band filter (9), a visible light camera (8) and an infrared camera (10), the receiving antenna (6) transmits light to the beam splitter (7), the beam splitter (7) transmits the light to the visible light camera (8) and the narrow-band filter (9) respectively, and the narrow-band filter (9) transmits infrared light with the central wavelength of 905/940/980nm and transmits the infrared light to the infrared camera (10); the main controller is used for realizing on-off control on the laser emitting module, the optical receiving module and the laser ranging module (2) and feeding the state of the sensor back to the data processing center; the cooperative control module is used for realizing high-precision distance measurement and speed measurement of a traffic monitoring object through a VS + LiDAR + GPS multi-sensor cooperative working mode, a multifunctional traffic monitoring system synchronous control technology and a multi-information fusion and decision technology, transmitting a processing result to the data processing center, and simultaneously transmitting visible light and infrared image data acquired by the optical receiving module to the data processing center; the data processing center is used for realizing the monitoring functions of license plate recognition, lane recognition, distance measurement and speed measurement through an image processing algorithm by using the information transmitted in the main controller and the cooperative control module; the image processing algorithm comprises a kernel correlation filtering algorithm and a target classification recognition algorithm based on deep learning. 2. The driving environment perception system based on cooperative control and deep learning as claimed in claim 1, wherein the laser emitting module includes a continuous wave semiconductor laser (4) with a wavelength of 905/940/980nm, a beam expander and a field stop (5), wherein the beam expander expands the laser to achieve global coverage of the monitored area. 3. A driving environment perception method based on cooperative control and deep learning is characterized by comprising the following steps: the laser emission module and the laser ranging module perform full coverage scanning on the monitored area, and transmit the acquired ranging data set to the data processing center for processing through the cooperative control module; the visible light camera (8) and the infrared camera (9) simultaneously acquire visible light data and infrared light data of a monitored area, and then transmit the acquired data to the data processing center for processing through the cooperative control module; the data processing center generates laser point cloud by using the infrared light data and the distance data set, and performs three-dimensional reconstruction by combining with the visible light data; and the data processing center classifies and identifies the three-dimensional reconstruction data of the target by using the trained deep learning network, and finally tracks the position of the target in the monitoring area by using a kernel correlation filtering method. 4. The driving environment perception method based on cooperative control and deep learning as claimed in claim 3, wherein the step of training the deep convolutional neural network model comprises: preprocessing the target three-dimensional reconstruction data; extracting three-dimensional projection views of a plurality of pieces of target three-dimensional reconstruction data, combining and converting the three-dimensional projection views into an input form suitable for a deep convolutional neural network to serve as a data set; marking the target three-dimensional reconstruction data according to the target category, randomly extracting half data as a training set, and taking the rest data as a test set; constructing a deep convolutional neural network, outputting probability distribution, and evaluating network performance by adopting cross error entropy as a loss function to find an optimal weight value; in the training process, the gradient of the neural network is calculated by an error back propagation method, an Adam method is used as an updating strategy of the weight value of the neural network, a Dropout method is adopted to randomly delete neurons to inhibit overfitting in each training, and finally the neural network model for driving environment target perception is obtained. 5. The driving environment perception method based on cooperative control and deep learning as claimed in claim 4, wherein in the step of constructing the deep convolutional neural network, the convolutional neural network is composed of three layers of convolutional units, two fully connected units and a softmax output layer, each convolutional unit comprises a convolutional layer, a ReLU layer and a pooling layer to extract a high-dimensional feature map of data, the fully connected units comprise an affine transformation layer and a ReLU layer, each layer uses the output of the previous layer as the input of the current layer, and finally connects the softmax layer. 6. The driving environment perception method based on cooperative control and deep learning as claimed in claim 3, wherein the step of performing classification recognition on the target three-dimensional reconstruction data by the data processing center using the trained deep learning network includes: converting the target three-dimensional model data into a data input form suitable for a deep convolutional neural network model; inputting the converted data into a deep convolutional neural network for forward propagation calculation; and outputting probability distribution through the softmax layer, and sequencing to obtain a target class corresponding to the maximum probability, namely the identified target result. 7. The driving environment perception method based on cooperative control and deep learning according to claim 3, wherein the three-dimensional reconstruction includes: importing laser point cloud data, and splicing the point cloud data by utilizing an ICP (inductively coupled plasma) algorithm; establishing a triangular grid and repairing; and selecting image points and space points in the visible light data by utilizing the acquired visible light data and space back intersection, and then performing central projection to complete texture mapping to finally obtain the three-dimensional model with the texture.",CN109444912_A.txt,"G01B11/00,G01S17/48,G01S17/58,G01S17/89,G06K9/00,G06K9/62,G06N3/04,G06N3/08,G06T3/40,G06T7/246","{'measuring; testing', 'computing; calculating; counting'}","['measuring length, thickness or similar linear dimensions; measuring angles; measuring areas; measuring irregularities of surfaces or contours', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models', 'image data processing or generation, in general', 'image data processing or generation, in general']","driving environment sensing system and method based on cooperative control and deep learning The invention discloses a driving environment sensing system and method based on cooperative control and deep learning, and relates to the technical field of intelligent transportation. The system comprises a laser transmitting module, an optical receiving module, a laser ranging module, a main controller, a cooperative control module and a data processing center. The laser transmitting module covers an area to be monitored, and visible light and infrared video images are collected by the optical receiving module; the cooperative control module processes the data returned by the a multi-sensorto complete the high-precision speed measurement and ranging measurement of a moving object and control the data acquisition of the multi-sensor and communication with the data processing center at the same time; and the data processing center performs three-dimensional reconstruction of the target, and then transmits the three-dimensional data to the deep learning network to perform target classification and identification, and finally locates and tracks the target in real time in the monitoring area to achieve the perception and monitoring of the driving environment. The driving environmentsensing system and method overcome the defects of the conventional system with low resolution and short detection distance, and have the characteristics of all-weather, long-distance and high-precision measurement.",measuring; testing computing; calculating; counting
US2020242383_A1,2020-04-20,2020-07-30,2010-06-07,AFFECTIVA,"MISHRA, TANIYAPEACOCK, TIMOTHYEL KALIOUBY, RANAMAVADATI, SEYEDMOHAMMADTURCOT, PANU JAMES",71732489,road monitoring,multimodal machine learning for vehicle manipulation,"Techniques for machine-trained analysis for multimodal machine learning vehicle manipulation are described. A computing device captures a plurality of information channels, wherein the plurality of information channels includes contemporaneous audio information and video information from an individual. A multilayered convolutional computing system learns trained weights using the audio information and the video information from the plurality of information channels. The trained weights cover both the audio information and the video information and are trained simultaneously. The learning facilitates cognitive state analysis of the audio information and the video information. A computing device within a vehicle captures further information and analyzes the further information using trained weights. The further information that is analyzed enables vehicle manipulation. The further information can include only video data or only audio data. The further information can include a cognitive state metric.","1. A computer-implemented method for machine-trained analysis comprising: capturing, into a computing device, a plurality of information channels, wherein the plurality of information channels includes contemporaneous audio information and video information from an individual; learning, on a multilayered convolutional network, trained weights using the audio information and the video information from the plurality of information channels, wherein the trained weights are trained from both the audio information and the video information and are trained simultaneously, and wherein the learning facilitates cognitive state analysis of the audio information and the video information; capturing, within a vehicle, further information and analyzing the further information using the trained weights; and manipulating the vehicle, based on the analyzing the further information.2. The method of claim 1 further comprising collecting additional information with the plurality of information channels from a second individual and learning the trained weights factoring in the additional information.3. The method of claim 1 wherein the further information includes only video data.4. The method of claim 1 wherein the further information includes only audio data.5. The method of claim 1 wherein the audio information and the video information comprise multimodal cognitive state information.6. The method of claim 1 wherein the learning comprises early fusion combination of video data and audio data for the analyzing of the further information.7. The method of claim 1 wherein the learning comprises hybrid learning using early fusion combination of portions of the audio information and the video information along with late fusion combination of portions of the audio information and the video information.8. The method of claim 1 wherein the capturing further information comprises intermittent information.9. The method of claim 8 wherein during capturing further information, one channel from the plurality of information channels drops out, and the analyzing the further information continues without the one channel that dropped out.10. The method of claim 1 wherein the plurality of information channels includes one or more of electrodermal activity, heart rate, heart rate variability, skin temperature, blood pressure, muscle movements, or respiration.11. The method of claim 1 wherein the analyzing of the further information further comprises counting occurrences of a specific emotion type.12. The method of claim 1 wherein the multilayered convolutional network includes multiple layers that include one or more convolutional layers and one or more hidden layers.13. The method of claim 12 wherein a last layer within the multiple layers provides output indicative of cognitive state.14. The method of claim 13 further comprising tuning the last layer within the multiple layers for a particular cognitive state.15. The method of claim 12 wherein the learning comprises assigning weights to inputs on one or more layers within the multilayered convolutional network.16. The method of claim 1 further comprising using iterative semi-supervised updates of model weights in the learning.17. The method of claim 1 further comprising learning image descriptors, as part of the multilayered convolutional network, for cognitive state content.18. The method of claim 17 wherein the image descriptors are identified based on a temporal co-occurrence with an external stimulus.19. The method of claim 1 further comprising training a classifier, as part of the multilayered convolutional network, for cognitive state content.20. The method of claim 1 wherein the manipulating the vehicle includes optimizing operation of an autonomous or semiautonomous vehicle.21. The method of claim 20 wherein the optimizing operation includes performing a lock out operation; recommending a break for a vehicle occupant; recommending a different route; recommending how far to drive; responding to traffic; responding to weather; adjusting seats, mirrors, climate control, lighting, music, audio stimuli, or interior temperature; brake activation; or steering control.22. The method of claim 1 further comprising generating a cognitive state metric for the individual for inclusion in the further information.23. The method of claim 22 wherein the cognitive state metric is calculated based on facial expression metrics.24. A computer program product embodied in a non-transitory computer readable medium for machine-trained analysis, the computer program product comprising code which causes one or more processors to perform operations of: capturing, into a computing device, a plurality of information channels, wherein the plurality of information channels includes contemporaneous audio information and video information from an individual; learning, on a multilayered convolutional network, trained weights using the audio information and the video information from the plurality of information channels, wherein the trained weights are trained from both the audio information and the video information and are trained simultaneously, and wherein the learning facilitates cognitive state analysis of the audio information and the video information; capturing, within a vehicle, further information and analyzing the further information using the trained weights; and manipulating the vehicle, based on the analyzing the further information.25. A computer system for machine-trained analysis comprising: a memory which stores instructions; one or more processors coupled to the memory wherein the one or more processors, when executing the instructions which are stored, are configured to: capture, into a computing device, a plurality of information channels, wherein the plurality of information channels includes contemporaneous audio information and video information from an individual; learn, on a multilayered convolutional network, trained weights using the audio information and the video information from the plurality of information channels, wherein the trained weights are trained from both the audio information and the video information and are trained simultaneously, and wherein the learning facilitates cognitive state analysis of the audio information and the video information; capture, within a vehicle, further information and analyzing the further information using the trained weights; and manipulate the vehicle, based on the analyzing the further information.",US2020242383_A1.txt,"B60W40/08,G06K9/00,G06N3/08","{'computing; calculating; counting', 'vehicles in general'}","['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models']","multimodal machine learning for vehicle manipulation Techniques for machine-trained analysis for multimodal machine learning vehicle manipulation are described. A computing device captures a plurality of information channels, wherein the plurality of information channels includes contemporaneous audio information and video information from an individual. A multilayered convolutional computing system learns trained weights using the audio information and the video information from the plurality of information channels. The trained weights cover both the audio information and the video information and are trained simultaneously. The learning facilitates cognitive state analysis of the audio information and the video information. A computing device within a vehicle captures further information and analyzes the further information using trained weights. The further information that is analyzed enables vehicle manipulation. The further information can include only video data or only audio data. The further information can include a cognitive state metric.",computing; calculating; counting vehicles in general
CN111086520_A,2020-01-17,2020-05-01,2020-01-17,BEIJING INSTITUTE OF TECHNOLOGY,SONG QIANGWANG GUANFENGZHANG DONGMIN,70399288,speed & trajectory,speed estimation algorithm suitable for four-wheel drive vehicle at multi-wheel high slip rate,"The invention discloses a speed estimation algorithm suitable for a four-wheel drive automobile at multi-wheel high slip rate, wherein the method is divided into two layers. According to the method, the first-layer algorithm comprises that a vehicle wheel angular speed is converted into a vehicle wheel longitudinal speed through a vehicle speed estimation module based on a wheel speed, the deviation between a tire rotating radius and a tire marking radius is corrected, the vehicle speed is estimated according to fuzzy logic, the vehicle speed is estimated according to an integral accelerationvariable by using an acceleration-based vehicle speed estimation module, and when a multi-wheel slip rate is large, the first-layer algorithm cannot accurately estimate the vehicle speed; in the second-layer algorithm, fuzzy logic is introduced into the vehicle speed estimation module to accurately judge the wheel slip condition, and the accurate vehicle speed is estimated; and the final vehicle speed is estimated according to the weighting coefficient of a second-layer vehicle speed estimation module and the two signals of a balance wheel and an acceleration sensor. According to the method, the temporary estimated vehicle speed is introduced to solve the problem of inaccurate estimation of the vehicle speed and the tire slip rate, so that the precision is improved, and the method is suitable for estimating the speed of the four-wheel drive vehicle during multi-wheel high slip rate.","1. the speed estimation algorithm suitable for the multi-wheel high slip rate of the four-wheel drive vehicle is characterized by comprising a wheel speed calculation module (1), a vehicle speed estimation module (2) based on the wheel speed, a signal filtering module (3), a vehicle speed estimation module (4) based on the acceleration, a slip rate confidence coefficient calculation module (5), a vehicle speed estimation module (6) and a final fuzzy control module (7), and further comprising the following steps of: 1) pre-estimating in a first layer algorithm, and estimating relatively accurate temporary vehicle speed as a reference value of a second layer algorithm on the basis of four wheel speed sensors and a longitudinal acceleration sensor under the condition of large slip rate; 2) the estimated value v of the vehicle speed obtained according to the step 1)e,wand ve aand finally estimating the longitudinal vehicle speed in a second-layer algorithm, and adjusting the confidence degrees under different wheel slip conditions to obtain an accurate longitudinal vehicle speed estimation value under the conditions of large slip rate and multi-wheel slip. 2. the algorithm for estimating the speed of a four-wheel drive vehicle at a high slip ratio of multiple wheels according to claim 1, wherein the step 1) comprises the following steps: 11) the wheel speed calculation module (1) calculates the wheel angular velocity omega according to the sensor signali,jconverted into linear velocity v of wheeli,jleading-in tire rotation coefficient  for calibrationperforming fixed correction; 12) wheel speed v obtained according to step 11)i,jthe vehicle speed estimation module (2) based on wheel speed selects the minimum speed from the four wheel speeds obtained by the sensor, calculates the slip rates of the four wheels as the input quantity of fuzzy control, and takes the output quantity as the estimated vehicle speed value ve,w; 13) the estimated vehicle speed v obtained according to the step 12)e,wthe acceleration-based vehicle speed estimation module (4) receives the acceleration signal after the signal filtering module (3), adopts small step time iterative integration, and then carries out calibration calculation on the estimated vehicle speed value v in a linear compensation modee a 3. the algorithm for estimating the multi-wheel high slip speed of a four-wheel drive vehicle according to claim 2, wherein in the step 11), the expression for introducing the tire rotation coefficient correction is as follows: vi,j=3.6i,jri,j wherein r isi,jis the tire radius; omegai,jangular velocity of wheel,  tyre rotation coefficient, vi,jis the wheel speed; i represents a front or rear wheel and j represents a left or right wheel. 4. the algorithm for estimating the multi-wheel high slip speed of a four-wheel drive vehicle according to claim 2, wherein in the step 12), the wheel speed-based vehicle speed estimation module has the following modes of minimum wheel speed: in the first case: all tires do not slip, so in the straight driving state, the wheel speeds of the four wheels should be equal, and the calculation formula is: vi,jtrue longitudinal speed in the second case: if at least one of the four tires does not slip, then assumethe vehicle speed calculated from this wheel is vm,mthe calculation formula is as follows: vi,jvm,mtrue longitudinal speed in the third case: the four tires all slip, the calculation formula is: vi,jvm,mnot less than real longitudinal speed wherein m is not equal to i, j, 1 is not more than m, i, j is not more than 2. 5. the algorithm for estimating the multi-wheel high slip speed of a four-wheel drive vehicle according to claim 2, wherein in the step 13), the vehicle speed estimation based on the acceleration is iteratively integrated by a small step time and is corrected by a linear compensation method according to the following expression: ve_a=ve_acalibration factor wherein a issensoris the vehicle longitudinal acceleration; vtimeto accelerate the initial vehicle speed of the integrator; ve_ais an estimated vehicle speed based on the acceleration. 6. the algorithm for estimating the speed of a four-wheel drive vehicle at a high slip ratio of multiple wheels according to claim 1, wherein the step 2) comprises the following steps: 21) firstly, respectively outputting variables to a slip ratio confidence coefficient calculation module (5) and a final fuzzy control module (7) according to a relatively accurate temporary estimated vehicle speed by a vehicle speed estimation module (6) and by considering the slip ratio and the number of slipping wheels; 22) analyzing confidence degrees of vehicle speed estimation under different working conditions, balancing longitudinal vehicle speed estimation values of a vehicle speed estimation module (2) based on wheel speed and a vehicle speed estimation module (4) based on acceleration by using confidence degree parameters, and converting a temporary vehicle speed value v into a temporary vehicle speed valuee tempfor recalculating slip rates of four tires 23) designing final fuzzy logic rule and comprehensively utilizing slip ratelongitudinal vehicle speed estimate v based on wheel speede,wlongitudinal vehicle speed estimated value v based on acceleratione,aand the raw sensor signal omegai,jestimating the final longitudinal vehicle speed ve 7. the algorithm for estimating the multi-wheel high slip speed of a four-wheel drive vehicle according to claim 6, wherein in the step 22), the expressions for calculating the temporary estimated vehicle speed and slip ratio are as follows: ve_w=min(vi,j)/fuzzy logic weight ve_temp=wve_w+(1-w)ve_a wherein ve_wis an estimated vehicle speed based on wheel speed; ve tempestimating vehicle speed for relative accuracy; w is a matrix of fuzzy logic weight; m is not equal to i, j, 1 is not more than m, i, j is not more than 2.",CN111086520_A.txt,B60W40/105,{'vehicles in general'},['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit'],"speed estimation algorithm suitable for four-wheel drive vehicle at multi-wheel high slip rate The invention discloses a speed estimation algorithm suitable for a four-wheel drive automobile at multi-wheel high slip rate, wherein the method is divided into two layers. According to the method, the first-layer algorithm comprises that a vehicle wheel angular speed is converted into a vehicle wheel longitudinal speed through a vehicle speed estimation module based on a wheel speed, the deviation between a tire rotating radius and a tire marking radius is corrected, the vehicle speed is estimated according to fuzzy logic, the vehicle speed is estimated according to an integral accelerationvariable by using an acceleration-based vehicle speed estimation module, and when a multi-wheel slip rate is large, the first-layer algorithm cannot accurately estimate the vehicle speed; in the second-layer algorithm, fuzzy logic is introduced into the vehicle speed estimation module to accurately judge the wheel slip condition, and the accurate vehicle speed is estimated; and the final vehicle speed is estimated according to the weighting coefficient of a second-layer vehicle speed estimation module and the two signals of a balance wheel and an acceleration sensor. According to the method, the temporary estimated vehicle speed is introduced to solve the problem of inaccurate estimation of the vehicle speed and the tire slip rate, so that the precision is improved, and the method is suitable for estimating the speed of the four-wheel drive vehicle during multi-wheel high slip rate.",vehicles in general
CN104951764_B,2015-06-17,2018-08-21,2015-06-17,ZHEJIANG UNIVERSITY OF TECHNOLOGY,CAO BINFAN JINGDONG TIANYANGRUAN TIHONG,54166406,speed & trajectory,identification method for behaviors of high-speed vehicle based on secondary spectrum clustering and hmm (hidden markov model)-rf (random forest) hybrid model,The invention discloses an identification method for behaviors of high-speed vehicle based on secondary spectrum clustering and an HMM (Hidden Markov Model)-RF (Random Forest) hybrid model. The identification method comprises the following steps of step 1: automatically classifying highway vehicle trajectories by the secondary spectrum clustering; step 2: extracting features of the highway vehicle trajectories based on a direction angle; step 3: constructing a vehicle behavior model based on the HMM; step 4: constructing a vehicle behavior model based on the RF; step 5: identifying a vehicle behavior hybrid model based on the HMM-RF.,"1. A high-speed vehicle behavior identification method based on quadratic spectral clustering and an HMM-RF hybrid model comprises the following steps: step 1, performing secondary spectral clustering automatic classification on vehicle tracks on a highway, extracting a vehicle track data set from a traffic video by using a vehicle detection and tracking algorithm, clustering the vehicle tracks by using track curvature characteristics, inclination angle similarity and LCSS (liquid Crystal System) similarity and combining a spectral clustering algorithm, and effectively distinguishing types of straight running, overtaking, lane changing and the like; the method comprises the steps of firstly, adopting a least square method to fit a polynomial to solve track curvature, and calculating the mean value of the first N maximum curvatures in a track as the track curvature; if the curvature is larger than the curvature threshold T, clustering curves; otherwise, the clustering is a non-curve clustering; secondly, constructing an LCSS similarity matrix for curve clustering, and clustering by using spectral clustering to obtain a final curve clustering result; fitting a track inclination angle for the non-curve clustering by using a least square method, constructing a track inclination angle similarity matrix, performing first clustering by using a spectral clustering algorithm to obtain a non-curve clustering intermediate result, then establishing a track LCSS similarity matrix of the non-curve clustering intermediate result, and performing second clustering on the non-curve clustering intermediate result by using spectral clustering to obtain a non-curve clustering final result; finally, integrating the two clustering results to determine a final clustering result; the track dip similarity is defined as follows: Similar  ( i , j )=1- |  i -  j | d  max ,0in,0jn--- ( 1 ) whereinIs the inclination of the ith track, dmax=max(|i-j|) is the maximum inclination angle difference, and n is the number of tracks; while k= l  m = 0 l T m ( x )  T m ( y ) -  m = 0 l T m ( x )   m = 0 l T m ( y ) l   m = 0 l T m ( x ) 2 -  m = 0 l T m ( x )   m = 0 l T m ( x ) Is the slope of the track, where Tm(x),Tm(y) coordinate values of x and y axes of the mth track point are shown, and l is the track length; the LCSS trajectory similarity is proposed by Vlachos et al and is defined as follows: D LCSS ( F p , F q )=1- LCSS ( F p , F q ) min ( T p , T q ) --- ( 2 ) wherein LCSS (F)p,Fq) Description of the track Fp,FqLength of the longest common substring in between, Tp,TqRespectively represent the track Fp,FqThe length of the LCSS, the recursion of the LCSS is defined as follows: LCSS ( F p , F q )= 0 T p = 0 | T q = 0 1 + LCSS ( F p T p - 1 , F q T q - 1 ) d E ( f p , T p , f q , T q ) <  max ( LCSS ( F p T p - 1 , F q T q ) , LCSS ( F p T p , F q T q - 1 ) ) otherwise --- ( 3 ) wherein the content of the first and second substances,represents a length of Tp-1,Tq-1Track F ofp,Fq,fp,fqIs track Fp,FqSample point of (a), dE(fp,Tp,fq,Tq) Denotes fp,fqThe Euclidean distance between the sample points represents a threshold value of the Euclidean distance between the two points; according to the track similarity measurement method, the similarity between every two tracks is calculated, and then a track similarity matrix S is formedxy1  x, y  n, and S is the adjacency matrix of the full connectivity graph, SxyIs the value at the similarity matrix coordinate (x, y), n is the number of tracks, i.e. the matrix size; the spectral clustering algorithm calculates the eigenvectors according to the track similarity matrix to find out the internal relation among data, and divides the track into different clusters; step 2, extracting the vehicle track characteristics of the expressway based on the direction angle, wherein the direction information generated by different vehicle behaviors in the traffic video can better describe the information of the vehicle driving state, can be used for distinguishing vehicle behavior modes, and is represented by the direction angle formed by adjacent track points; let the coordinate at time t be (x) on the vehicle trajectory sequencet,yt) The coordinate at time t +1 is (x)t+1,yt+1) Then form a direction angle=arctan((yt+1-yt)/(xt+1-xt) ); in consideration of the requirements on identification accuracy and real-time performance, balanced quantization coding is carried out on direction angles in 16 directions, each pi/8 is quantized to one direction, each direction interval is coded according to a counterclockwise sequence, and the direction angles correspond to 0-15 code words in sequence, as shown in fig. 2; finally, all direction angle sequences theta obtained in sequence are utilized1,2,,n-1Form a new sequence L of characteristic values of the vehicle trajectory={1,2,,n-1}; Step 3, building a vehicle behavior model based on the HMM, and building a corresponding vehicle behavior model based on the HMM for the same type of vehicle track according to the vehicle track characteristic sequence after quantization coding; continuously iterating the characteristic sequence sample to the initial model until the model converges; suppose the random observation sequence is O  O1,o2,,oNAn HMM may be defined as a triplet   pi, a, B, and there are M (typically 3-8) markov states: (a) model initialization Initial matrix pi  pikDescription of the probability pi of an observation sequence at an initial state t-1k=P(q1=sk),skRepresents the kth hidden Markov state, q1 represents the distribution at time t-1, k is 1. ltoreq.M, andstate transition matrix a  aklIs used to describe the probability a of transition between stateskl=P(qt=sl|qt-1=sk) K is 1. ltoreq. k, l. ltoreq. M, andobserving probability matrix bl(u)  P, describing the output probability of state l for the observed value: bl(u)=P{Ot=Vu|qt=sl1 < l < M, 1 < u < N, andm is the number of states, N is the total number of coded symbols; (b) updating of models Re-estimating the lambda triple by using new vehicle track data and adopting a Baum-Welch algorithm; then, calculating the maximum likelihood values of the front model and the rear model by using a forward algorithm until the difference value of the maximum likelihood values of the front model and the rear model is within a threshold value, and stopping iteration; step 4, building a vehicle behavior model based on a Random Forest (RF), using the output of a feature sequence through the multidimensional probability of a corresponding HMM vehicle behavior model as the input vector of the random forest model, establishing the random forest vehicle behavior model, and finally combining to form a vehicle behavior recognition hybrid model based on the HMM-RF; the HMM model has good vehicle track modeling capability and strong classification capability of the RF model, and provides a vehicle track behavior identification method based on an HMM-HF mixed model; the specific idea is that an HMM is used as a part of a vehicle track model and forms the vehicle track model together with an RF model, and the HMM model is used as the prefix of the vehicle track model, so that characteristic transformation which is used for distinguishing multi-class vehicle track data of the RF model is realized; the specific process of training the vehicle track hybrid model is as follows: 1) resampling the clustered different types of track data to a range with a uniform length N, extracting direction angle characteristics, and constructing a new characteristic sequence 2) Respectively and iteratively training HMM models corresponding to vehicle behaviors through a Baum-Welch algorithm, such as lane changing, straight driving, parking, reverse driving, abnormal turning and other models; 3) obtaining an HMM model of a corresponding type obtained by training the feature sequence again to obtain N-1 multi-dimensional probability output as an input vector of the random forest RF model, carrying out model training, and determining a final HMM-RF mixed model; obviously, the probability output of the tracks belonging to the HMM models after passing through the models is higher, while the output probability of the tracks not belonging to the HMM models after passing through the models is lower, so that the classification capability can be improved; step 5, based on HMM-RF vehicle behavior hybrid model identification, resampling vehicle track data acquired in real time to a uniform length N, then carrying out quantitative coding (0-15) on the vehicle track according to a direction angle, and extracting vehicle track features to N-1 dimensional features with feature dimensions of 20-30 while considering identification accuracy and real-time requirements; thus, each vehicle track sample becomes an N-1-dimensional probability feature vector after passing through the HMM model, and the probability feature vector is used as an input vector of the random forest RF model for secondary classification and identification; the HMM-RF vehicle behavior hybrid model identification specific process is as follows: 1) new vehicle tracks collected in real time are resampled to be within the range of the uniform length N, the direction angle characteristics are extracted, and a new characteristic sequence is constructed 2) Obtaining output N-1 multidimensional probabilities of T different models through T different HMM models (lane changing, straight running, parking, reverse running, abnormal turning and the like) 3) Inputting N-1 multi-dimensional probabilities of the T different models into a random forest model, comparing the maximum predicted probability sum in all the trees, and determining the type of the vehicle track.",CN104951764_B.txt,"G06K9/00,G06K9/62",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']",identification method for behaviors of high-speed vehicle based on secondary spectrum clustering and hmm (hidden markov model)-rf (random forest) hybrid model The invention discloses an identification method for behaviors of high-speed vehicle based on secondary spectrum clustering and an HMM (Hidden Markov Model)-RF (Random Forest) hybrid model. The identification method comprises the following steps of step 1: automatically classifying highway vehicle trajectories by the secondary spectrum clustering; step 2: extracting features of the highway vehicle trajectories based on a direction angle; step 3: constructing a vehicle behavior model based on the HMM; step 4: constructing a vehicle behavior model based on the RF; step 5: identifying a vehicle behavior hybrid model based on the HMM-RF.,computing; calculating; counting
US2010209888_A1,2009-02-18,2010-08-19,2009-02-18,"GM GLOBAL TECHNOLOGY OPERATIONSCHIN, YUEN-KWOKHUANG, JIHUALIN, WILLIAM C.","CHIN, YUEN-KWOKHUANG, JIHUALIN, WILLIAM C.",42560248,speed & trajectory,vehicle stability enhancement control adaptation to driving skill based on curve-handling maneuvers,"A system and method that classifies a driver's driving skill based on curve-handling maneuvers. The curve-handling maneuvers can be identified based on the drivers steering activity, vehicle yaw motion and a change in the vehicle heading direction.","1. A method for determining a driver's driving skill of a vehicle, said method comprising: determining whether the vehicle is in a curve-handling maneuver on a curve; determining a lateral deviation of the vehicle from a center of the curve; determining the smoothness of a steering control of the vehicle during the curve-handling maneuver; determining the smoothness of a speed control of the vehicle during the curve-handling maneuver; and classifying the driver's driving skill based on the lateral deviation from the center of the curve, the smoothness of the steering control and the smoothness of the speed control.2. The method according to claim 1 wherein determining the lateral deviation includes determining the lateral deviation based on images from a forward-looking camera on the vehicle.3. The method according to claim 1 wherein determining the smoothness of the steering control includes determining damping characteristics of the vehicle and the number and magnitude of corrections in a drivers steering input.4. The method according to claim 3 wherein determining the smoothness of the steering control includes employing frequency-domain analysis techniques.5. The method according to claim 3 wherein determining the smoothness of the steering control includes constructing a steering command and comparing the drivers steering input to the steering command.6. The method according to claim 1 wherein determining the smoothness of the steering control includes calculating indexes based on a normalized error where the indexes include a mean absolute value of the normalized error, a maximum absolute value of normalized error, a number of zero crossings and a magnitude of higher-frequency components of the normalized error.7. The method according to claim 1 wherein classifying the driver's driving skill includes providing original feature extraction.8. The method according to claim 7 wherein providing original feature extraction includes providing original feature extraction of maximum lateral deviation towards the outer side of the curve, maximum lateral acceleration, maximum yaw rate, speed corresponding to the maximum acceleration, mean of the absolute value of the normalized error, maximum absolute value of the normalized error, number of zero crossings, magnitude of the higher-frequency components of the normalized error, mean of the absolute value of the localized peaks of the normalized error, maximum lateral jerk and the correlation between the steering input and the yaw rate.9. The method according to claim 1 wherein classifying the driver's driving skill includes using a principal component analysis technique.10. A method for determining a driver's driving skill of a vehicle, said method comprising: determining whether the vehicle is in a curve-handling maneuver on a curve; determining a lateral deviation of the vehicle from a center of the curve; determining the smoothness of a steering control of the vehicle during the curve-handling maneuver by employing frequency-domain analysis techniques to determine damping characteristics of the vehicle steering and the number of magnitude of corrections in a driver's steering input; and classifying the driver's driving skill based on the lateral deviation from the center of the curve and the smoothness of the steering control.11. The method according to claim 10 wherein determining the lateral deviation includes determining the lateral deviation based on images from a forward-looking camera on the vehicle.12. The method according to claim 10 wherein determining the smoothness of the steering control includes calculating indexes based on a normalized error where the indexes include a mean absolute value of the normalized error, a maximum absolute value of normalized error, a number of zero crossings and a magnitude of higher-frequency components of the normalized error.13. The method according to claim 10 wherein classifying the driver's driving skill includes providing original feature extraction.14. The method according to claim 13 wherein providing original feature extraction includes providing original feature extraction of maximum lateral deviation towards the outer side of the curve, maximum lateral acceleration, maximum yaw rate, speed corresponding to the maximum acceleration, mean of the absolute value of the normalized error, maximum absolute value of the normalized error, number of zero crossings, magnitude of the higher-frequency components of the normalized error, mean of the absolute value of the localized peaks of the normalized error, maximum lateral jerk and the correlation between the steering input and the yaw rate.15. A system for determining a driver's driving skill of a vehicle, said system comprising: means for determining whether the vehicle is in a curve-handling maneuver on a curve; means for determining the lateral deviation of the vehicle from a center of the curve; means for determining the smoothness of a steering control of the vehicle during the curve-handling maneuver; means for determining the smoothness of a speed control of the vehicle during the curve-handling maneuver; and means for classifying the driver's driving skill based on the lateral deviation from the center of the curve, the smoothness of the steering control and the smoothness of the speed control.16. The system according to claim 15 wherein the means for determining the smoothness of the steering control includes means for determining damping characteristics of the vehicle and the number and magnitude of corrections in a drivers steering input.17. The system according to claim 16 wherein the means for determining the smoothness of the steering control includes means for employing frequency-domain analysis techniques.18. The system according to claim 16 wherein the means for determining the smoothness of the steering control includes means for constructing a steering command and comparing the drivers steering input to the steering command.19. The system according to claim 15 wherein the means for determining the smoothness of the steering control includes means for calculating indexes based on a normalized error where the indexes include a mean absolute value of the normalized error, a maximum absolute value of normalized error, a number of zero crossings and a magnitude of higher-frequency components of the normalized error.20. The system according to claim 15 wherein the means for classifying the driver's driving skill includes means for providing original feature extraction.",US2010209888_A1.txt,G09B19/16,{'education; cryptography; display; advertising; seals'},"['educational or demonstration appliances; appliances for teaching, or communicating with, the blind, deaf or mute; models; planetaria; globes; maps; diagrams (devices for psychotechnics or for testing reaction times a61b5/16; games, sports, amusements a63; projectors, projector screens g03b)']","vehicle stability enhancement control adaptation to driving skill based on curve-handling maneuvers A system and method that classifies a driver's driving skill based on curve-handling maneuvers. The curve-handling maneuvers can be identified based on the drivers steering activity, vehicle yaw motion and a change in the vehicle heading direction.",education; cryptography; display; advertising; seals
US7912637_B2,2007-06-25,2011-03-22,2007-06-25,"MICROSOFT CORPORATIONHORVITZ ERIC J.OFEK, EYALLI JINLOMBARDI, STEVESCHWARTZ JOSEPHCOUCKUYT, JEFFREY D.QIAN ZHENYUSWARTZ, TANYA LISAZHUGE, YUEWU XINPAULIN, ALAN","HORVITZ ERIC J.OFEK, EYALLI JINLOMBARDI, STEVESCHWARTZ JOSEPHCOUCKUYT, JEFFREY D.QIAN ZHENYUSWARTZ, TANYA LISAZHUGE, YUEWU XINPAULIN, ALAN",40137376,road monitoring,landmark-based routing,"Driving directions can be helpful if in addition to spatial information, landmark information is provided. Landmarks assist in adding context to directions as well as allowing for a greater likelihood of success of an operator following directions. There can be employment of physical identification of landmarks as well as processing regarding the utility of a landmark in regards to driving directions. Driving directions can be highly useful if integrated landmarks relate to knowledge possessed by an operator of a vehicle. Landmark based driving direction can be integrated with advertisements that relate to the directions.","1. A system for providing to a user of a vehicle route guidance with landmark information, the system comprising: at least one sensor on the vehicle, the at least one sensor collecting information comprising at least one of user stress, user feedback, voice recognition, facial recognition, gesture recognition and language parsers; a generation component that: creates a route, and selects, based on a plurality of scores and information comprising at least one of the user stress, user feedback, voice recognition, facial recognition, gesture recognition and language parsers collected by the at least one sensor, a landmark to be included in the landmark information; a modification component that: integrates the landmark information comprising the landmark selected based on the plurality of scores and the information collected by the at least one sensor upon the route to obtain an integrated route; and a transmission component that emits the integrated route to an auxiliary location.2. The system of claim 1, wherein: the plurality of scores comprise a score calculated for the landmark based on at least one of a static property of the landmark and a relationship of the landmark to the route, and the generation component selects the landmark based on the calculated score.3. The system of claim 1, wherein the at least one sensor collects information relating to operation of the vehicle and comprises at least one of a tire pressure sensor, tire wear sensor, vibration sensor, noise sensor, air quality sensor, power meters, fuel sensors, energy levels sensor, and energy utilization sensor.4. The system of claim 1, further comprising storage that holds a landmark database that is used in integration of the landmark information to the route.5. The system of claim 1, further comprising an analysis component that performs an operation upon information related to the route.6. The system of claim 1, further comprising a calculation component that performs at least one estimation in relation to an operator perception of the landmark, wherein the estimation is used in integration of the landmarks upon the route.7. The system of claim 1, further comprising artificial intelligence that makes an inference or determination on what landmark information to integrate upon the route.8. The system of claim 1, further comprising an input component that obtains information from the user that is used in creation of the route.9. The system of claim 1, wherein the information collected by the at least one sensor is user stress.10. The system of claim 3, wherein the information collected by the at least one sensor comprises information on an amount of fuel remaining in the vehicle and the generation component selects the landmark as a landmark comprising a station to re-fuel the vehicle.11. The system of claim 6, wherein the at least one estimation in relation to the operator perception of the landmark comprises an estimation of a perceptual salience of the landmark based on at least one of a time of day and a season of the year.12. The system of claim 11, wherein an analysis component selects the landmark based on the estimation of the perceptual salience of the landmark comprising lighting.13. The system of claim 11, wherein an analysis component selects the landmark based on the estimation of the perceptual salience of the landmark comprising weather.14. The system of claim 7, wherein the artificial intelligence determines a probability of usefulness of the landmark.15. A method for providing route guidance with landmark information, the method comprising: operating at least one processor to: determine a route based on user input from a user; select landmarks based on a plurality of scores, wherein a number of the selected landmarks is selected based on a likelihood of the user being lost while using the route such that the number of the selected landmarks is higher when the likelihood of the user being lost is higher; integrate the selected landmarks into the route to obtain an integrated route, the integrated route comprising directions for following the route, at least one of the directions comprising a direction of travel relative to a selected landmark from the selected landmarks; and emit the integrated route to an auxiliary location.16. The method of claim 15, wherein selecting the landmarks based on a plurality of scores comprises preferentially selecting a landmark to which the user of the integrated route is inferred to have prior exposure.17. The method of claim 16, wherein selecting the landmarks based on a plurality of scores comprises preferentially selecting a landmark that is highly visible to a driver of a vehicle traversing the route.18. The method of claim 17, wherein the auxiliary location comprises a display screen or a speaker.19. The system of claim 1, wherein the at least one sensor collects information relating to environment around the vehicle and comprises at least one of a terrain sensor, noise sensor, air quality sensor, video perception sensors and audio perception sensors.20. The system of claim 19, wherein information collected by the video perception sensors comprises at least lighting of the environment.",US7912637_B2.txt,"G01C21/30,G01C21/34",{'measuring; testing'},"['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)']","landmark-based routing Driving directions can be helpful if in addition to spatial information, landmark information is provided. Landmarks assist in adding context to directions as well as allowing for a greater likelihood of success of an operator following directions. There can be employment of physical identification of landmarks as well as processing regarding the utility of a landmark in regards to driving directions. Driving directions can be highly useful if integrated landmarks relate to knowledge possessed by an operator of a vehicle. Landmark based driving direction can be integrated with advertisements that relate to the directions.",measuring; testing
CN110304075_B,2019-07-04,2020-06-26,2019-07-04,TSINGHUA UNIVERSITY,CHEN RUILI KEQIANGYU JIELIU JINXINXU MINGCHANGLUO YUGONGWANG TINGHANWANG YONGSHENGZHONG ZHIHUA,68078121,speed & trajectory,vehicle trajectory predicting method based on hybrid dynamic bayesian networks and gaussian process,"The invention belongs to the technical field of automatic vehicle driving environment cognition and decision-making, and especially relates to a vehicle trajectory predicting method based on hybrid dynamic bayesian networks and gaussian process. According to the method, parameters of MDBN and GP are learned through natural vehicle driving data, and a plurality of vehicle kinematic models are combined through utilizing MDBN, so that short-term trajectory prediction and estimated probabilities of driving intention and driving characteristics are obtained, and then long-term trajectory predictionand representation of uncertainty prediction are conducted through using GP. By adopting the method, short-term prediction characteristics based on a vehicle physical movement model as well as long-term trajectory prediction and representation of uncertainty prediction according to driver information can both taken into account. Compared to an existing vehicle trajectory predicting method, vehicle models, abstract intention and data driving are combined together, and the expansibility of the MDBN model and the GP model are strong, and thus the method is suitable for different driving scenarios and can combine more effective situational information like road information and traffic information.","1. a vehicle track prediction method based on a hybrid dynamic bayesian network and a gaussian process is characterized in that: the method comprises the following steps: step 1, constructing a natural driving database; establishing a test set of peripheral vehicle related sequence information, road related sequence information and traffic related sequence information acquired by an automatic driving vehicle sensing system, and a training set for calibrating driving intention and driving characteristics of the information; wherein the test set comprises a test set of a hybrid dynamic bayesian network and a test set of a gaussian process; the training set comprises a training set of a hybrid dynamic bayesian network and a training set of a gaussian process; step 2, obtaining short-term predicted trajectories of surrounding vehicles and estimated probabilities of driving intentions and driving characteristics by adopting a hybrid dynamic bayesian network; taking driver information, vehicle model selection and vehicle state information as hidden layer variables, taking surrounding vehicle related sequence information, road related sequence information and traffic related sequence information acquired by an automatic driving vehicle sensing system as observation layer variables, and building a hybrid dynamic bayesian network model; and obtaining the output of the hybrid dynamic bayesian network through posterior probability inference: predicting short-term trajectory based on the vehicle model and the estimated probability of driving intention and driving characteristics, and taking the output as the input of step 4; step 3, establishing gaussian process functions under different driving intentions and different driving characteristics; comprises the following steps: step 3.1 setting of mean function and covariance function with x as an input, the expression of the gaussian process function is as follows: f(x)gp(u(x),(x,x)) the mean function u (x) represents the vehicle track trend under a certain driving intention, so that the mean function can be used for representing the expected predicted track; the covariance function  (x, x ') represents both the variance of the different inputs x themselves and the variance between (x, x'), and therefore the covariance function can be used to represent the uncertainty corresponding to the desired predicted trajectory; setting corresponding mean functions u (x) according to vehicle tracks under different driving intentions; the covariance function  (x, x') represented in the form of a noisy mean-square index is set as follows: wherein fis the signal standard deviation, l is the characteristic length, nto observe the noise standard deviation,  is the kronecker function; step 3.2, learning of unknown parameters of gaussian process function learning related unknown parameters by using the training set of the gaussian process established in the step 1 according to the mean function u (x) and the covariance function  (x, x') set in the step 3.1, and obtaining parameter learning results by using a gradient-based optimization algorithm according to the logarithmic marginal likelihood function and the partial derivatives of the unknown parameters thereof, thereby respectively establishing gaussian process functions under different driving intents and driving characteristics; step 4, long-term trajectory prediction and uncertainty representation are carried out based on a hybrid dynamic bayesian network and a gaussian process; determining the corresponding driving intention and driving characteristics by using a maximum probability principle according to the driving intention and the estimated probability of the driving characteristics output by the hybrid dynamic bayesian network in the step 2, thereby determining the corresponding gaussian process function in the step 3 according to the driving intention and the driving characteristics; using the short-term predicted track based on the vehicle model output by the hybrid dynamic bayesian network in the step 2 as vehicle track sequence information x1predicted future trajectory is x2then (x)1,x2) the gaussian distribution obeyed is as follows: wherein the content of the first and second substances,represents a normal function;x1and x2the respective corresponding mean functions are respectively u1and u2(ii) a the covariance matrix  is a symmetric matrix, i.e.,   t,x1and x2the respective covariance functions are 11sum-sigma22,x1and x2the corresponding covariance function is 1221and is12=21t; then the vehicle trajectory x is known1future possible trajectory x of vehicle2conditional probability p (x) of2|x1) the expression of gaussian distribution obeyed is as follows: wherein u is2|1sum-sigma2|1is a variable x2|1corresponding mean function and covariance function, and (u)2|1,2|1) the expression (c) is derived from the following formula: u2|1=u2+12t11-1(x1-u1) 2|1=22-12t11-112 and finally obtaining the future track and uncertainty representation thereof in the long-term domain of the vehicle under the driving intention and the driving characteristics determined by using the maximum probability principle. 2. the hybrid dynamic bayesian network and gaussian process based vehicle trajectory prediction method of claim 1, wherein: in the step 1, the step of processing the raw material, the peripheral vehicle related sequence information comprises vehicle position, vehicle speed, vehicle acceleration, vehicle yaw velocity, the distance between the vehicle and the two sides of the road and the opening and closing conditions of left and right steering tail lamps of the vehicle; the road related sequence information comprises structural characteristics of roads and road indication signs; the traffic-related sequence information includes surrounding traffic indication signs and traffic light states. 3. the hybrid dynamic bayesian network and gaussian process based vehicle trajectory prediction method of claim 1, wherein: the step 1 comprises the following steps: 1.1 acquisition of driving data collecting vehicle related sequence information, road related sequence information and traffic related sequence information around an automatic driving vehicle sensing system in a normal driving process; 1.2 classifying and calibrating driving intentions defining possible driving intentions of surrounding vehicles according to the scene of the vehicles; for each possible driving intention, selecting key characteristic parameters for classifying and calibrating the driving intention; then, the driving intention of the surrounding vehicles within a period of time is judged and corresponding driving intention labels are set by combining the related sequence information of the surrounding vehicles acquired in the step 1.1; the scene of the vehicle is obtained by an intelligent navigation system of the automatic driving vehicle, and comprises a high-speed scene, a mountain road scene, an urban normal straight road scene, an urban traffic signal lamp intersection scene and an urban traffic signal lamp-free intersection scene; the driving intention comprises straight running along the current road, turning along the current road, left lane changing, right lane changing, left turning, right turning, turning around, parking and starting; the key characteristic parameters for classifying and calibrating the driving intention comprise the transverse position of the vehicle, the transverse speed of the vehicle, the yaw speed of the vehicle, the distance between the vehicle and the two sides of the road and the opening and closing conditions of left and right steering tail lamps of the vehicle; 1.3 classification and calibration of driving characteristics firstly, defining possible driving characteristics of surrounding vehicles, selecting key characteristic parameters for classification and judgment of the driving characteristics, judging the driving characteristics of the surrounding vehicles under a certain driving intention and setting corresponding driving characteristic labels according to the related sequence information of the surrounding vehicles acquired in the step 1.1 and the driving intention of the surrounding vehicles in a period of time acquired in the step 1.2; the driving characteristics include slow driving, smooth driving, and jerky driving; the key characteristic parameters for the classification and determination of the driving characteristics comprise a vehicle longitudinal speed, a vehicle lateral speed, a vehicle longitudinal acceleration, a vehicle lateral acceleration and a vehicle yaw rate; 1.4 creation of training set and test set taking the peripheral vehicle related sequence information, the road related sequence information and the traffic related sequence information collected in the step 1.1 as a test set of the hybrid dynamic bayesian network mdbn; taking sequence information of front section time corresponding to a transverse position and a longitudinal position of a vehicle under a certain driving intention and driving characteristics as a test set of a gaussian process; 1.1, obtaining a training set of a hybrid dynamic bayesian network (mdbn) after the surrounding vehicle related sequence information, the road related sequence information and the traffic related sequence information which are collected in the step 1.1 are classified and calibrated according to the driving intention in the step 1.2 and the driving characteristics in the step 1.3; according to the training set of the hybrid dynamic bayesian network mdbn, all sequence information corresponding to the transverse position and the longitudinal position of the vehicle under a certain driving intention and driving characteristics is used as one training set of the gaussian process gp. 4. the hybrid dynamic bayesian network and gaussian process based vehicle trajectory prediction method of claim 1, wherein: in the step 2, in the step of processing, the observation layer variable, the hidden layer variable and the connection relation between the two variables are obtained by utilizing natural driving data and an improved genetic algorithm; and obtaining the probability parameter corresponding to the connection relation by a maximum likelihood method. 5. the hybrid dynamic bayesian network and gaussian process based vehicle trajectory prediction method of claim 1, wherein: in the step 2, in the step of processing, the vehicle model comprises a uniform speed model, a uniform acceleration model and a uniform speed steering model; the vehicle state information includes a vehicle lateral position, a vehicle lateral velocity, a vehicle lateral acceleration, a vehicle longitudinal position, a vehicle longitudinal velocity, a vehicle longitudinal acceleration, and a vehicle yaw rate. 6. the hybrid dynamic bayesian network and gaussian process based vehicle trajectory prediction method of claim 1, wherein: in the step 2, in the step of processing, the hidden variables further comprise other related high-level hidden variables, and the other related high-level hidden variables comprise risk assessment of the vehicle and the environment and interaction between the vehicles; the observation layer variables further comprise other relevant bottom layer observation information corresponding to other relevant high layer hidden variables of the hidden layer variables. 7. the hybrid dynamic bayesian network and gaussian process based vehicle trajectory prediction method of claim 1, wherein: the step 2 specifically comprises the following steps: 2.1 defining hidden and observation layer nodes of hybrid dynamic bayesian network model hidden nodes contain three layers of information: the first layer is a driver information variable d, the second layer is a vehicle model selection variable m, and the third layer is a vehicle state information variable s; the hidden layer variable set h is { d, m, s }, wherein { d, m } is a discrete variable, and { s } is a continuous variable; the observation layer node mainly comprises one layer of information: the related sequence information of surrounding vehicles, the related sequence information of roads and the related sequence information of traffic, which are collected by an automatic driving vehicle sensing system; the observation layer variable set is { o }, and is a continuous variable; 2.2 optimization of network architecture given a variable set { d, m, s, o } and a training set t, finding a network n  b,  which best matches the training set t by means of learning and searching, and using a criterion function as a measure of the degree of matching; wherein, b represents a network structure, and theta represents a parameter of the network; 2.3 learning of unknown parameters between network nodes the conditional probability of part hidden layer variables is obtained through a data statistics method, the conditional probability between the rest hidden layers and the observed quantity is subjected to parameter learning by using the training set of the mixed dynamic bayesian network mdbn obtained in the step 1, the solution is achieved by using a maximum likelihood estimation method, the likelihood function l (theta) related to the network parameter theta under the known training sample d is logp (d | theta), and the optimal solution is solved by using a gradient-based optimization algorithm 2.4 posterior probabilistic reasoning with introduction of observation layer variables the posterior probability refers to the probability distribution p (h | o) of hidden layer variables under the known observation layer variables, that is, the process of deducing the corresponding driving intention, driving characteristics, vehicle model and vehicle state under the test set of the hybrid dynamic bayesian network mdbn obtained in step 1, and the short-term trajectory prediction based on the vehicle model and the estimation probability of the driving intention and the driving characteristics are obtained. 8. the hybrid dynamic bayesian network and gaussian process based vehicle trajectory prediction method of claim 1, wherein: in the step 2, the posterior probability inference is assumed density filtering approximation inference, and the specific process is as follows: 1) and (3) prediction: obtaining the joint probability score of t +1 moment by using the probability distribution of t momentcloth; all discrete hidden layer variables { m) at time tt,dtthe joint distribution of discrete hidden layer variable m at time t +1t,dt,mt+1,dt+1the joint distribution is: continuous hidden layer variable (s) at time t +1t+1the conditional probabilities are: 2) updating: introducing an observation variable at the t +1 moment to obtain new probability distribution; let the observed variable of the vehicle model be { v }, and satisfythe rest of the observed variables in the o are e, and the discrete hidden layer variable m at the t +1 momentt,dt,mt+1,dt+1the joint distribution of the { is: wherein the content of the first and second substances, continuous hidden layer variable (s) at time t +1t+1the conditional probabilities are: 3) marginalizing: marginalizing the variables before the t +1 moment in the obtained t +1 moment probability distribution, and only keeping the probability distribution of the variables at the current t +1 moment so as to be used for the iteration of the next prediction and updating step: wherein d represents a driver information variable, the driver information including a driving intention and a driving characteristic; m represents a vehicle model selection variable, and the vehicle model comprises a constant speed model, an acceleration model and a constant speed steering model; s represents a vehicle state information variable, and the vehicle state information comprises a vehicle transverse position, a vehicle transverse speed, a vehicle transverse acceleration, a vehicle longitudinal position, a vehicle longitudinal speed, a vehicle longitudinal acceleration and a vehicle yaw rate; by usingobtaining the estimated probability of the vehicle model, the driving intention and the driving characteristic at the moment of t + 1; by usinga short-term predicted trajectory based on the vehicle model is obtained. 9. the hybrid dynamic bayesian network and gaussian process based vehicle trajectory prediction method of claim 1, wherein: in the step 3.2, the maximum likelihood method is selected to learn the unknown parameters in the gaussian process. 10. the hybrid dynamic bayesian network and gaussian process based vehicle trajectory prediction method of claim 1, wherein: the specific process of the step 3.2 is as follows: according to the mean function u (x) and the covariance function  (x, x') set in step 3.1, the gaussian process gp training set d established in step 1 is usedgp(x,y)={(xi,yi),i=1:n1learning the unknown parameters involved, where n1for the sequence length of the training samples, let the unknown parameters of the mean function and covariance function be   i,i=1:n2in which n is2for the number of parameters, the log marginal likelihood function logp (y | x, ) is: wherein the content of the first and second substances,represents a normal function; x and y represent the lateral and longitudinal vehicle position; u. ofysum ya mean function and a covariance function corresponding to the variable y; selecting a maximum likelihood method, and solving to obtain unknown parameters of the gaussian process by the following formula obtaining a parameter learning result by utilizing a gradient-based optimization algorithm, wherein each component theta of an unknown parameter theta is subjected toithe derivation is shown as follows: wherein gamma  sigmay-1(y-uy), and respectively establishing gaussian process functions under different driving intentions and driving characteristics through parameter learning of the gaussian process.",CN110304075_B.txt,B60W50/00,{'vehicles in general'},['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit'],"vehicle trajectory predicting method based on hybrid dynamic bayesian networks and gaussian process The invention belongs to the technical field of automatic vehicle driving environment cognition and decision-making, and especially relates to a vehicle trajectory predicting method based on hybrid dynamic bayesian networks and gaussian process. According to the method, parameters of MDBN and GP are learned through natural vehicle driving data, and a plurality of vehicle kinematic models are combined through utilizing MDBN, so that short-term trajectory prediction and estimated probabilities of driving intention and driving characteristics are obtained, and then long-term trajectory predictionand representation of uncertainty prediction are conducted through using GP. By adopting the method, short-term prediction characteristics based on a vehicle physical movement model as well as long-term trajectory prediction and representation of uncertainty prediction according to driver information can both taken into account. Compared to an existing vehicle trajectory predicting method, vehicle models, abstract intention and data driving are combined together, and the expansibility of the MDBN model and the GP model are strong, and thus the method is suitable for different driving scenarios and can combine more effective situational information like road information and traffic information.",vehicles in general
US2020286382_A1,2019-03-07,2020-09-10,2019-03-07,TOYOTA MOTOR CORPORATION,"AVEDISOV, SERGEIBANSAL, GAURAVGUO, RUILU, HONGSHENG",72335351,road monitoring,data-to-camera (d2c) based filters for improved object detection in images based on vehicle-to-everything communication,"The disclosure describes a method for an ego vehicle. The method includes receiving a vehicle-to-everything (V2X) message that describes an object that is within proximity of an ego vehicle. The method further includes generating a set of data-to-camera (D2C) filters that are specific to the object described by the V2X message. The method further includes applying the set of D2C filters to image data that describes an initial image of the object. The method further includes generating a modified image, based on applying the set of D2C filters to the image data, wherein the modified image includes an indication of (1) a location and a size of the object in the initial image and (2) a type of object in the initial image.","1. A method comprising: receiving a vehicle-to-everything (V2X) message that describes an object that is within proximity of an ego vehicle; generating a set of data-to-camera (D2C) filters that are specific to the object described by the V2X message; applying the set of D2C filters to image data that describes an initial image of the object; and generating a modified image, based on applying the set of D2C filters to the image data, wherein the modified image includes an indication of (1) a location and a size of the object in the initial image and (2) a type of object in the initial image.2. The method of claim 1, further comprising: providing the modified image to an Advanced Driver Assistance System of the ego vehicle to reduce or eliminate a driving hazard caused by the object.3. The method of claim 1, wherein the V2X message includes a Basic Safety Message (BSM) that describes the location of the object, the size of the object, an identity of the object, and a heading of the object.4. The method of claim 1, wherein the image data is generated by one or more cameras that are part of the ego vehicle and the initial image is a raw image.5. The method of claim 1, further comprising: receiving range data describing a range between the object and the ego vehicle, wherein the range is measured by a range finding sensor of the ego vehicle.6. The method of claim 1, wherein the object is a remote vehicle and the V2X message is a Basic Safety Message (BSM) received from the remote vehicle.7. The method of claim 1, wherein the set of D2C filters are generated in real time and the set of D2C filters describe digital data that is operable to modify the initial image.8. The method of claim 1, wherein the modified image further includes a bounding box that surrounds the object.9. A system for an ego vehicle, comprising: a processor; and a non-transitory memory storing computer code which, when executed by the processor, causes the processor to: receive a vehicle-to-everything (V2X) message that describes an object that is within proximity of an ego vehicle; generate a set of data-to-camera (D2C) filters that are specific to the object described by the V2X message; apply the set of D2C filters to image data that describes an initial image of the object; and generate a modified image, based on applying the set of D2C filters to the image data, wherein the modified image includes an indication of (1) a location and a size of the object in the initial image and (2) a type of object in the initial image.10. The system of claim 9, wherein the computer code, when executed by the processor, further causes the processor to: provide the modified image to an Advanced Driver Assistance System of the ego vehicle to reduce or eliminate a driving hazard caused by the object.11. The system of claim 9, wherein the V2X message includes a Basic Safety Message (BSM) that describes the location of the object, the size of the object, an identity of the object, and a heading of the object.12. The system of claim 9, wherein the image data is generated by one or more cameras that are part of the ego vehicle and the initial image is a raw image.13. The system of claim 9, wherein the computer code, when executed by the processor, further causes the processor to: receive range data describing a range between the object and the ego vehicle, wherein the range is measured by a range finding sensor of the ego vehicle.14. The system of claim 9, wherein the object is a remote vehicle and the V2X message is a Basic Safety Message (BSM) received from the remote vehicle.15. A computer program product comprising a non-transitory memory storing computer-executable code that, when executed by a processor, causes the processor to: receive a vehicle-to-everything (V2X) message that describes an object that is within proximity of an ego vehicle; generate a set of data-to-camera (D2C) filters that are specific to the object described by the V2X message; apply the set of D2C filters to image data that describes an initial image of the object; and generate a modified image, based on applying the set of D2C filters to the image data, wherein the modified image includes an indication of (1) a location and a size of the object in the initial image and (2) a type of object in the initial image.16. The computer program product of claim 15, wherein the computer-executable code further causes the processor to: provide the modified image to an Advanced Driver Assistance System of the ego vehicle to reduce or eliminate a driving hazard caused by the object.17. The computer program product of claim 15, wherein the V2X message includes a Basic Safety Message (BSM) that describes the location of the object, the size of the object, an identity of the object, and a heading of the object.18. The computer program product of claim 15, wherein the image data is generated by one or more cameras that are part of the ego vehicle and the initial image is a raw image.19. The computer program product of claim 15, wherein the computer-executable code further causes the processor to: receive range data describing a range between the object and the ego vehicle, wherein the range is measured by a range finding sensor of the ego vehicle.20. The computer program product of claim 15, wherein the object is a remote vehicle and the V2X message is a Basic Safety Message (BSM) received from the remote vehicle.",US2020286382_A1.txt,"G06K9/00,G06K9/62,G08G1/16,H04W4/40","{'electric communication technique', 'signalling', 'computing; calculating; counting'}","['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'wireless communication networks (broadcast communication h04h; communication systems using wireless links for non-selective communication, e.g. wireless extensions h04m1/72)']","data-to-camera (d2c) based filters for improved object detection in images based on vehicle-to-everything communication The disclosure describes a method for an ego vehicle. The method includes receiving a vehicle-to-everything (V2X) message that describes an object that is within proximity of an ego vehicle. The method further includes generating a set of data-to-camera (D2C) filters that are specific to the object described by the V2X message. The method further includes applying the set of D2C filters to image data that describes an initial image of the object. The method further includes generating a modified image, based on applying the set of D2C filters to the image data, wherein the modified image includes an indication of (1) a location and a size of the object in the initial image and (2) a type of object in the initial image.",electric communication technique signalling computing; calculating; counting
US2020294403_A1,2020-06-02,2020-09-17,2016-12-22,XEVO,"MCKELVIE, SAMUEL JAMESWELLAND, ROBERT VICTORLUDWIG, JOHN HAYESCORDELL, JOHN PALMERTONG, RICHARD CHIA TSING",62627138,speed & trajectory,method and system for providing artificial intelligence analytic (aia) services for performance prediction,"One embodiment of the present invention predicts a vehicular event relating to machinal performance using information obtained from interior and exterior sensors, vehicle onboard computer (""VOC""), and cloud data. The process of predication is able to activate interior and exterior sensors mounted on a vehicle operated by a driver for obtaining current data relating to external surroundings, interior settings, and internal mechanical conditions of the vehicle. After forwarding the current data to VOC to generate a current vehicle status representing real-time vehicle performance in accordance with the current data, retrieving a historical data associated with the vehicle including mechanical condition is retrieved. In one aspect, a normal condition signal is issued when the current vehicle status does not satisfy with the optimal condition based on the historical data. Alternatively, a race car condition is issued when the current vehicle status meets with the optimal condition.","1. A method, comprising: activating a plurality of sensors associated with a vehicle; obtaining data from the plurality of sensors relating to external surroundings, interior settings, or internal mechanical conditions of the vehicle; generating a current vehicle status representing current vehicle performance based on the obtained data; retrieving historical data associated with the vehicle; and issuing a normal condition signal in response to the current vehicle status failing to satisfy an optimal condition based on the historical data.2. The method of claim 1, further comprising: issuing a race-car condition signal indicating the vehicle is used for racing in response to the current vehicle status satisfying the optimal condition based on the historical data.3. The method of claim 1, further comprising: sending a performance report to a third party indicating a current mechanical condition of the vehicle based on the current vehicle performance and the historical data.4. The method of claim 1, wherein activating the plurality of sensors includes: enabling one or more outward-facing cameras on the vehicle to record an environment outside the vehicle.5. The method of claim 1, wherein activating the plurality of sensors includes: enabling one or more inward-facing cameras on the vehicle to record an environment inside the vehicle.6. The method of claim 1, wherein activating the plurality of sensors includes: recording real-time data relating to at least one of vehicle performance, road conditions, traffic congestion, or weather conditions. 7 The method of claim 1, wherein issuing the normal condition signal includes: determining that the current vehicle status fails to satisfy the optimal condition based on the current vehicle status failing meet performance requirements as manufactured based on the historical data.8. The method of claim 1, wherein issuing the normal condition signal includes: determining that the current vehicle status fails to satisfy the optimal condition based on the current vehicle status indicating signs of wear and tear.9. The method of claim 1, further comprising: scheduling a maintenance appointment for the vehicle based on the current vehicle status.10. The method of claim 1, further comprising: initiating a manufacture recall related to the vehicle at least partially based on the current vehicle status.11. A computing system, comprising: a memory that stores computer instructions; and a processor that executes the computer instruction to: activate a plurality of sensors associated with a vehicle; obtain data from the plurality of sensors relating to external surroundings, interior settings, or internal mechanical conditions of the vehicle; generate a current vehicle status representing current vehicle performance based on the obtained data; retrieve historical data associated with the vehicle; and issue a race-car condition signal indicating the vehicle is used for racing in response to the current vehicle status satisfying an optimal condition based on the historical data.12. The computing system of claim 11, wherein the processor executes further computer instructions to: issue a normal condition signal in response to the current vehicle status failing to satisfy an optimal condition based on the historical data.13. The computing system of claim 11, wherein the processor executes further computer instructions to: send a performance report to a third party indicating a current mechanical condition of the vehicle based on the current vehicle performance and the historical data.14. The computing system of claim 11, wherein the processor executes further computer instructions to: send a performance report to a third party indicating skills or mistakes associated with a driver of the vehicle based on the current vehicle performance and the historical data.15. The computing system of claim 11, wherein the processor issues the race-car condition signal by further executing further computer instructions to: determine that the current vehicle status satisfies the optimal condition based on the current vehicle status meeting performance requirements as manufactured and failing to indicate signs of wear and tear.16. A non-transitory computer-readable storage medium having stored thereon instructions that, when executed by a processor, cause the processor to perform actions, the actions comprising: activating a plurality of sensors associated with a vehicle; obtaining data from the plurality of sensors relating to external surroundings, interior settings, or internal mechanical conditions of the vehicle; generating a current vehicle status representing current vehicle performance based on the obtained data; retrieving historical data associated with the vehicle; and sending a performance report to a third party indicating a current mechanical condition of the vehicle based on the current vehicle performance and the historical data.17. The non-transitory computer-readable storage medium of claim 16, further comprising: issuing a race-car condition signal indicating the vehicle is used for racing in response to the current vehicle status satisfying the optimal condition based on the historical data.18. The non-transitory computer-readable storage medium of claim 16, further comprising: issuing a normal condition signal in response to the current vehicle status failing to satisfy an optimal condition based on the historical data.19. The non-transitory computer-readable storage medium of claim 16, further comprising: scheduling a maintenance appointment for the vehicle based on the current vehicle status.20. The non-transitory computer-readable storage medium of claim 16, further comprising: initiating a manufacture recall related to the vehicle at least partially based on the current vehicle status.",US2020294403_A1.txt,"B60R16/023,G06K9/00,G06K9/62,G06N20/00,G06N5/02,G06N7/00,G06Q10/00,G06Q30/00,G07C5/00,G07C5/08,G07C5/10,G08G1/14,G08G1/16,H04L12/40,H04N7/18","{'electric communication technique', 'signalling', 'computing; calculating; counting', 'checking-devices', 'vehicles in general'}","['vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for', 'time or attendance registers; registering or indicating the working of machines; generating random numbers; voting or lottery apparatus; arrangements, systems or apparatus for checking not provided for elsewhere', 'time or attendance registers; registering or indicating the working of machines; generating random numbers; voting or lottery apparatus; arrangements, systems or apparatus for checking not provided for elsewhere', 'time or attendance registers; registering or indicating the working of machines; generating random numbers; voting or lottery apparatus; arrangements, systems or apparatus for checking not provided for elsewhere', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'transmission of digital information, e.g. telegraphic communication ({coding or ciphering apparatus for cryptographic or other purposes involving the need for secrecy g09c;} arrangements common to telegraphic and telephonic communication h04m)', 'pictorial communication, e.g. television']","method and system for providing artificial intelligence analytic (aia) services for performance prediction One embodiment of the present invention predicts a vehicular event relating to machinal performance using information obtained from interior and exterior sensors, vehicle onboard computer (""VOC""), and cloud data. The process of predication is able to activate interior and exterior sensors mounted on a vehicle operated by a driver for obtaining current data relating to external surroundings, interior settings, and internal mechanical conditions of the vehicle. After forwarding the current data to VOC to generate a current vehicle status representing real-time vehicle performance in accordance with the current data, retrieving a historical data associated with the vehicle including mechanical condition is retrieved. In one aspect, a normal condition signal is issued when the current vehicle status does not satisfy with the optimal condition based on the historical data. Alternatively, a race car condition is issued when the current vehicle status meets with the optimal condition.",electric communication technique signalling computing; calculating; counting checking-devices vehicles in general
EP3690753_A1,2019-02-04,2020-08-05,2019-02-04,ELEKTROBIT AUTOMOTIVE,"GRIGORESCU, SORIN MIHAIMARINA, LIVIU",65724339,road monitoring,determination of the driving context of a vehicle,"The present invention is related to a method, a computer program, and an apparatus for determining a driving context of a vehicle. In a first step, sensor data of one or more sensors of the vehicle are received (10). Then an occupancy grid is determined (11) based on the sensor data. Finally, the occupancy grid is parsed (12) with a convolutional neural network for determining the driving context.","1. A method for determining a driving context of a vehicle (40), the method comprising: - receiving (10) sensor data (SD) of one or more sensors (41) of the vehicle (40); - determining (11) an occupancy grid (OG) based on the sensor data (SD); and - parsing (12) the occupancy grid (OG) with a convolutional neural network (23) for determining the driving context.2. The method according to claim 1, wherein the convolutional neural network (23) constructs a grid representation of the driving environment by converting the occupancy grid (OG) into an image representation, where the grid cells of the occupancy grid (OG) are coded as image pixels.3. The method according to claim 1 or 2, wherein the occupancy grid (OG) is constructed using the Dempster-Shafer theory.4. The method according to any of the preceding claims, wherein the occupancy information of the grid cells of the occupancy grid (OG) is gradually decreased over time.5. The method according to any of the preceding claims, wherein the convolutional neural network (23) consists of a first convolutional layer with 48 kernels and a second convolutional layer with 96 kernels.6. The method according to claim 5, wherein the size of the convolution kernel is 99 for the first convolutional layer and 55 for the second convolutional layer.7. The method according to any of the preceding claims, wherein the convolutional neural network (23) comprises three fully connected layers linked to a final Softmax activation function for calculating driving context probabilities.8. The method according to any of the preceding claims, wherein the sensor data (SD) are at least one of Sonar data, Lidar data, and Radar data.9. The method according to any of the preceding claims, wherein the driving context is one of inner city, motorway, and parking lot.10. A computer program code comprising instructions, which, when executed by at least one processor, cause the at least one processor to perform the method of any of claims 1 to 9.11. An apparatus (20) for determining a driving context of a vehicle (40), the system (20) comprising: - an input (21) for receiving (10) sensor data (SD) of one or more sensors (41) of the vehicle (40); - an occupancy grid fusion unit (22) for determining (11) an occupancy grid (OG) based on the sensor data (SD); and - a convolutional neural network (23) for parsing (12) the occupancy grid (OG) to determine the driving context.12. A driver assistance system comprising an apparatus (20) according to claim 11 or being configured to perform a method according to any of claims 1 to 9 for selecting a driving strategy.13. An autonomous or semi-autonomous vehicle (40) comprising a driver assistance system according to claim 12.",EP3690753_A1.txt,G06N3/04,{'computing; calculating; counting'},['computing arrangements based on specific computational models'],"determination of the driving context of a vehicle The present invention is related to a method, a computer program, and an apparatus for determining a driving context of a vehicle. In a first step, sensor data of one or more sensors of the vehicle are received (10). Then an occupancy grid is determined (11) based on the sensor data. Finally, the occupancy grid is parsed (12) with a convolutional neural network for determining the driving context.",computing; calculating; counting
WO2020219028_A1,2019-04-23,2020-10-29,2019-04-23,"GOOGLEMAYSTER, YANSHUCKER, BRIAN","MAYSTER, YANSHUCKER, BRIAN",66677220,road monitoring,generation of surface maps to improve navigation,"Provided are methods, systems, devices, and tangible non-transitory computer readable media for mapping geographical surfaces. The disclosed technology can access image data and sensor data. The image data can include a plurality of images of one or more locations and semantic information associated with the one or more locations. The sensor data can include sensor information associated with detection of one or more surfaces at the one or more locations by one or more sensors. One or more irregular surfaces can be detected based at least in part on the image data and the sensor data. The one or more irregular surfaces can include the one or more surfaces associated with the image data and the sensor data that satisfies one or more irregular surface criteria at each of the one or more locations respectively. Map data including information associated with the one or more irregular surfaces can be generated.","1. A computer-implemented method of mapping, the computer-implemented method comprising:accessing, by a computing system comprising one or more processors, image data and sensor data, wherein the image data comprises a plurality of images of one or more locations and semantic information associated with the one or more locations, and wherein the sensor data comprises sensor information associated with detection of one or more surfaces at the one or more locations by one or more sensors;determining, by the computing system, one or more irregular surfaces based at least in part on the image data and the sensor data, wherein the one or more irregular surfaces comprise the one or more surfaces associated with the image data and the sensor data that satisfy one or more irregular surface criteria at each of the one or more locations respectively; andgenerating, by the computing system, map data comprising information associated with the one or more irregular surfaces.2. The computer-implemented method of claim 1, wherein the determining, by the computing system, the one or more irregular surfaces based at least in part on the image data and the sensor data, wherein the one or more irregular surfaces comprise the one or more surfaces associated with the image data and the sensor data that satisfy the one or more irregular surface criteria at each of the one or more locations respectively comprises:determining, by the computing system, that the one or more irregular surface criteria are satisfied when the sensor data indicates that one or more portions of the one or more surfaces exceed a surface area threshold and include a protuberance that exceeds a height threshold or a depression that exceeds a depth threshold.3. The computer-implemented method of claim 2, wherein the height threshold is based at least in part on a height above an average height of a ground surface of the one or more surfaces, and wherein the depth threshold is based at least in part on a depth below an average height of the ground surface of the one or more surfaces.4. The computer-implemented method of any preceding claim, wherein the determining, by the computing system, the one or more irregular surfaces based at least in part on the image data and the sensor data, wherein the one or more irregular surfaces comprise the one or more surfaces associated with the image data and the sensor data that satisfy the one or more irregular surface criteria at each of the one or more locations respectively comprises:determining, by the computing system, one or more portions of the one or more surfaces having a shape that satisfies one or more irregular shape criteria associated with irregularity in a length of sides of the one or more surfaces or irregularity in angles of the one or more surfaces.5. The computer-implemented method of any preceding claim, wherein the determining, by the computing system, the one or more irregular surfaces based at least in part on the image data and the sensor data, wherein the one or more irregular surfaces comprise the one or more surfaces associated with the image data and the sensor data that satisfy the one or more irregular surface criteria at each of the one or more locations respectively comprises:determining, by the computing system, one or more vehicle characteristics of a vehicle;determining, by the computing system, one or more surface characteristics associated with each of the one or more surfaces, wherein the one or more surface characteristics comprise one or more gradients associated with the one or more surfaces or a surface height associated with the one or more surfaces; anddetermining, by the computing system, that the one or more irregular surface criteria are satisfied when the one or more surface characteristics associated with each of the one or more surfaces satisfy one or more surface characteristic criteria based at least in part on the one or more vehicle characteristics of the vehicle.6. The computer-implemented method of claim 5, wherein the one or more vehicle characteristics comprise a ground clearance of the vehicle, a height of a vehicle, a width of a vehicle, a distance between a front wheel of the vehicle and a front bumper of the vehicle, or a firmness of a vehicle suspension system.7. The computer-implemented method of any preceding claim, further comprising: determining, by the computing system, based at least in part on the image data and the sensor data, one or more irregular surface types associated with each of the one or more irregular surfaces; anddetermining, by the computing system, that the one or more irregular surfaces are intentionally irregular surfaces if the one or more irregular surface types associated with the image data for an irregular surface of the one or more irregular surfaces match the one or more irregular surface types associated with the sensor data for the irregular surface.8. The computer-implemented method of claim 7, further comprising:determining, by the computing system, that the one or more irregular surfaces that are not intentionally irregular surfaces are one or more unintentionally irregular surfaces; and generating, by the computing system, data associated with implementing a first notification in response to the one or more intentionally irregular surfaces and data associated with implementing a second notification in response to the one or more unintentionally irregular surfaces.9. The computer-implemented method of any preceding claim, wherein the one or more sensors include one or more light detection and ranging (LiDAR) devices configured to generate the sensor data based at least in part on a LiDAR scan of the one or more surfaces of the one or more locations.10. The computer-implemented method of any preceding claim, further comprising:controlling one or more vehicle systems of a vehicle based at least in part on the map data, wherein the one or more vehicle systems comprise one or more motor systems, one or more steering systems, one or more notification systems, one or more braking systems, or one or more lighting systems.11. The computer-implemented method of any preceding claim, further comprising:determining, by the computing system, based at least in part on the image data, a geographic region or weather conditions associated with each of the one or more locations; and adjusting, by the computing system, the one or more irregular surface criteria based at least in part on the geographic region or the weather conditions associated with each of the one or more locations.12. The computer-implemented method of any preceding claim, further comprising:generating data associated with implementing one or more indications based at least in part on a vehicle being within a predetermined distance of the one or more irregular surfaces, wherein the one or more indications comprise one or more visual indications, one or more maps comprising the one or more locations of the one or more irregular surfaces, one or more textual descriptions of the one or more irregular surfaces, or one or more auditory indications associated with the one or more irregular surfaces.13. The computer-implemented method of any preceding claim, wherein the one or more irregular surface criteria comprise an irregularity height threshold or an irregularity depth threshold, and further comprising:accessing vehicle height data associated with a ground clearance of a vehicle; and adjusting the irregularity height threshold or the irregularity depth threshold based at least in part on the ground clearance of the vehicle.14. The computer-implemented method of any preceding claim, further comprising:using the map data comprising information associated with the one or more irregular surfaces to determine a navigational route for a vehicle.15. One or more tangible non-transitory computer-readable media storing computer-readable instructions that when executed by one or more processors cause the one or more processors to perform operations, the operations comprising:accessing image data and sensor data, wherein the image data comprises semantic information that is descriptive of one or more geographic features of a geographic region, and wherein the sensor data is indicative of one or more surface elements associated with one or more surfaces in the geographic region; determining whether there is an association between the one or more surface elements and the semantic information that is descriptive of the one or more geographic features of the geographic region; andgenerating map data for the geographic region associated with the one or more surface elements based on whether there is the association between the one or more surface elements and the semantic information that is descriptive of the one or more geographic features.16. The one or more tangible non-transitory computer-readable media of claim 15, wherein the map data comprises geographic information associated with one or more locations of the geographic region where the one or more surface elements are associated with the semantic information.17. The one or more tangible non-transitory computer-readable media of claim 15 or 16, wherein the semantic information comprises data descriptive of one or more geographic features in the geographic region, and wherein the one or more surface elements are physically separate from the one or more geographic features.18. A computing system comprising:one or more processors;one or more non-transitory computer-readable media storing instructions that when executed by the one or more processors cause the one or more processors to perform operations comprising:accessing image data and sensor data, wherein the image data comprises semantic information that is descriptive of one or more geographic features of a geographic region, and wherein the sensor data is indicative of one or more surface elements associated with one or more surfaces in the geographic region;determining whether there is an association between the one or more surface elements and the semantic information that is descriptive of the one or more geographic features of the geographic region; andgenerating map data for the geographic region associated with the one or more surface elements based on whether there is the association between the one or more surface elements and the semantic information that is descriptive of the one or more geographic features.19. The computing system of claim 18, further comprising:determining, based at least in part on the image data and the sensor data, one or more surface types associated with each of the one or more surface elements; andgenerating the map data comprising semantic information associated with the one or more surface types of the one or more surface elements at one or more locations of the geographic region.20. The computing system of claim 19, wherein the one or more surface types comprise one or more ground surfaces or one or more non-ground surfaces, and wherein the one or more ground surfaces comprise one or more paved ground surfaces, one or more unpaved ground surfaces, one or more sidewalk surfaces, or one or more gravel surfaces.",WO2020219028_A1.txt,G01C21/32,{'measuring; testing'},"['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)']","generation of surface maps to improve navigation Provided are methods, systems, devices, and tangible non-transitory computer readable media for mapping geographical surfaces. The disclosed technology can access image data and sensor data. The image data can include a plurality of images of one or more locations and semantic information associated with the one or more locations. The sensor data can include sensor information associated with detection of one or more surfaces at the one or more locations by one or more sensors. One or more irregular surfaces can be detected based at least in part on the image data and the sensor data. The one or more irregular surfaces can include the one or more surfaces associated with the image data and the sensor data that satisfies one or more irregular surface criteria at each of the one or more locations respectively. Map data including information associated with the one or more irregular surfaces can be generated.",measuring; testing
US2015073910_A1,2014-11-17,2015-03-12,2007-12-11,VOICEBOX TECHNOLOGIES,"KENNEWICK, MICHAEL R.DI CRISTO, PHILIPPETJALVE, MICHAELBALDWIN, LARRYCHEUNG, CATHERINESALOMON, ARIGUTTIGOLI, SHEETALARMSTRONG, LYNNZIMMERMAN, BERNIEMENAKER, SAM",40722539,gps,system and method for providing advertisements based on navigation-related preferences,"Advertisements may be provided based on navigation-related preferences. In certain implementations, a current location associated with a user may be obtained. One or more navigation-related preferences associated with the user may be obtained. An advertisement may be determined based on the current location and the navigation-related preferences. The advertisement may be provided for presentation to the user. In some implementations, a directional proximity of the user to a location associated with the advertisement may be determined. The advertisement may be determined (or selected for the user) based on the directional proximity and the navigation-related preferences.","1. A method for providing advertisements based on user location and navigation-related preferences, the method being implemented on a computer system having one or more physical processors executing computer program instructions which, when executed, perform the method, the method comprising: obtaining, by the computer system, a current location associated with a user; obtaining, by the computer system, one or more navigation-related preferences associated with the user; determining, by the computer system, an advertisement based on the current location and the one or more navigation-related preferences; and providing, by the computer system, the advertisement for presentation to the user.2. The method of claim 1, wherein the one or more navigation-related preferences comprise at least one of a preference related to estimated time of arrival, a preference related to an amount of intermediate stops while en route to a destination, a preference related to avoidance of traffic, a preference related to avoidance of a location, a preference related to an amount of traveling, or a preference related to preferred routes, and wherein determining the advertisement comprises determining the advertisement based on the current location and at least one of a preference related to estimated time of arrival, a preference related to an amount of intermediate stops while en route to a destination, a preference related to avoidance of traffic, a preference related to avoidance of a location, a preference related to an amount of traveling, or a preference related to preferred routes.3. The method of claim 1, further comprising: determining, by the computer system, a targeted audience associated with the advertisement; and determining, by the computer system, whether the targeted audience corresponds to the one or more navigation-related preferences, wherein determining the advertisement comprises determining the advertisement based on the current location and a determination that the target audience corresponds to the one or more navigation-related preferences.4. The method of claim 1, further comprising: determining, by the computer system, based on the current location, a directional proximity of the user to a location associated with the advertisement; wherein determining the advertisement comprises determining the advertisement based on the directional proximity and the one or more navigation-related preferences.5. The method of claim 4, wherein determining the directional proximity comprises determining the direction proximity based on the current location and a direction of travel of the user.6. The method of claim 1, wherein a set of advertisements are stored at a user device associated with the user, and wherein determining the advertisement comprises selecting, from the set of advertisements, the advertisement based on the current location and the one or more navigation-related preferences.7. The method of claim 1, further comprising: obtaining, by the computer system, the advertisement via a network connection based on the determination of the advertisement.8. The method of claim 1, wherein the one or more navigation-related preferences are determined based on information used to interpret natural language utterances of the user.9. The method of claim 1, wherein the one or more navigation-related preferences comprise a preference related to estimated time of arrival, and wherein determining the advertisement comprises determining the advertisement based on the current location and a preference related to estimated time of arrival.10. The method of claim 1, wherein the one or more navigation-related preferences comprise a preference related to an amount of intermediate stops while en route to a destination, and wherein determining the advertisement comprises determining the advertisement based on the current location and a preference related to an amount of intermediate stops while en route to a destination.11. The method of claim 1, wherein the one or more navigation-related preferences comprise a preference related to avoidance of traffic, and wherein determining the advertisement comprises determining the advertisement based on the current location and a preference related to avoidance of traffic.12. The method of claim 1, wherein the one or more navigation-related preferences comprise a preference related to avoidance of a location, and wherein determining the advertisement comprises determining the advertisement based on the current location and a preference related to avoidance of a location.13. The method of claim 1, wherein the one or more navigation-related preferences comprise a preference related to a temporal amount of traveling, and wherein determining the advertisement comprises determining the advertisement based on the current location and a preference related to a temporal amount of traveling.14. The method of claim 1, wherein the one or more navigation-related preferences comprise a preference related to a spatial amount of traveling, and wherein determining the advertisement comprises determining the advertisement based on the current location and a preference related to a spatial amount of traveling.15. The method of claim 1, wherein the one or more navigation-related preferences comprise a preference related to preferred routes, and wherein determining the advertisement comprises determining the advertisement based on the current location and a preference related to preferred routes.16. A system of providing advertisements based on user location and navigation-related preferences, the system comprising: one or more physical processors programmed with computer program instructions which, when executed, cause the one or more physical processors to: obtain a current location associated with a user; obtain one or more navigation-related preferences associated with the user; determine an advertisement based on the current location and the one or more navigation-related preferences; and provide the advertisement for presentation to the user.17. The system of claim 16, wherein the one or more navigation-related preferences comprise at least one of a preference related to estimated time of arrival, a preference related to an amount of intermediate stops while en route to a destination, a preference related to avoidance of traffic, a preference related to avoidance of a location, a preference related to an amount of traveling, or a preference related to preferred routes, and wherein determining the advertisement comprises determining the advertisement based on the current location and at least one of a preference related to estimated time of arrival, a preference related to an amount of intermediate stops while en route to a destination, a preference related to avoidance of traffic, a preference related to avoidance of a location, a preference related to an amount of traveling, or a preference related to preferred routes.18. The system of claim 16, wherein the one or more physical processors are further caused to: determine, based on the current location, a directional proximity of the user to a location associated with the advertisement; wherein determining the advertisement comprises determining the advertisement based on the directional proximity and the one or more navigation-related preferences.19. The system of claim 18, wherein determining the directional proximity comprises determining the direction proximity based on the current location and a direction of travel of the user.20. The system of claim 16, wherein the one or more navigation-related preferences are determined based on information used to interpret natural language utterances of the user.",US2015073910_A1.txt,"G06Q30/02,G10L15/00","{'computing; calculating; counting', 'musical instruments; acoustics'}","['data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for', 'speech analysis or synthesis; speech recognition; speech or voice processing; speech or audio coding or decoding']","system and method for providing advertisements based on navigation-related preferences Advertisements may be provided based on navigation-related preferences. In certain implementations, a current location associated with a user may be obtained. One or more navigation-related preferences associated with the user may be obtained. An advertisement may be determined based on the current location and the navigation-related preferences. The advertisement may be provided for presentation to the user. In some implementations, a directional proximity of the user to a location associated with the advertisement may be determined. The advertisement may be determined (or selected for the user) based on the directional proximity and the navigation-related preferences.",computing; calculating; counting musical instruments; acoustics
US2020318973_A1,2019-04-02,2020-10-08,2019-04-02,GM GLOBAL TECHNOLOGY OPERATIONS,"MURAD, MOHANNADRAPHAEL, ERIC L.BAI FANNASERIAN, MOHAMMADBUSH, LAWRENCE A.Ren, Pengfei",72518169,gps,method and apparatus of parallel tracking and localization via multi-mode slam fusion process,"A method for vehicle tracking and localization includes receiving, by a controller, odometry data from a sensor of the first vehicle; geospatial data from a Global Positioning System (GPS) device of the first vehicle; inertial data from an inertial measurement unit (IMU) of the first vehicle; estimating an estimated-current location of the first vehicle and an estimated-current trajectory of the first vehicle using the odometry data from the sensor, the geospatial data from the GPS device, and the inertial data from the IMU of the first vehicle; inputting the inertial data into a Bayesian Network to determine a predicted location of the first vehicle and a predicted trajectory of the first vehicle, and updating the Bayesian Network using the estimated-current location and the estimated-current trajectory of the first vehicle using the odometry data and the geospatial data.","1. A method for vehicle tracking and localization, comprising: receiving, by a first controller of a first vehicle, odometry data from a sensor of the first vehicle; receiving, by the first controller of the first vehicle, geospatial data from a Global Positioning System (GPS) device of the first vehicle; receiving, by the first controller of the first vehicle, inertial data from an inertial measurement unit (IMU) of the first vehicle; estimating, by the first controller of the first vehicle, an estimated-current location of the first vehicle and an estimated-current trajectory of the first vehicle using the odometry data from the sensor, the geospatial data from the GPS device, and the inertial data from the IMU of the first vehicle; inputting the inertial data into a Bayesian Network to determine a predicted location of the first vehicle and a predicted trajectory of the first vehicle, wherein the Bayesian Network runs on the first controller of the first vehicle; and updating the Bayesian Network using the estimated-current location and the estimated-current trajectory of the first vehicle using the odometry data and the geospatial data; detecting, by the first controller of the first vehicle, a second vehicle using the odometry data of the sensor of the first vehicle; estimating, by the first controller of the first vehicle, an estimated-current location of a second vehicle and an estimated-current trajectory of the second vehicle using the odometry data from the sensor, the geospatial data from the GPS device, and the inertial data from the IMU of the first vehicle; and transmitting the estimated-current location of the first vehicle, the estimated-current trajectory of the first vehicle, the estimated-current location of the second vehicle, and the estimated-current trajectory of the second vehicle to the second vehicle.2. The method of claim 1, further comprising extracting, by the first controller of the first vehicle, feature data from the odometry data, wherein the feature data is data about objects and an arrangement of the objects detected by the sensor, and the feature data includes a feature map that shows the arrangement of the objects sensed by the sensor.3. The method of claim 2, further comprising determining, by the first controller of the first vehicle, feature mappoints using the geospatial data received by from the GPS device, wherein the feature mappoints are geospatial locations of features.4. The method of claim 3, further comprising matching the feature map with the feature mappoints.5. The method of claim 4, wherein estimating, by the first controller of the first vehicle, the estimated-current location of the first vehicle and the estimated-current trajectory of the first vehicle includes using a pose graph optimization process.6. The method of claim 4, wherein estimating, by the first controller of the first vehicle, the estimated-current location of the first vehicle and the estimated-current trajectory of the first vehicle includes using an extended Kalman filter.7. The method of claim 6, estimating, by the first controller of the first vehicle, the estimated-current location of the first vehicle and the estimated-current trajectory of the first vehicle includes using an extended Kalman filter and fusing the odometry data, the geospatial data, and the inertial data to determine the estimated-current location of the first vehicle and the estimated-current trajectory of the first vehicle.8. The method of claim 7, wherein the GPS device of the first vehicle is a first GPS device; the geospatial data is a first geospatial data; the second vehicle includes a second GPS device and a second controller in communication with the second GPS device; and the method further includes receiving, by the second controller of the second vehicle, the estimated-current location of the first vehicle, the estimated-current trajectory of the first vehicle, the estimated-current location of the second vehicle, and the estimated-current trajectory of the second vehicle to the second vehicle from the first controller of the first vehicle.9. The method of claim 8, further comprising: receiving, by the second controller of the second vehicle, second geospatial data from the second GPS device of the second vehicle; and determining, by the second controller of the second vehicle, an updated-estimated-current location of the second vehicle using the second geospatial data from the second GPS device and the estimated-current location of the second vehicle transmitted by the first vehicle.10. The method of claim 9, further comprising determining, by the first controller of the first vehicle, semantic mappoints data from the odometry data.11. The method of claim 1, wherein the sensor is a camera configured to capture an image.12. The method of claim 1, wherein the sensor is a lidar sensor.13. A system for tracking and localization of a first vehicle, comprising: a camera configured to capture images, wherein the camera is configured to generate odometry data, the odometry data is indicative of a position and an orientation of the first vehicle, and the camera is configured to detect a second vehicle; a communication system configured to wirelessly communicate information between the first vehicle and a second vehicle; a Global Positioning System (GPS) device configured to determine a GPS-location of the first vehicle, and the GPS device is configured to generate geospatial data, and the geospatial data is indicative of the GPS-location of the first vehicle; an inertial measurement unit (IMU) configured to measure an angular and linear motion of the first vehicle, wherein the IMU is configured to generate inertial data, and the inertial data is indicative of the angular and linear motion of the first vehicle; a first controller is in communication with the communication system and the camera, wherein the first controller is programmed to: receive the odometry data from the camera of the first vehicle; receive the geospatial data from GPS device of the first vehicle; receive the inertial data from the inertial measurement unit (IMU) of the first vehicle; estimate an estimated-current location of the first vehicle and an estimated-current trajectory of the first vehicle using the odometry data from the camera, the geospatial data from the GPS device, and the inertial data from the IMU of the first vehicle; input the inertial data into a Bayesian Network to determine a predicted location of the first vehicle and a predicted trajectory of the first vehicle, wherein the Bayesian Network runs on the first controller of the first vehicle; and update the Bayesian Network using the estimated-current location and the estimated-current trajectory of the first vehicle using the odometry data and the geospatial data; detect the second vehicle using the odometry data of the camera of the first vehicle; estimate an estimated-current location of a second vehicle and an estimated-current trajectory of the second vehicle using the odometry data from the camera, the geospatial data from the GPS device, and the inertial data from the IMU of the first vehicle; command the communication system to transmit the estimated-current location of the first vehicle, the estimated-current trajectory of the first vehicle, the estimated-current location of the second vehicle, and the estimated-current trajectory of the second vehicle to the second vehicle.14. The system of claim 13, wherein the first controller is programmed to extract feature data from the odometry data, the feature data is data about objects and an arrangement of the objects detected by the sensor, and the feature data includes a feature map that shows the arrangement of the objects sensed by the sensor.15. The system of claim 14, wherein the first controller is programmed to determine feature mappoints using the geospatial data received by from the GPS device, and the feature mappoints are geospatial locations of features.16. The system of claim 15, wherein the first controller is programmed to match the feature map with the feature mappoints.17. The system of claim 16, wherein the first controller is programmed to use a pose graph optimization process to estimate the estimated-current location of the first vehicle and the estimated-current trajectory of the first vehicle.18. The system of claim 16, wherein the first controller is programmed to use an extended Kalman filter to estimate the estimated-current location of the first vehicle and the estimated-current trajectory of the first vehicle.19. The system of claim 18, wherein the first controller is programmed to fuse the odometry data, the geospatial data, and the inertial data to determine the estimated-current location of the first vehicle and the estimated-current trajectory of the first vehicle.20. The system of claim 19, further comprising a Lidar sensor.",US2020318973_A1.txt,"G01C21/16,G01C21/32,G01S17/02,G01S17/93,G01S19/49,G06T7/70","{'measuring; testing', 'computing; calculating; counting'}","['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'image data processing or generation, in general']","method and apparatus of parallel tracking and localization via multi-mode slam fusion process A method for vehicle tracking and localization includes receiving, by a controller, odometry data from a sensor of the first vehicle; geospatial data from a Global Positioning System (GPS) device of the first vehicle; inertial data from an inertial measurement unit (IMU) of the first vehicle; estimating an estimated-current location of the first vehicle and an estimated-current trajectory of the first vehicle using the odometry data from the sensor, the geospatial data from the GPS device, and the inertial data from the IMU of the first vehicle; inputting the inertial data into a Bayesian Network to determine a predicted location of the first vehicle and a predicted trajectory of the first vehicle, and updating the Bayesian Network using the estimated-current location and the estimated-current trajectory of the first vehicle using the odometry data and the geospatial data.",measuring; testing computing; calculating; counting
US2017082757_A1,2015-05-07,2017-03-23,2014-05-07,CONTINENTAL TEVES,"STAEHLIN, ULRICHSWOBODA, ADAMGROTENDORST, THOMASARBITMANN, MAXIMGREWE, RALPHKOMAR, MATTHIASBASTIAN, ZYDEKKUNKEL, ANNEMARIE",53189027,gps,determination of redundant absolute positions by means of vehicle-dynamics sensors,"The invention relates to a method for determining a reference position as the basis for a correction of a GNSS position of a vehicle located using a Global Satellite Navigation System (GNSS), which contains an absolute position of the vehicle, comprising: recording the absolute position of the vehicle using the GNSS when an output signal (from a motion recording sensor in the vehicle has a characteristic progression; determining the reference position based on the sensed absolute position and assigning the reference position to the characteristic progression of the output signal.","1. 1-20. (canceled)21. A method for determining a reference position as the basis for a correction of an absolute position of a vehicle located using a Global Satellite Navigation System (GNSS) comprising: recording of the absolute position of the vehicle using the GNSS; recording an output signal of a motion recording sensor of the vehicle; recognizing when a characteristic progression is present in the output signal, wherein the characteristic progression represents a signal pattern which is dependent on uneven surfaces on the road; determining the absolute position as the reference position; assigning the reference position to the characteristic progression of the output signal; and correcting an already stored reference position based on the absolute position using a learning method or the formation of an average value if the absolute position lies in an area around the stored reference position and the output signal represents a known signal pattern.22. The method of claim 21, wherein the characteristic progression of the output signal is based on a predetermined surface structure of a road on which the vehicle is driving.23. The method of claim 21, wherein the motion recording sensor measures at least one of a position, a velocity, and an acceleration of at least one component of the vehicle.24. The method of claim 21, further comprising correcting the reference position based on a newly sensed absolute position when the characteristic progression in the output signal from the movement sensor is newly detected.25. The method of claim 21, further comprising recording of a further absolute position of the vehicle when the output signal comprises a further characteristic progression which differs from the characteristic progression.26. The method of claim 25, wherein an interval between the characteristic progression and the further characteristic progression fulfills a first predetermined condition.27. The method of claim 21, further comprising deleting the reference position based on a second predetermined condition.28. The method of claim 27, wherein the second predetermined condition is fulfilled when a degree of integrity for the assignment between the reference position and the characteristic progression of the output signal falls below a threshold value.29. The method of claim 21, further comprising recording a map, by entering the specific reference position as a map position of a road into the map.30. The method of claim 29, wherein the motion recording sensor is one of an acceleration sensor and a mobile device with an acceleration sensor.31. The method of claim 30, wherein the progression of the acceleration of the acceleration sensor is recorded as linked to the reference position.32. The method of claim 30, wherein a progression of a longitudinal acceleration sensor is recorded in relation to reference positions.33. The method of claim 30, further comprising: evaluating the acceleration progression, and detecting uneven surfaces on the road on the basis of acceleration peaks.34. The method of claim 33, further comprising detecting uneven surfaces on the road on the basis of a road image recorded using a camera.35. The method of claim 33, further comprising producing at least one of a locally administered map and centrally administered map of uneven surfaces on the road.36. The method of claim 33, wherein the uneven surfaces on the road are classified depending on acceleration peaks.37. The method of claim 30, wherein the progression of the acceleration is linked to at least one of further driving situations, a driver type and vehicle parameters.38. The method of claim 37, wherein by means of a cluster analysis of the acceleration progression and the respective parameters, acceleration progression clusters are formed.39. The method of claim 30, wherein at least one of set velocities and hazardous points are derived from the progression of at least one of the acceleration and the uneven surfaces on the road.40. A control device for a correction of an absolute position of a vehicle located using a Global Satellite Navigation System (GNSS) comprising: a selection filter which recognizes and filters a characteristic progression is present in an output signal of a motion recording sensor of the vehicle, wherein the characteristic progression represents a signal pattern which is dependent on uneven surfaces on the road; position determination facility to determine a reference position from an absolute position received by a vehicle GNSS receiver of the vehicle; wherein the position determination facility assigns the reference position to the characteristic progression of the output signal; and a merging filter to correct an already stored reference position based on the absolute position using one of: a learning method and the formation of an average value if the absolute position lies when an area around the stored reference position and the output signal represents a known signal pattern.41. The control device of claim 40, wherein the characteristic progression of the output signal is based on a predetermined surface structure of a road on which the vehicle is driving.42. The control device of claim 40, wherein the motion recording sensor measures at least one of a position, a velocity, and an acceleration of at least one component of the vehicle.43. The control device of claim 40, wherein the reference position is corrected based on a newly sensed absolute position when the characteristic progression in the output signal from the movement sensor is newly detected.44. The control device of claim 40, further comprising recording of a further absolute position of the vehicle when the output signal comprises a further characteristic progression which differs from the characteristic progression.45. The control device of claim 44, wherein an interval between the characteristic progression and the further characteristic progression fulfills a first predetermined condition.46. The control device of claim 40, wherein the reference position is deleted based on a second predetermined condition.47. The control device of claim 46, wherein the second predetermined condition is fulfilled when a degree of integrity for the assignment between the reference position and the characteristic progression of the output signal falls below a threshold value.48. The control device of claim 40, wherein a map is created by entering the specific reference position as a map position of a road into the map.49. The control device of claim 48, wherein the motion recording sensor is one of an acceleration sensor and a mobile device with an acceleration sensor.50. The control device of claim 49, wherein the progression of the acceleration of the acceleration sensor is recorded as linked to the reference position.51. The control device of claim 49, wherein a progression of a longitudinal acceleration sensor is recorded in relation to reference positions.52. The control device of claim 49, wherein the device evaluates the acceleration progression, and detects uneven surfaces on the road on the basis of acceleration peaks.53. The control device of claim 52, wherein the device detects uneven surfaces on the road on the basis of a road image recorded using a camera.54. The control device of claim 52, wherein the device produces at least one of a locally administered map and centrally administered map of uneven surfaces on the road.55. The control device of claim 52, wherein the uneven surfaces on the road are classified depending on acceleration peaks.56. The control device of claim 49, wherein the progression of the acceleration is linked to at least one of further driving situations, a driver type and vehicle parameters.57. The control device of claim 18, wherein by means of a cluster analysis of the acceleration progression and the respective parameters, acceleration progression clusters are formed.58. The control device of claim 49, wherein at least one of set velocities and hazardous points are derived from the progression of at least one of the acceleration and the uneven surfaces on the road.",US2017082757_A1.txt,"G01C21/32,G01S19/40,G01S19/45",{'measuring; testing'},"['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves']","determination of redundant absolute positions by means of vehicle-dynamics sensors The invention relates to a method for determining a reference position as the basis for a correction of a GNSS position of a vehicle located using a Global Satellite Navigation System (GNSS), which contains an absolute position of the vehicle, comprising: recording the absolute position of the vehicle using the GNSS when an output signal (from a motion recording sensor in the vehicle has a characteristic progression; determining the reference position based on the sensed absolute position and assigning the reference position to the characteristic progression of the output signal.",measuring; testing
CN111104885_A,2019-12-10,2020-05-05,2019-12-10,"NANJING UNIVERSITY OF POSTS AND TELECOMMUNICATIONS",DING FEILI JINGZHANG DENGYINZHU HONGBOLI YONGJUNAI CHENGWANPAN YUZHEZHOU RUNWU,70422820,vehicle classification,vehicle identification method based on video deep learning,"The invention discloses a vehicle recognition method based on video deep learning, and the method comprises the following steps: a, carrying out the real-time collection of a road condition through avehicle-mounted camera, carrying out the image segmentation of a video through hardware equipment, forming an independent binary image, and carrying out the preprocessing of the image through an imagetarget detection algorithm; step b, in the process of performing binarization processing on the preprocessed grayscale image by using deep learning, performing statistics by using an iterative method, calculating grayscale distribution of the image, and performing iteration on the grayscale distribution to determine an optimal segmentation threshold of the image; c, importing the processed grey-scale map into an edge computing platform to perform feature extraction on the grey-scale map, separating a target from a background area, separating a complex background from a vehicle, and only enhancing a vehicle part; and d, importing the processed image into an intelligent cloud mirror terminal for display. The vehicle identification method based on video deep learning provided by the invention is high in detection speed, high in detection efficiency, low in picture precision requirement and capable of ensuring accuracy.","1. a vehicle identification method based on video deep learning is characterized in that: the method comprises the following steps: a, collecting road conditions in real time by using a vehicle-mounted camera, carrying out image segmentation on a video through hardware equipment to form an independent binary image, and preprocessing the image by adopting an image target detection algorithm; b, in the process of carrying out binarization processing on the preprocessed gray level image by utilizing deep learning, counting and calculating each gray level distribution of the image by using an iterative method, and iterating to determine the optimal segmentation threshold value of the image; c, importing the processed gray level image into an edge computing platform to perform feature extraction, separating a target from a background area, separating a complex background from a vehicle, and only enhancing the vehicle part; and d, importing the processed image into an intelligent cloud mirror terminal for display. 2. the video deep learning-based vehicle identification method according to claim 1, characterized in that: in the step a, the picture preprocessing process comprises the steps of carrying out shape detection and carrying out gray level transformation, gray level stretching, denoising and filtering. 3. the video deep learning-based vehicle identification method according to claim 1, characterized in that: in the step b, the method for calculating the gray distribution by the iterative method and determining the optimal segmentation threshold of the image comprises the following specific steps: step b1, removing the maximum and minimum gray values, and taking the median of the rest image gray values as an initial threshold value t (j), wherein j is the iteration number, and the initial j is 0; step b2, using t (j) to segment the image, and dividing the image into two regions, c1(j)c2(j); step b3, calculating c by using formula1(j) average gray value of (2): calculating c by formula2(j) average gray value of (2): whereinis the region c at the jth iteration1the number of the pixel points of (a),is the region c at the jth iteration2f (x, y) represents the gray value of (x, y) point in the image; step b4, calculating a new threshold value by using a formula; and b5, making j equal to j +1, and repeating the steps b2, b3 and b4, so that the difference between t (j +1) and t (j) is smaller than a specified value or j reaches the maximum iteration number. 4. the video deep learning-based vehicle identification method according to claim 1, characterized in that: in step c, the method for separating the target and the background area specifically comprises the following steps: step c1, introducing color features as auxiliary features of svm classification, combining the features of hsv space-based color histograms with an orb feature algorithm for object identification, increasing the robustness of identification, and separating a target from a background region; step c2, when extracting the target and background area, extracting edge points reflecting gray level change by using an edge detection algorithm, then removing some edge points or filling edge break points, connecting the edges into a complete line, and extracting a primary image; and c3, carrying out image classification on the extracted primary image by adopting a support vector machine (svm) to obtain vehicle characteristics. 5. the video deep learning-based vehicle identification method according to claim 4, characterized in that: step c1 includes the following specific steps: step c11, dividing the color space of the image frame in the video sequence into a plurality of subintervals, each subinterval corresponding to a bin interval of the histogram; step c12, dividing the h space into 8 parts according to the visual resolution capability of the human body, dividing the s space and the v space into 3 parts respectively, and carrying out unequal interval quantization on the hsv space according to different color ranges and subjective color perception; step c13, synthesizing each color component according to the formula g  hqsqv + sqv + v to construct a one-dimensional feature vector according to the quantization level obtained in step c12, wherein qs and qv are the quantization levels of the components s and v, respectively; step c14, traversing the image, counting the number of image pixel points in each subinterval of the color space, and calculating the number of image pixel points in each subinterval of the color space through a formulacalculating the color distribution condition of the image frame to extract the required color features, wherein k is a certain dimension of a histogram vector, l represents the number of histogram bins, and n iskthe number of pixel points drawn into a corresponding color space subinterval, and n represents the total number of pixel points of the image; h (k) represents the composition distribution of the colors in the image, i.e. which colors appear and the probability of each color appearing. and c14, detecting and acquiring an initial target position through an orb feature matching algorithm, inputting a target image as a feature template, and correcting the search window through the direction information of the orb feature points, thereby detecting and acquiring a moving target position. 6. the video deep learning-based vehicle identification method according to claim 4, characterized in that: step c3 includes the following specific steps: step c31, adding the candidate area searching target box of the rpn network to 322642128225625122the length-width ratio of the candidate region is 1: 1. 1: 2. 2: 1; the obtained vehicle characteristic picture is led into an rpn network, and the rpn network translates and scalescorrecting the suggestion frames to obtain suggestion frames, and simultaneously removing inaccurate suggestion frames; and c32, determining a feature mapping block corresponding to the foreground region of the suggestion frame and the original picture, vectorizing the feature mapping block by using a pyramid pooling method to obtain a feature vector, inputting the feature vector into an svm for specific classification, and performing classification learning by using the extracted primary image as a sample class and the image in the training library as another image class by using a one-to-many method to obtain the vehicle features.",CN111104885_A.txt,"G06K9/00,G06K9/34,G06K9/46,G06K9/62,G06N3/04,G06N3/08",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models']","vehicle identification method based on video deep learning The invention discloses a vehicle recognition method based on video deep learning, and the method comprises the following steps: a, carrying out the real-time collection of a road condition through avehicle-mounted camera, carrying out the image segmentation of a video through hardware equipment, forming an independent binary image, and carrying out the preprocessing of the image through an imagetarget detection algorithm; step b, in the process of performing binarization processing on the preprocessed grayscale image by using deep learning, performing statistics by using an iterative method, calculating grayscale distribution of the image, and performing iteration on the grayscale distribution to determine an optimal segmentation threshold of the image; c, importing the processed grey-scale map into an edge computing platform to perform feature extraction on the grey-scale map, separating a target from a background area, separating a complex background from a vehicle, and only enhancing a vehicle part; and d, importing the processed image into an intelligent cloud mirror terminal for display. The vehicle identification method based on video deep learning provided by the invention is high in detection speed, high in detection efficiency, low in picture precision requirement and capable of ensuring accuracy.",computing; calculating; counting
US10325315_B1,2018-03-08,2019-06-18,2018-03-08,CAPITAL ONE SERVICES,"BAJAJ, RAMANTANG, QIAOCHUYAJNIK, SANJIVDAGLEY, GEOFFREYVASISHT, SUNIL SUBRAHMANYAMPRICE, MICAHWYLIE, STEPHEN MICHAELHOOVER, JASON RICHARDDUGAL, ARJUN",66825999,vehicle classification,database image matching using machine learning with output personalization,"A system for processing an image including a vehicle using machine learning can include a processor in communication with a client device, and a storage medium storing instructions that, when executed, cause the processor to perform operations including: determining a location of the client device; receiving an image of a vehicle from the client device; matching, using a machine learning algorithm, the image to one or more images of vehicles in a database, the database listing vehicles located at the determined location and including images of the vehicles; retrieving vehicle information from the database based on the matched one or more images in the database; obtaining user information relating to a financing request for the vehicle; determining a real-time quote for the vehicle based on the vehicle information and user information; and transmitting the real-time quote for display on the client device.","1. A system for processing an image including a vehicle using machine learning, comprising: a processor in communication with a client device; and a storage medium storing instructions that, when executed, configure the processor to perform operations comprising: determining a location of the client device; mapping the location of the client device to a vehicle dealership; receiving an image of a vehicle from the client device; matching, using a machine learning algorithm, the image to one or more images of vehicles in a database of the vehicle dealership, the database listing vehicles located at the vehicle dealership, the database including images of the listed vehicles, wherein matching includes: inputting the image to an input layer of a first convolutional neural network; extracting features indicating characteristics of the vehicle from an output layer of the first convolutional neural network; inputting the extracted features to an input layer of a second convolutional neural network; determining attributes of the vehicle from an output layer of the second convolutional neural network; and identifying the matched one or more images in the database as images having associated attributes matching the determined attributes; retrieving vehicle information from the database based on the matched one or more images in the database; obtaining, using the client device, user information relating to a financing request for the vehicle shown in the image from a system associated with a financial services provider; determining a real-time quote for the vehicle based on the vehicle information and the user information; and transmitting the real-time quote for display on the client device.2. The system of claim 1, wherein the operations further comprise: determining that the image is unacceptable; and transmitting information for display on the client device, the information including an error message or user guidance.3. The system of claim 2, wherein the user guidance comprises a guiding line for obtaining a second image of the vehicle.4. The system of claim 1, wherein obtaining the user information comprises: authenticating a user of the client device; retrieving the user's account information based on the authentication, wherein the user's account information includes information from a pre-qualification vehicle financing application.5. The system of claim 1, wherein obtaining the user information comprises: transmitting a vehicle financing application form for display on the client device; and receiving user input from the client device based on the application form.6. The system of claim 1, wherein obtaining the user information comprises: authenticating a user of the client device; retrieving the user's account information based on the authentication; prefilling at least a portion of a vehicle financing application form based on the account information; and transmitting the application form for display on the client device.7. The system of claim 1, wherein the image is matched to more than one image in the database, and wherein the operations further comprise: transmitting the more than one matched images for display on the client device; transmitting instructions for requesting user input to verify the vehicle; and verifying the vehicle based on the user input.8. The system of claim 1, wherein the operations further comprise: determining a location of the vehicle based on geographical identification information embedded in the first image; obtaining a map of the dealership from the database, the map providing an expected location of the vehicle associated with the matched image; and comparing the expected location with the determined location to verify the vehicle.9. A computer-implemented method for processing an image including a vehicle using machine learning, the method comprising: determining a location of a client device; mapping the location of the client device to a vehicle dealership; receiving an image of a vehicle from the client device; matching, by a system using machine learning, the image to one or more images of vehicles in a database of the vehicle dealership, the database listing vehicles located at the vehicle dealership, the database including images of the listed vehicles, wherein matching includes: inputting the image to an input layer of a first convolutional neural network; extracting features indicating characteristics of the vehicle from an output layer of the first convolutional neural network; inputting the extracted features to an input layer of a second convolutional neural network; determining attributes of the vehicle from an output layer of the second convolutional neural network; and identifying the matched one or more images in the database as images having associated attributes matching the determined attributes; retrieving vehicle information from the database, based on the matched one or more images in the database; obtaining, using the client device, user information relating to a financing request for the vehicle shown in the image from a system associated with a financial services provider; determining a real-time quote for the vehicle based on the vehicle information and the user information; and transmitting the real-time quote for display on the client device.10. The method of claim 9, further comprising: determining that the image is unacceptable; and transmitting information for display on the client device, the information including an error message or user guidance.11. The system of claim 10, wherein the user guidance comprises a guiding line for obtaining a second image of the vehicle.12. The method of claim 9, wherein obtaining the user information comprises: authenticating a user of the client device; retrieving the user's account information based on the authentication, wherein the user's account information includes information from a pre-qualification vehicle financing application.13. The method of claim 9, wherein obtaining the user information further comprises: transmitting a vehicle financing application form for display on the client device; and receiving user input from the client device based on the application form.14. A non-transitory computer-readable medium storing instructions that, when executed by a processor, cause the processor to operate a computer system for processing an image including a vehicle using machine learning, comprising: determining a location of a client device; mapping the location of the client device to a vehicle dealership; receiving an image of a vehicle from the client device; matching, by a system using machine learning, the image to one or more images of vehicles in a database of the vehicle dealership, the database listing vehicles located at the vehicle dealership, the database including images of the listed vehicles, wherein matching includes: inputting the image to an input layer of a first convolutional neural network; extracting features indicating characteristics of the vehicle from an output layer of the first convolutional neural network; inputting the extracted features to an input layer of a second convolutional neural network; determining attributes of the vehicle from an output layer of the second convolutional neural network; and identifying the matched one or more images in the database as images having associated attributes matching the determined attributes; retrieving vehicle information from the database, based on the matched one or more images in the database; obtaining, using the client device, user information relating to a financing request for the vehicle shown in the image from a system associated with a financial services provider; determining a real-time quote for the vehicle based on the vehicle information and the user information; and transmitting the real-time quote for display on the client device; retrieving the user's account information based on the authentication, wherein the user's account information includes information from a pre-qualification vehicle financing application.",US10325315_B1.txt,"G06K9/62,G06N3/04,G06N3/08,G06Q40/02",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for']","database image matching using machine learning with output personalization A system for processing an image including a vehicle using machine learning can include a processor in communication with a client device, and a storage medium storing instructions that, when executed, cause the processor to perform operations including: determining a location of the client device; receiving an image of a vehicle from the client device; matching, using a machine learning algorithm, the image to one or more images of vehicles in a database, the database listing vehicles located at the determined location and including images of the vehicles; retrieving vehicle information from the database based on the matched one or more images in the database; obtaining user information relating to a financing request for the vehicle; determining a real-time quote for the vehicle based on the vehicle information and user information; and transmitting the real-time quote for display on the client device.",computing; calculating; counting
US2008043020_A1,2006-08-18,2008-02-21,2006-08-18,MICROSOFT CORPORATION,"OFEK, EYALLAWLER, STEPHEN L.THOTA, CHANDRASEKHARSNOW, BRADFORD J.WELSH, RICK D.NANDURI, JAYARAM N.M.ROWE, SEAN S.",39100974,road monitoring,user interface for viewing street side imagery,"The claimed subject matter provides a system and/or a method that facilitates providing an immerse view having at least one portion related to aerial view data and a disparate portion related to a first-person ground-level view. A receiver component can receive at least one of geographic data and an input. An interface component can generate an immersed view based on at least one of the geographic data and the input, the immersed view includes a first portion of aerial data and a second portion of a first-person perspective view corresponding to a location related to the aerial data.","1. A system that facilitates providing geographic data, comprising: a receiver component that receives at least one of geographic data and an input; and an interface component that generates an immersed view based on at least one of the geographic data and the input, the immersed view includes a first portion of aerial data and a second portion of at least one of a first-person perspective view and a third-person perspective view corresponding to a location related to the aerial data.2. The system of claim 1, the geographic data is at least one of 2-dimensional geographic data, 3-dimensional geographic data, aerial data, street-side imagery, a first-person perspective imagery data, a third-person perspective imagery data, video associated with geography, video data, ground-level imagery, planetary data, planetary ground-level imagery, satellite data, digital data, images related to a geographic location, orthographic map data, scenery data, map data, street map data, hybrid data related to geography data, road data, aerial imagery, and data related to at least one of a map, geography, and outer space.3. The system of claim 1, the input is at least one of a starting address, a starting point, a location, an address, a zip code, a state, a country, a county, a landmark, a building, an intersection, a business, a longitude, a latitude, a global positioning (GPS) coordinate, a user input, a mouse click, an input device signal, a touch-screen input, a keyboard input, a location related to land, a location related to water, a location related to underwater, a location related to outer space, a location related to a solar system, and a location related to an airspace.4. The system of claim 1, the first portion further comprising an orientation icon that can indicate the location and direction related to the aerial data.5. The system of claim 4, the orientation icon is at least one of an automobile, a bicycle, a person, a graphic, an arrow, an all-terrain vehicle, a motorcycle, a van, a truck, a boat, a ship, a submarine, a space ship, a bus, a plane, a jet, a unicycle, a skateboard, a scooter, a self-balancing human transporter, and an icon that provides a direction associated with the aerial data.6. The system of claim 4, the second portion of the at least one of the first-person perspective view and the third-person perspective view includes at least one of the following: a first section illustrating at least one of a first-person perspective view and a third-person perspective view based on a center direction indicated by the orientation icon on the aerial data; a second section illustrating at least one of a first-person perspective view and a third-person perspective view based on a left direction indicated by the orientation icon on the aerial data; and a third section illustrating at least one of a first-person perspective view and a third-person perspective view based on a right direction indicated by the orientation icon on the aerial data.7. The system of claim 6, further comprising a skin that provides an interior appearance wrapped around at least one of the first section, the second section, and the third section of the second portion, the skin corresponds to at least an interior aspect of the representative orientation icon.8. The system of claim 7, the skin is at least one of the following: an automobile interior skin; a sports car interior skin; a motorcycle first-person perspective skin; a person-perspective skin; a bicycle first-person perspective skin; a van interior skin; a truck interior skin; a boat interior skin; a submarine interior skin; a space ship interior skin; a bus interior skin; a plane interior skin; a jet interior skin; a unicycle first-person perspective skin; a skateboard first-person perspective skin; a scooter first-person perspective skin; and a self-balancing human transporter first perspective skin.9. The system of claim 1, the interface component allows at least one of a display of the immersed view and an interaction with the immersed view.10. The system of claim 1, further comprising an application programmable interface (API) that can format the immersed view for implementation on an entity.11. The system of claim 10, the entity is at least one of a device, a PC, a pocket PC, a tablet PC, a website, the Internet, a mobile communications device, a smartphone, a portable digital assistant (PDA), a hard disk, an email, a document, a component, a portion of software, an application, a server, a network, a TV, a monitor, a laptop, and a device capable of interacting with data.12. The system of claim 1, at least one of the orientation icon and the at least one of the first-person perspective view and the third-person perspective view is based upon at least one of the following paradigms: a car paradigm; a vehicle paradigm; a transporting device paradigm; a ground-level paradigm; a sea-level; a planet-level paradigm; an ocean floor-level paradigm; a designated height in the air paradigm; a designated height off the ground paradigm; and a particular coordinate paradigm.13. The system of claim 1, the first portion and the second portion of the immersed view are dynamically updated in real-time based upon the location of an orientation icon overlaying the aerial data giving a video-like experience.14. The system of claim 1, the second portion of the at least one of first-person perspective view and third-person perspective view includes a plurality of sections illustrating a respective first-person view based on a particular direction indicated by an orientation icon within the aerial data.15. The system of claim 1, further comprising a snapping ability that allows one of the following: an orientation icon to maintain a pre-established course in a dimension of space; an orientation icon to maintain a pre-established course upon the aerial data during a movement of the orientation icon; and an orientation icon to maintain a pre-established view associated with a location on the map to ensure optimal view of such location during a movement of the orientation icon.16. The system of claim 1, further comprising an indication within the immersed view that first-person perspective view imagery is unavailable by employing at least one of the following: an orientation icon that becomes semi-transparent to indicate imagery is unavailable; and an orientation icon that includes headlights, the headlights turn off to indicate imagery is unavailable.17. The system of claim 1, the immersed view further comprising a direct gesture that allows a selection and a dragging movement of the orientation icon on the aerial data such that the second portion illustrates a view that mirrors the direction of the dragging movement to enhance location targeting.18. A computer-implemented method that facilitates providing geographic data, comprising: receiving at least one of geographic data and an input; generating an immersed view with a first portion of map data and a second portion with at least one of first-person perspective data and third-person perspective data; and utilizing an orientation icon to identify a location on the aerial data to allow the second portion to display at least one of a first-person perspective data and a third-person data that corresponds to such location.19. The method of claim 18, further comprising: utilizing a snapping feature to maintain a course of navigation associated with the aerial data; and employing at least one skin with the second portion, the skin correlates to the orientation icon to simulate at least one of an interior perspective in context of the orientation icon.20. A computer-implemented system that facilitates providing an immersed view to display geographic data, comprising: means for receiving at least one of geographic data and an input; means for generating an immersed view based on at least one of the geographic data and the input; and means for including a first portion of aerial data and a second portion of a first-person perspective view corresponding to a location related to the aerial data within the immersed view.",US2008043020_A1.txt,G06T15/20,{'computing; calculating; counting'},"['image data processing or generation, in general']","user interface for viewing street side imagery The claimed subject matter provides a system and/or a method that facilitates providing an immerse view having at least one portion related to aerial view data and a disparate portion related to a first-person ground-level view. A receiver component can receive at least one of geographic data and an input. An interface component can generate an immersed view based on at least one of the geographic data and the input, the immersed view includes a first portion of aerial data and a second portion of a first-person perspective view corresponding to a location related to the aerial data.",computing; calculating; counting
US2021014678_A1,2018-03-19,2021-01-14,2018-03-19,FORD MOTOR COMPANY,"AICH, SUDIPTOTANG, CHIH-WEISEAGRAVES, JAMELKOCK, Beaudry",67987422,gps,proximity-based shared transportation reservations,"Various approaches allow for shared transportation reservations and related actions to be made based on a user's proximity to a shared transportation station. Data is received concerning the user's computing device, such as a vehicle, a mobile device, an internet-connected device, or a cellular network-connected device. Approaches can use a variety of data formats, including image data, location coordinates, or wirelessly-transmitted data. Based on such data and a criterion such as a threshold proximity to the station, the user's account status is determined and access to same authorized. Via the account, numerous actions may be taken, including automatically triggering a transport mode reservation or return, prompting the user to reserve or return the shared transportation, or debiting the user per a fee associated with the transportation reservation. Further, neural network techniques may be applied to train a model for generating related recommendations.","1. A computer-implemented method, comprising: receiving location coordinates associated with a geographic location of a computing device; receiving user identification information; determining a registered user from the user identification information, the registered user associated with a user account of a service provider; authorizing access to the user account; determining that the geographic location is within a threshold distance to a transport station; and triggering a transport mode reservation event in the user account.2. The computer-implemented method of claim 1, wherein the transport mode reservation event comprises at least one of: finalizing a transportation reservation, debiting the user account an amount associated with the transportation reservation event, or providing a notification to confirm a transport reservation or otherwise corresponding to the transport mode reservation event.3. The computer-implemented method of claim 2, wherein the notification is a prompt concerning at least one transportation reservation opportunity.4. The computer-implemented method of claim 1, wherein the location coordinates are Global Positioning System (GPS) navigation system data.5. A computer-implemented method, comprising: receiving data associated with a user device; determining a registered user from the data, the registered user associated with a user account of a service provider; authorizing access to the user account; determining that the user device satisfies a transport reservation criterion; and triggering a transport mode reservation event in the user account.6. The computer-implemented method of claim 5, wherein the transport reservation criterion is the user device being within a threshold distance to a transport station.7. The computer-implemented method of claim 6, wherein the transport station is a bicycle station.8. The computer-implemented method of claim 5, wherein the transport mode reservation event comprises at least one of: finalizing a transportation reservation, debiting the user account an amount associated with the transport mode reservation event, or providing a notification to confirm a transport reservation or otherwise corresponding to the transport mode reservation event.9. The computer-implemented method of claim 5, wherein the data comprises at least one of: image data obtained from a camera, location coordinates, or Bluetooth or other wirelessly-transmitted data.10. The computer-implemented method of claim 5, wherein the data is image data, the method further comprising: analyzing the image data to determine a feature vector; comparing the feature vector to a plurality of stored feature vectors; and determining a stored feature vector from the plurality of stored feature vectors that matches the feature vector to at least a threshold deviation, the stored feature vector associated with the registered user.11. The computer-implemented method of claim 5, wherein the user device is at least one of: a vehicle, a mobile device, an internet-connected device, or a cellular network-connected device.12. The computer-implemented method of claim 5, further comprising: using a trained model to generate a recommendation for transport mode reservation events.13. A computing system, comprising: at least one processor; and memory including instructions that, when executed by the at least one processor, cause the computing system to: receive data concerning a user device; determine a registered user from the data, the registered user associated with a user account of a service provider; authorize access to the user account; determine that the user device satisfies a transport reservation criterion; and trigger a transport mode reservation event in the user account.14. The computing system of claim 13, wherein the transport reservation criterion is the user device being within a threshold distance to a transport station.15. The computing system of claim 13, wherein the transport mode reservation event comprises at least one of: finalizing a transportation reservation, debiting the user account an amount associated with the transport mode reservation event, or providing a notification to confirm a transport reservation or otherwise corresponding to the transport mode reservation event.",US2021014678_A1.txt,"G01C21/34,G06Q10/02,G06Q50/30,H04W12/00","{'measuring; testing', 'electric communication technique', 'computing; calculating; counting'}","['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for', 'wireless communication networks (broadcast communication h04h; communication systems using wireless links for non-selective communication, e.g. wireless extensions h04m1/72)']","proximity-based shared transportation reservations Various approaches allow for shared transportation reservations and related actions to be made based on a user's proximity to a shared transportation station. Data is received concerning the user's computing device, such as a vehicle, a mobile device, an internet-connected device, or a cellular network-connected device. Approaches can use a variety of data formats, including image data, location coordinates, or wirelessly-transmitted data. Based on such data and a criterion such as a threshold proximity to the station, the user's account status is determined and access to same authorized. Via the account, numerous actions may be taken, including automatically triggering a transport mode reservation or return, prompting the user to reserve or return the shared transportation, or debiting the user per a fee associated with the transportation reservation. Further, neural network techniques may be applied to train a model for generating related recommendations.",measuring; testing electric communication technique computing; calculating; counting
CN111401438_A,2020-03-13,2020-07-10,2020-03-13,CONTROLEXPERT CHINA COMPANY,WANG XIAOCHUN,71432500,vehicle classification,"image sorting method, device and system","The invention discloses an image sorting method, device and system. The method comprises the steps: obtaining a first image set associated with a vehicle insurance case; inputting the first image setinto a machine learning model for analysis to obtain the type of each image in the first image set, and allocating a label for indicating the type of each image to each image in the first image set; and pushing each image in the first image set and the classification label corresponding to each image to a storage position corresponding to the type. According to the invention, the technical problems of tedious operation, time consumption, low efficiency and high error rate caused by manual photo sorting in the prior art are solved.","1. an image sorting method, comprising: acquiring a first image set associated with a vehicle insurance case; inputting the first image set into a machine learning model for analysis to obtain the type of each image in the first image set, and allocating a label for indicating the type of each image to each image in the first image set; and respectively pushing each image in the first image set and the classification label corresponding to each image to a storage position corresponding to the type. 2. the method of claim 1, wherein obtaining a first set of images associated with a vehicle insurance case comprises: acquiring identity information of a target object; and when the identity information passes the verification, receiving the image uploaded by the target object, and storing the image uploaded by the target object into the first image set. 3. the method of claim 1, wherein before pushing each image of the first set of images and the classification label corresponding to the each image to the storage location corresponding to the type, the method further comprises: determining a first image type list corresponding to the car insurance case; counting the image types of all the images in the first image set to obtain a second image type list; comparing the first image type list with the second image type list to obtain a comparison result; and determining whether to respectively push each image in the first image set and the classification label corresponding to each image to a storage position corresponding to the type according to the comparison result. 4. the method of claim 3, wherein determining whether to push each image in the first set of images and the classification label corresponding to the each image to a storage location corresponding to the type respectively according to the comparison result comprises: when the comparison result indicates that the first image category list is inconsistent with the second image type list, determining to suspend pushing each image of the first image set and the classification label corresponding to each image to a storage position corresponding to the type respectively; when the comparison result indicates that the first image category list is consistent with the second image type list, determining that the images of the first image set and the classification labels corresponding to the images are allowed to be respectively pushed to storage positions corresponding to the types. 5. the method of claim 4, wherein after determining to suspend pushing each image of the first set of images and the classification label corresponding to the each image to the storage location corresponding to the type, respectively, the method further comprises: when the comparison result indicates that the image type in the second image type list is the partial image type in the first image type list, generating first prompt information, wherein the first prompt information is used for prompting to continue to acquire the image associated with the car insurance case; and when the comparison result indicates that the image types in the second image type list contain all the image types in the first image type list and a specified image type which does not exist in the first image type list exists, generating second prompt information, wherein the second prompt information is used for prompting whether an image corresponding to the specified image type is reserved or not. 6. the method of claim 5, wherein after generating the second prompting message, the method further comprises: receiving a confirmation instruction from the target object; when the confirmation instruction is to delete the image of the specified image type, deleting the image of the specified type from the first image set, and respectively pushing each image in the first image set after deleting the image of the specified type to a corresponding storage position; and when the confirmation instruction is to reserve the image of the specified image type, continuing to push each image in the first image set to a corresponding storage position respectively. 7. the method of claim 1, wherein pushing each image in the first set of images and the classification label corresponding to the each image to a storage location corresponding to the type comprises: and storing the images and the classification labels corresponding to the images into a server for evaluating the car insurance cases. 8. the method of claim 1, wherein after inputting the first set of images into a machine learning model for analysis, and obtaining the type of each image in the first set of images, the method further comprises: determining an accuracy rate of the type of each image; and when the accuracy is smaller than a preset threshold value, adjusting the model parameters of the machine learning model. 9. the method according to any one of claims 1 to 8, wherein the type of each image comprises at least one of: identity card, driving license, car damage photo, injury photo, object damage photo, scene photo, document image. 10. a method for displaying data, comprising: displaying the acquired first image set associated with the car insurance case; displaying the type of each image in the first image set obtained by inputting the first image set into a machine learning model for analysis, and displaying a label which is distributed to each image in the first image set and used for indicating the type of each image; and displaying the storage positions of the images in the first image set and the classification labels corresponding to the images. 11. an image sorting apparatus, characterized by comprising: an acquisition module for acquiring a first set of images associated with a vehicle insurance case; the analysis module is used for inputting the first image set into a machine learning model for analysis to obtain the type of each image in the first image set, and allocating a label for indicating the type of each image to each image in the first image set; and the storage module is used for respectively pushing each image in the first image set and the classification label corresponding to each image to a storage position corresponding to the type. 12. an image sorting system, comprising: the system comprises an image acquisition device, a network side device and a communication device, wherein the image acquisition device is used for acquiring images related to a vehicle insurance case to obtain a first image set and uploading the first image set to the network side device; the network side device is configured to input the first image set to a machine learning model for analysis, obtain a type of each image in the first image set, and allocate a label for indicating the type of each image to each image in the first image set; and respectively pushing each image in the first image set and the classification label corresponding to each image to a storage position corresponding to the type. 13. a non-volatile storage medium, characterized in that the storage medium includes a stored program, wherein an apparatus in which the storage medium is located is controlled to execute the image sorting method according to any one of claims 1 to 9 when the program is executed. 14. a processor for executing a program stored in a memory connected to the processor, wherein the program is executed to perform the image sorting method according to any one of claims 1 to 9.",CN111401438_A.txt,"G06F16/55,G06K9/62,G06N20/00,G06Q40/08",{'computing; calculating; counting'},"['electric digital data processing (computer systems based on specific computational models g06n)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for']","image sorting method, device and system The invention discloses an image sorting method, device and system. The method comprises the steps: obtaining a first image set associated with a vehicle insurance case; inputting the first image setinto a machine learning model for analysis to obtain the type of each image in the first image set, and allocating a label for indicating the type of each image to each image in the first image set; and pushing each image in the first image set and the classification label corresponding to each image to a storage position corresponding to the type. According to the invention, the technical problems of tedious operation, time consumption, low efficiency and high error rate caused by manual photo sorting in the prior art are solved.",computing; calculating; counting
US2020273342_A1,2019-06-06,2020-08-27,2019-02-27,ANI TECHNOLOGIES PRIVATE,"KOMPALLI, PRAMOD SANKARNAGARAJAN, SATHYA NARAYANANNIGAM, APURVVIDAPANAKAL, SRIKANTH",72142035,road monitoring,calibration of fixed image-capturing device for predicting driving lane,"Lane prediction for driving assistance is provided. A camera of a vehicle is calibrated to obtain column values for each row of pixels of an image based on a plurality of lines captured in the image. The plurality of lines may be either parallel or perpendicular to a calibration lane captured in the image. The column values of each row of pixels may be utilized to predict a driving lane for the vehicle. When an on-road object is detected in the predicted driving lane, a warning message for an impending collision may be generated and communicated to a driver of the vehicle, thereby facilitating driving assistance to the driver in real-time.","1. A method, comprising: identifying, by circuitry, in a first image including a plurality of lines on a calibration lane of a first object, a first plurality of rows of pixels corresponding to the plurality of lines perpendicular to the calibration lane; determining, by the circuitry, from the first image, a first width of a first line corresponding to a first row of pixels of the first plurality of rows of pixels; determining, by the circuitry, a second width of a second line corresponding to a second row of pixels of a second plurality of rows of pixels in the first image based on at least the first width and an angle between the first line and the second line, wherein the angle is formed by joining endpoints of the first line and the second line; estimating, by the circuitry, column values for each line corresponding to each row of pixels in the first image based on at least one of the first width or the second width associated with a corresponding line; and storing, by the circuitry in a memory, the estimated column values for each line, wherein a driving lane for the first object is predicted based on the stored column values, and wherein a second object in a second image is analyzed for impending collision with the first object when the second object is detected in the driving lane of the first object.2. The method of claim 1, further comprising: converting, by the circuitry, the first image from a first color model to a second color model; and filtering, by the circuitry, the converted first image to obtain a filtered first image including a predetermined color, wherein the plurality of lines is identified from the first image by executing edge detection of the filtered first image.3. The method of claim 2, wherein the first and second color models are at least one of a black and white color model, a greyscale color model, a red, green, and blue (RGB) color model, a hue, saturation, and value (HSV) color model, a cyan, magenta, yellow, and black (CMYK) color model, a hue, saturation, and brightness (HSB) color model, a hue, saturation, and lightness (HSL) color model, or a hue, chroma, and value (HCV) color model, and wherein the first color model is different from the second color model.4. The method of claim 1, wherein the first image comprises the first and second pluralities of rows of pixels.5. The method of claim 1, further comprising: detecting, by the circuitry, a bottom edge, a first side edge, a second side edge, and a top edge of the second object in the second image; generating, by the circuitry, a bounding box including the second object based on the bottom edge, the first side edge, the second side edge, and the top edge; and estimating, by the circuitry, an overlapping area between the bounding box and the driving lane, wherein the second object is detected for the impending collision when the overlapping area is greater than or equal to a threshold area.6. The method of claim 5, further comprising: generating, by the circuitry, a warning message based on at least a distance of the second object from the first object, when the overlapping area is greater than or equal to the threshold area; and communicating, by the circuitry to an entity associated with the first object, the warning message indicating the impending collision.7. The method of claim 1, wherein the first image and the second image are captured by an image-capturing device that is mounted at a center of the first object such that the first image and the second image are equally divided into two halves along an axis passing via the image-capturing device, and wherein the first image and the second image include an equal number of row pixels and column pixels.8. The method of claim 7, wherein the first width and the second width are determined in terms of number of column pixels in the first image.9. A method, comprising: identifying, by circuitry, in a first image of a calibration lane of a first object, a plurality of lines parallel to the calibration lane; determining, by the circuitry, for each row of pixels in the first image, an observed width between the plurality of lines; estimating, by the circuitry, for each row of pixels in the first image, column values based on the observed width of a corresponding row of pixels, a first object width of the first object, and an actual lane width between the plurality of lines; and storing, by the circuitry in a memory, the estimated column values for each row of pixels, wherein a driving lane for the first object is predicted based on the stored column values, and wherein a second object in a second image is analyzed for impending collision with the first object when the second object is detected in the driving lane of the first object.10. The method of claim 9, further comprising: converting, by the circuitry, the first image from a first color model to a second color model; and filtering, by the circuitry, the converted first image to obtain a filtered first image including a predetermined color, wherein the plurality of lines is identified from the first image by executing edge detection of the filtered first image.11. The method of claim 10, wherein the first and second color models are at least one of a black and white color model, a greyscale color model, a red, green, and blue (RGB) color model, a hue, saturation, and value (HSV) color model, a cyan, magenta, yellow, and black (CMYK) color model, a hue, saturation, and brightness (HSB) color model, a hue, saturation, and lightness (HSL) color model, or a hue, chroma, and value (HCV) color model, and wherein the first color model is different from the second color model.12. The method of claim 9, further comprising: detecting, by the circuitry, a bottom edge, a first side edge, a second side edge, and a top edge of the second object in the second image; generating, by the circuitry, a bounding box including the second object based on the bottom edge, the first side edge, the second side edge, and the top edge; and estimating, by the circuitry, an overlapping area between the bounding box and the driving lane, wherein the second object is detected for the impending collision when the overlapping area is greater than or equal to a threshold area.13. The method of claim 12, further comprising: generating, by the circuitry, a warning message based on at least a distance of the second object from the first object, when the overlapping area is greater than or equal to the threshold area; and communicating, by the circuitry to an entity associated with the first object, the warning message indicating the impending collision.14. The method of claim 9, wherein the first image and the second image are captured by an image-capturing device that is mounted at a center of the first object such that the first image and the second image are equally divided into two halves along an axis passing via the image-capturing device.15. The method of claim 14, wherein the first image and the second image include an equal number of row pixels and column pixels, and wherein the observed width is determined in terms of the number of column pixels in the first image.16. A method, comprising: identifying, by circuitry, from a first image including a calibration lane of a first object, a plurality of lines; determining, by the circuitry, a first width of each of a plurality of rows of pixels in the first image based on at least one of the plurality of lines; estimating, by the circuitry, for each of the plurality of rows of pixels in the first image, column values based on at least the first width of a corresponding row of pixels; and storing, by the circuitry in a memory, the estimated column values for each row of pixels, wherein a driving lane for the first object is predicted based on the stored column values, and wherein a second object in a second image is analyzed for impending collision with the first object when the second object is detected in the driving lane of the first object.17. The method of claim 16, wherein the plurality of lines is perpendicular to the calibration lane of the first object.18. The method of claim 16, wherein the plurality of lines is parallel to the calibration lane of the first object.19. The method of claim 16, further comprising: detecting, by the circuitry, a bottom edge, a first side edge, a second side edge, and a top edge of the second object in the second image; generating, by the circuitry, a bounding box including the second object based on the bottom edge, the first side edge, the second side edge, and the top edge; and estimating, by the circuitry, an overlapping area between the bounding box and the driving lane, wherein the second object is detected for the impending collision when the overlapping area is greater than or equal to a threshold area.20. The method of claim 19, further comprising: generating, by the circuitry, a warning message based on at least a distance of the second object from the first object, when the overlapping area is greater than or equal to the threshold area; and communicating, by the circuitry to an entity associated with the first object, the warning message indicating the impending collision.",US2020273342_A1.txt,"B60R1/00,G06K9/00,G06K9/36,G06T7/13,G06T7/60,G06T7/80,G08G1/16","{'signalling', 'computing; calculating; counting', 'vehicles in general'}","['vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general', 'image data processing or generation, in general', 'image data processing or generation, in general', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})']","calibration of fixed image-capturing device for predicting driving lane Lane prediction for driving assistance is provided. A camera of a vehicle is calibrated to obtain column values for each row of pixels of an image based on a plurality of lines captured in the image. The plurality of lines may be either parallel or perpendicular to a calibration lane captured in the image. The column values of each row of pixels may be utilized to predict a driving lane for the vehicle. When an on-road object is detected in the predicted driving lane, a warning message for an impending collision may be generated and communicated to a driver of the vehicle, thereby facilitating driving assistance to the driver in real-time.",signalling computing; calculating; counting vehicles in general
US10507793_B1,2018-08-17,2019-12-17,2018-08-17,"DE MOURA PARTIKA, FELIPE BORIS","DE MOURA PARTIKA, FELIPE BORIS",68841471,anomaly detection,"alarm, safety device and device for expelling attackers for motor vehicles","An alarm, safety device and device for expelling attackers for vehicles, to protect the vehicle and its occupants when stopped, parked or moving, containing a sound alarm, a lighting alarm, a surveillance camera inside the vehicle for face recognition of a non-authorized driver, a remote system for sending information to the police authority, having a smoke release system, surveillance cameras and sensors installed within boxes at external locations of the vehicle. The surveillance cameras monitor the external activity of the vehicle, the images as received by the cameras being analyzed in real time by an internal processing center, provided with an artificial intelligence processor, processing those images by a public domain algorithm, trained for recognizing violence-related objects, wherein the images are processed and transformed into data matrices which are submitted to a neural open code network, for recognizing the presence of violence-related objects in the hands of a person in or near the vehicle.","1. An alarm, safety device and device for expelling attackers for motor vehicles, used to protect a stopped, parked, or moving motor vehicle and its occupants being in the stopped, parked or moving motor vehicle comprising: a sound alarm, a lighting alarm, a surveillance camera inside the motor vehicle, for face recognition of a non-authorized driver, a remote system for sending information to a police authority, a smoke release system, surveillance cameras and sensors installed within boxes at external locations of the motor vehicle, wherein the surveillance cameras are intended to monitor the external activity of said motor vehicle, the images as received by said surveillance cameras are analyzed in real time by an internal processing center, provided with an artificial intelligence processor, processing images by a public domain algorithm, using deep learning with an open source neural network, trained for recognizing weapons, wherein images are processed by an open source library, where they are transformed into data matrices which are submitted to the open source neural network that recognizes the weapons, wherein when the artificial intelligence processor recognizes the presence of a weapon in the hands of a person who is near said motor vehicle, and, when detecting violent behavior, the alarm safety device enters a protection mode and starts steps to protect the occupants of the motor vehicle, by issuing a sound alarm, a lighting alarm, by using a horn and lights of the motor vehicle, and activating smoke release devices, jointly installed with the surveillance cameras and the sensors, wherein a smoke curtain is released by a tube towards said person, repelling an attack/robbery and causing said person to evade, simultaneously with the recording of images received by the surveillance cameras and remote remittance of said images and a location of the motor vehicle using a global positioning system (GPS) to the police authority, informing occurrence and publishing location information on an encrypted open code data network.2. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, wherein the public domain algorithm with artificial intelligence and deep learning from an artificial intelligence center, which is installed on the processor of the internal security center, is also able to analyze, from images received from the surveillance cameras, information transmitted by the sensors to detect the violent behavior.3. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, wherein the images received by the surveillance cameras and the information received by the sensors, are analyzed by an artificial intelligence center in real time, and stored on a memory of the internal security center and remotely sent to an external computer in the security center, wherein said data is processed by said external computer, using the public domain algorithm with artificial intelligence and deep learning, to improve and enhance the recognition of violent behavior, with system updating, wherein the external computer, after the system is updated, remotely sends that update back to users, for automatic and non-stop update of the internal processing center, installed at the internal security center in the motor vehicles.4. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, wherein improvements in the algorithm for recognizing violent behavior as performed by the external computer are remotely sent from said external computer to the internal processing center of the alarm system as installed on the internal security center of motor vehicles, permanently updating and improving the system.5. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, further comprising an alarm system, comprising the internal processing center, which, by analyzing images and information sent by the cameras and/or sensors, recognizes violent behavior near the motor vehicle, and enters protection mode by activating defenses in the motor vehicle, duly installed within the boxes, installed at external locations of said motor vehicle, by issuing the sound alarm and the lighting alarm, wherein both the sound alarm and the lighting alarm use horns and lights, respectively, already installed in the motor vehicle.6. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, wherein when a possibility of violent behavior persists, because the person has not been repelled by the sound alarm and/or the lighting alarm, the internal security center activates, with no interference from an occupant of the motor vehicle, the smoke release device or another element for expelling the person, throwing a smoke curtain using the tube, towards the person, so to repel the attack.7. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, wherein the protection mode for the alarm system is configured to be manually activated by an occupant of the motor vehicle using a panic button installed within said motor vehicle at a hidden place, to avoid its identification from outside the motor vehicle, wherein the occupant of the motor vehicle, when activating the panic button, activates the protection mode of the alarm system and its functions are the same as disclosed upon an automatic activation of the protection mode.8. The alarm, safety device and device for expelling attackers for motor vehicles of claim 5, the alarm system, upon entering protection mode, enables recording of the images of surveillance cameras and sensors on a memory of the internal security center, wherein said internal security center remotely sends such images to the police authority (18), jointly with the location of the vehicle, obtained by the GPS, or where an occupant of the motor vehicle previously decides.9. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, wherein the boxes comprise two interconnected cases, which are installed on windows, windscreens and rear window of the motor vehicles, a first one of the cases located inside the motor vehicle and a second one of the cases located outside the motor vehicle, wherein the two cases are interconnected by connectors, so to fix them to the window, the windscreen or the rear window and allow passage of a transmission wire from the surveillance cameras, a data transmission wire from the sensors and an outlet tube for the smoke produced by the smoke release device.10. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, at least one of the boxes is installed on outer rearview mirrors of the motor vehicle and comprises two interconnected cases, wherein the two interconnected cases are installed by embedding one of the cases into a first one of the outer rearview mirrors, and fixing another one of the cases to an external side of a second one of the rearview mirrors, with the surveillance camera, the sensors and the smoke release device turned to a front of the motor vehicle.11. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, wherein the boxes comprise an internal case and an external case, and the smoke release device is installed within the internal and external cases, and comprises a container installed in the internal case, the container containing an electric resistor and a fluid, wherein the electric resistor, upon receipt of an instruction from the internal processing center, heats the fluid which, upon being heated by the electric resistor, evaporates thus becoming smoke, which is pumped by an electric pump or expelled by injection of compressed gas, through a compressed gas tube disposed between the internal case and the external case, to an outlet tube disposed on the external case, being expelled by the outlet tube towards the person.12. The alarm, safety device and device for expelling attackers for motor vehicles of claim 11, wherein refilling the fluid of the electrical resistor and the connected tube, constituting the smoke release device, is accomplished by removing the container from the external case, filling the container with new fluid and returning the container into the internal case; and replacing the electrical resistor is accomplished by removing the container from the external case, replacing a damaged electrical resistor with a new electrical resistor and returning the container into the internal case; and replacing the compressed gas tube is accomplished by opening the internal case, removing said compressed gas tube, replacing the compressed gas tube with a new full compressed gas tube and closing the internal case.13. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, wherein the system allows, besides the smoke release device, the use of pepper spray, tear gas, paralyzing shots, electric shock, glaring light, and/or another kind of gas, which are activated by the internal security center, so to repel the person, when the alarm enters protection mode.14. The alarm, safety device and device for expelling attackers for motor vehicles of claim 11, wherein the internal security center, the surveillance cameras, the sensors, the smoke release device and the electric pump supplied by a rechargeable self-autonomous battery, which is connected to a battery of the motor vehicle, for a full recharge.15. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, wherein the internal security center is housed inside the motor vehicle in a protected and shielded place, where the artificial intelligence processor, memories, the remote communication system and additional electric/electronic components are disposed.16. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, wherein the boxes with safety devices are installed in any quantity on the windows, windscreens, rear window and/or external rearview mirrors, wherein the internal processing center synchronizes all the boxes, so to activate them simultaneously and/or only by pre-determined events.17. The alarm, safety device and device for expelling attackers for motor vehicles of claim 1, wherein the alarm system is configured to permit remote connection to the police department, to private intranet networks, public internet, integration with mobile phone applications, or where the user decides, allowing for data exchange, image exchange, location of the motor vehicle, mapping of locations of risk, software update for the device, publication of location information at the encrypted open code data network.",US10507793_B1.txt,"B60Q5/00,B60Q9/00,B60R25/10,B60R25/102,B60R25/104,B60R25/30,B60R25/31,G08B13/196,G08B15/02","{'signalling', 'vehicles in general'}","['arrangement of signalling or lighting devices, the mounting or supporting thereof or circuits therefor, for vehicles in general', 'arrangement of signalling or lighting devices, the mounting or supporting thereof or circuits therefor, for vehicles in general', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'signalling or calling systems; order telegraphs; alarm systems', 'signalling or calling systems; order telegraphs; alarm systems']","alarm, safety device and device for expelling attackers for motor vehicles An alarm, safety device and device for expelling attackers for vehicles, to protect the vehicle and its occupants when stopped, parked or moving, containing a sound alarm, a lighting alarm, a surveillance camera inside the vehicle for face recognition of a non-authorized driver, a remote system for sending information to the police authority, having a smoke release system, surveillance cameras and sensors installed within boxes at external locations of the vehicle. The surveillance cameras monitor the external activity of the vehicle, the images as received by the cameras being analyzed in real time by an internal processing center, provided with an artificial intelligence processor, processing those images by a public domain algorithm, trained for recognizing violence-related objects, wherein the images are processed and transformed into data matrices which are submitted to a neural open code network, for recognizing the presence of violence-related objects in the hands of a person in or near the vehicle.",signalling vehicles in general
US10395313_B1,2018-03-08,2019-08-27,2018-03-08,CAPITAL ONE SERVICES,"BAJAJ, RAMANTANG, QIAOCHUYAJNIK, SANJIVDAGLEY, GEOFFREYVASISHT, SUNIL SUBRAHMANYAMPRICE, MICAHWYLIE, STEPHEN MICHAELHOOVER, JASON RICHARDDUGAL, ARJUN",67700505,vehicle classification,image analysis and identification using machine learning with output personalization,"A system for processing an image including a vehicle using machine learning can include a processor in communication with a client device, and a storage medium storing instructions that, when executed, cause the processor to perform operations including: receiving an image of a vehicle from the client device; extracting one or more features from the image; based on the extracted features and using a machine learning algorithm, determining a make and a model of the vehicle; obtaining user information relating to a financing request for the vehicle; determining a real-time quote for the vehicle based on the make, the model, and the user information; and transmitting the real-time quote for display on the client device.","1. A system for processing an image including a vehicle using machine learning, comprising: a processor in communication with a client device; and a storage medium storing instructions that, when executed, configure the processor to perform operations comprising: authenticating a user of the client device; retrieving the user's account information based on the authentication, wherein the user's account information includes information from a pre-qualification vehicle financing application; receiving an image of a vehicle from the client device; inputting the image to an input layer of a first convolutional neural network; extracting one or more features from an output layer of the image using a first convolutional neural network; inputting the extracted features to an input layer of a second convolutional neural network; determining a make and a model of the vehicle from an output layer of the second convolutional neural network; obtaining user information relating to a financing request for the vehicle; determining a real-time quote for the vehicle based on the make, the model, and the user information; and transmitting the real-time quote for display on the client device.2. The system of claim 1, wherein the extracted features include at least one of a logo of the vehicle and a shape of headlights of the vehicle.3. The system of claim 1, wherein the operations further comprise: determining that the image is unacceptable; and transmitting information for display on the client device, the information including an error message or user guidance.4. The system of claim 3, wherein the user guidance comprises a guiding line for obtaining a second image of the vehicle.5. The system of claim 1, wherein obtaining the user information comprises: transmitting a vehicle financing application form for display on the client device; and receiving user input from the client device based on the application form.6. The system of claim 1, wherein obtaining the user information comprises: authenticating a user of the client device; retrieving the user's account information based on the authentication; prefilling at least a portion of a vehicle financing application form based on the account information; and transmitting the application form for display on the client device.7. The system of claim 1, wherein: the operations further comprise obtaining one or more references images using a computer network, and wherein determining the make and the model of the vehicle is further based on comparison between the one or more references images and the received image.8. The system of claim 7, wherein the comparison is performed using the second convolutional neural network.9. A computer-implemented method for processing an image including a vehicle using machine learning, the method comprising: authenticating a user of the client device; retrieving the user's account information based on the authentication, wherein the user's account information includes information from a pre-qualification vehicle financing application; receiving an image of a vehicle from a client device; inputting the image to an input layer of a first convolutional neural network; extracting one or more features from an output layer of the image using a first convolutional neural network; inputting the extracted features to an input layer of a second convolutional neural network; determining a make and a model of the vehicle from an output layer of the second convolutional neural network; obtaining user information relating to a financing request for the vehicle; determining a real-time quote for the vehicle based on the vehicle information and user information; and transmitting the real-time quote for display on the client device.10. The method of claim 9, further comprising, based on the identified attributes of the vehicle, determining a year and a trim of the vehicle.11. The method of claim 9, further comprising: determining that the image is unacceptable; and transmitting information for display on the client device, the information including an error message or user guidance.12. The system of claim 11, wherein the user guidance comprises a guiding line for obtaining a second image of the vehicle.13. The method of claim 9, wherein obtaining the user information further comprises: transmitting a vehicle financing application form for display on the client device; and receiving user input from the client device based on the application form.14. A non-transitory computer-readable medium storing instructions that, when executed by a processor, cause the processor to operate a computer system for processing an image including a vehicle using machine learning, comprising: authenticating a user of the client device; retrieving the user's account information based on the authentication, wherein the user's account information includes information from a pre-qualification vehicle financing application; receiving an image of a vehicle from a client device; inputting the image to an input layer of a first convolutional neural network; extracting one or more features from an output layer of the image using a first convolutional neural network; inputting the extracted features to an input layer of a second convolutional neural network; determining a make and a model of the vehicle from an output layer of the second convolutional neural network; obtaining user information relating to a financing request for the vehicle; determining a real-time quote for the vehicle based on the vehicle information and user information; and transmitting the real-time quote for display on the client device.",US10395313_B1.txt,"G06F17/24,G06K9/62,G06N3/04,G06N3/08,G06Q40/02",{'computing; calculating; counting'},"['electric digital data processing (computer systems based on specific computational models g06n)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for']","image analysis and identification using machine learning with output personalization A system for processing an image including a vehicle using machine learning can include a processor in communication with a client device, and a storage medium storing instructions that, when executed, cause the processor to perform operations including: receiving an image of a vehicle from the client device; extracting one or more features from the image; based on the extracted features and using a machine learning algorithm, determining a make and a model of the vehicle; obtaining user information relating to a financing request for the vehicle; determining a real-time quote for the vehicle based on the make, the model, and the user information; and transmitting the real-time quote for display on the client device.",computing; calculating; counting
CN108595576_A,2018-04-17,2018-09-28,2018-04-17,GOSUNCN TECHNOLOGY GROUP COMPANY,HOU YUQINGHUANG TONGYULIU SHUANGGUANGHE CANHAOSONG YIBINGWANG GANGGAO WENGUO,63622835,vehicle classification,vehicle search-by-image method based on database,"The invention provides a vehicle search-by-image method based on a database. The method comprises the following steps that: obtaining a vehicle body area in a target vehicle image; extracting the characteristics of the vehicle body area in the target vehicle image, and displaying the characteristics; and according to the characteristic retrieval database of the vehicle body area, obtaining an image similar to the vehicle body area of the target vehicle. The method has the beneficial effects that the vehicle search-by-image method based on the database can effectively solve the problems of a great quantity of complex calculation, cost and the like by deep learning can be effectively solved, the problems of complex calculation and model verification brought by the deep learning do not need to be considered, and in addition, pictures with high retrieval similarity can be searched in a second level on the basis of the search by image of the database.","1. A database-based vehicle map searching method is characterized by comprising the following steps: acquiring a vehicle body area in a target vehicle image; extracting features of a vehicle body region in a target vehicle image and displaying the features; and searching a database according to the characteristics of the vehicle body area to obtain an image similar to the vehicle body area of the target vehicle. 2. The database-based vehicle map searching method according to claim 1, wherein the database is determined according to vehicle images in a gate system, and comprises the following steps: acquiring an image of each vehicle in the checkpoint system; and extracting the characteristics of each vehicle image in the checkpoint system and storing the characteristics in a database. 3. The database-based vehicle mapping method according to claim 1, wherein the step of extracting the features of the body region in the image of the target vehicle, searching the database according to the features of the body region, and obtaining the image similar to the body region of the target vehicle comprises: dividing the characteristics of the vehicle body area into a plurality of priority levels, extracting the characteristics of the vehicle body area according to the priority levels, retrieving according to the characteristics of the vehicle body area to obtain an image similar to the characteristics of the vehicle body area of the target vehicle, wherein the retrieval range of the characteristics of the vehicle body area is a similar image retrieved by the characteristics of the vehicle body area with the priority of the previous layer, and the retrieval range of the characteristics of the vehicle body area with the highest priority is a database. 4. The database-based vehicle map searching method according to claim 3, wherein the priority levels of the vehicle body regions are set to five layers, namely, the highest priority, the high priority, the common priority, the low priority and the lowest priority; the highest priority is the overall outline characteristic of the front of the vehicle; the high priority is one or more of vehicle lamp characteristics, license plate character characteristics, vehicle logo characteristics, vehicle front end grille characteristics and bumper air opening characteristics; the common priority is one or more of insurance identification character characteristics, annual inspection identification character characteristics, upper half body outline and face characteristics of drivers and conductors and suspension part characteristics; the low priority is a skylight structural feature; the lowest priority is a vehicle color feature. 5. The database-based vehicle mapping method of claim 4, wherein the database is updated at least once a year. 6. The database-based vehicle mapping method according to claim 1, further comprising, after the step of obtaining the body region in the target vehicle image, the steps of: setting four calibration points in a vehicle body area in the acquired target vehicle image, wherein the four calibration points are respectively a left side rearview mirror, a right side rearview mirror, a left side front tire and a right side front tire; performing trapezoidal distortion correction on the vehicle body area in the acquired target vehicle image according to the calibration point; and outputting the body region in the trapezoidal distortion corrected target vehicle image. 7. The database-based vehicle mapping method according to claim 1, wherein before the obtaining of the body region in the target vehicle image, the method further comprises the steps of: acquiring a lane line image, and judging whether the lane line image is consistent before and after geometric correction processing; and if the lane line images before and after the geometric correction processing are inconsistent, correcting the position of the camera for acquiring the lane line images. 8. The database-based vehicle map searching method according to claim 1, wherein the database is determined according to the image of the rear body, and comprises the steps of acquiring the image of the rear body of each vehicle, extracting the features of the image of the rear body of each vehicle, and storing the features in the database; the features of the vehicle body region in the extracted target vehicle image comprise the overall contour feature of the rear of the vehicle, the feature of a luggage frame, the feature of license plate characters, the feature of vehicle tail chrome plating characters and the feature of vehicle sticker.",CN108595576_A.txt,G06K9/00,{'computing; calculating; counting'},['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers'],"vehicle search-by-image method based on database The invention provides a vehicle search-by-image method based on a database. The method comprises the following steps that: obtaining a vehicle body area in a target vehicle image; extracting the characteristics of the vehicle body area in the target vehicle image, and displaying the characteristics; and according to the characteristic retrieval database of the vehicle body area, obtaining an image similar to the vehicle body area of the target vehicle. The method has the beneficial effects that the vehicle search-by-image method based on the database can effectively solve the problems of a great quantity of complex calculation, cost and the like by deep learning can be effectively solved, the problems of complex calculation and model verification brought by the deep learning do not need to be considered, and in addition, pictures with high retrieval similarity can be searched in a second level on the basis of the search by image of the database.",computing; calculating; counting
KR102122850_B1,2020-03-03,2020-06-15,2020-03-03,SARADA,KIM BYEONG HOONJI SEUNG TAEHAN HAG YONGSEONGWOOK HA,71081429,vehicle classification,solution for analysis road and recognition vehicle license plate employing deep-learning,"An embodiment relates to a deep learning-based traffic analysis and vehicle number recognition solution. More specifically, traffic environment information is collected for each camera device installed in a number of different traffic monitoring areas, the traffic environment information is converted into big data to be learned based on predetermined deep learning, and a deep learning-based traffic environment learning result is applied to perform traffic analysis for each of a plurality of different environmental conditions. Therefore, by applying artificial intelligence traffic analysis technology through the same, traffic analysis is systematic and reliable, and suitable traffic analysis and traffic operation are performed in accordance with changes in the traffic environment.","1. the traffic analysis and vehicle number recognition method of central control center includes a first step of collecting traffic environment information for each camera device installed for each multiple different traffic monitoring areas during traffic monitoring; and a third step of traffic analysis for multiple different ambient environment conditions by applying traffic environment learning results based on the learned dip running. it includes a second step of extracting vehicle feature metadata from the collected traffic environment information after the first step; a third follower step for extracting the vehicle feature value from a preset ai format for extracting the feature value of the vehicle image patch according to the extracted vehicle feature metadata; and a fifth step of inferring the vehicle number and recognizing the vehicle number by a preset lpr (license plate recognition) net in the recognized vehicle. 2. the method according to claim 1, further comprising, after the third step, deriving a traffic policy for multiple different traffic elements by applying the traffic analysis results for each traffic analysis environment specific to the predetermined traffic policy setting format. 3. the method according to claim 1, further comprising a step 4-2 for predicting traffic abnormalities by applying traffic analysis results for specific environmental conditions of the traffic analyzed in a predetermined traffic abnormality pattern format, after the third step. 4. the dip running based traffic analysis and vehicle number recognition method according to claim 3, wherein step 4-2 senses the abnormal pattern by the object specific image distribution of the traffic abnormal pattern format traffic analysis result according to the environment condition of the traffic analyzed. 5. the method of claim 4, wherein the step 4-2 comprises extracting object metadata from traffic analysis results of surrounding environmental conditions; and a step 4-2-3 for detecting an abnormal pattern by applying the object classified results to the object specific image distribution of the traffic abnormality pattern format. 6. amend status: delete 7. the method of claim 1, wherein the second step of generating a background image by accumulating an image continuously inputted from the collected traffic environment information; a second follower-2 step of comparing the input image and the background image to calculate a pixel with a difference as much as a preset value; the title method comprises the steps of: (2) removing the noise of a foreground image through a morphology filter, (2) acquiring the outline of a vehicle feature from the removed foreground image, (2) acquiring the outline of a vehicle feature, (2) combining the outline of a preset minimum size with the adjacent outline, and treating with a single object, a second follower -7 step for tracking the single object as characteristic of the object that is robbed by changing the brightness by using color characteristics from the kcf tracker consisting of a tracking frame work using characteristics of a circulation matrix; step-9 of correcting an object by using object extraction results and kcf tracker prediction results as predetermined n to 1; and step 2 of extracting by generating feature metadata from the classified vehicle. 8. the method of claim 1, wherein the fourth step estimates the size and position of the entire vehicle by combining by estimating and estimating the overall size and position of each feature for a plurality of different vehicle features in the ai format. 9. amend status: delete 10. amend status: delete 11. amend status: delete",KR102122850_B1.txt,"G06T5/00,G06T7/13,G06T7/194,G08G1/017,G08G1/04,H04N7/18","{'electric communication technique', 'signalling', 'computing; calculating; counting'}","['image data processing or generation, in general', 'image data processing or generation, in general', 'image data processing or generation, in general', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'pictorial communication, e.g. television']","solution for analysis road and recognition vehicle license plate employing deep-learning An embodiment relates to a deep learning-based traffic analysis and vehicle number recognition solution. More specifically, traffic environment information is collected for each camera device installed in a number of different traffic monitoring areas, the traffic environment information is converted into big data to be learned based on predetermined deep learning, and a deep learning-based traffic environment learning result is applied to perform traffic analysis for each of a plurality of different environmental conditions. Therefore, by applying artificial intelligence traffic analysis technology through the same, traffic analysis is systematic and reliable, and suitable traffic analysis and traffic operation are performed in accordance with changes in the traffic environment.",electric communication technique signalling computing; calculating; counting
US10510234_B2,2017-12-21,2019-12-17,2016-12-21,AXIS,"DANIELSSON, NICLASMOLIN, SIMON",57755012,anomaly detection,method for generating alerts in a video surveillance system,"A method for generating an alert signal in a surveillance system comprising: detecting a targeted individual in a video stream, selecting the targeted individual, and tracking the targeted individual, as first steps. The method also comprises classifying actions of the detected individual over a plurality of image frames in the video stream in response to the identification of the detected object as being a targeted person, and generating an alert signal if the classified action of the object is classified as a predefined alert-generating action.","1. A method for generating an alert signal in a surveillance system, the method comprising: detecting an object in a video stream; selecting the object as a targeted object; prompting a selection of a predefined alert generation action from among a set of prohibited actions stored in a database upon the selection of the target object, wherein the selection is correlated to the tracked object and/or a particular area; tracking the targeted object; cutting out a part of each image frame of the video stream, wherein the part of each image frame includes the targeted object; recognizing actions conducted by the tracked object by analyzing the cut out parts; classifying the recognized actions; and generating an alert signal if one or more of the actions of the tracked object is classified as the predefined alert-generating action.2. The method of claim 1, wherein the detecting is performed by an object detection algorithm or motion detection algorithm.3. The method of claim 1, wherein the selecting the object as the targeted object is performed by comparing an appearance model for the object with a database of appearance models for multiple objects.4. The method of claim 3, wherein an appearance model of the targeted object is recorded to the database, either as a new post or as an update.5. The method of claim 1, wherein the selecting the object as the targeted object is performed by a recognition algorithm.6. The method of claim 1, wherein the recognizing actions is performed as a two-stage process, in a first stage a first action recognition is performed, at a camera capturing the video stream, for detecting an action candidate, and in a second stage video data comprising the object triggering the action candidate is sent to a remote server for verifying or rejecting the action candidate as a true action.7. The method of claim 1, further comprising performing a handover from a first device to a second device of the surveillance system, wherein the handover includes handing over an appearance model describing the tracked object.8. The method of claim 1, wherein the recognizing actions is based on a spatial/contextual recognition approach, a temporal recognition approach or a combination of the two.9. A system comprising: a digital network camera, wherein the digital network camera includes an image processing circuitry configured to capture a video stream; a processor configured to: detect an object in the video stream; select the object as a targeted object; prompt a selection of a predefined alert generating action from among a set of prohibited actions stored in a database upon selection of the target object, wherein the selection is correlated to the tracked object and/or a particular area; track the targeted object, wherein the targeted object is included in a cut out part of each image frame of the video stream; recognize actions conducted by the tracked object by analyzing the cut out parts and classify the recognized actions; a second processor configured to generate an alert signal if one or more of the actions of the tracked object is classified as the predefined alter-generating action; and a user interface configured to receive commands from a user and to communicate with the digital network camera, the first processor, and the second processor.10. The system of claim 9, wherein the second processor forms part of the digital network camera.11. The system of claim 9, wherein the second processor is found in a server.12. An article of manufacture including a non-transitory computer-readable storage medium having instructions stored thereon for generating an alert signal in a surveillance system, execution of which by a computing device causes the computing device to perform operations comprising: detecting an object in a video stream; selecting the object as a targeted object; prompting a selection of a predefined alert generation action from among a set of prohibited actions stored in a database upon the selection of the target object, wherein the selection is correlated to the tracked object and/or a particular area; tracking the targeted object; cutting out a part of each image frame of the video stream, wherein the part of each image frame includes the targeted object; recognizing actions conducted by the tracked object by analyzing the cut out parts; classifying the recognized actions; and generating an alert signal if one or more of the actions of the tracked object is classified as the predefined alert-generating action.",US10510234_B2.txt,"G06K9/00,G06K9/32,G06K9/62,G08B13/196,H04N7/18","{'electric communication technique', 'signalling', 'computing; calculating; counting'}","['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'signalling or calling systems; order telegraphs; alarm systems', 'pictorial communication, e.g. television']","method for generating alerts in a video surveillance system A method for generating an alert signal in a surveillance system comprising: detecting a targeted individual in a video stream, selecting the targeted individual, and tracking the targeted individual, as first steps. The method also comprises classifying actions of the detected individual over a plurality of image frames in the video stream in response to the identification of the detected object as being a targeted person, and generating an alert signal if the classified action of the object is classified as a predefined alert-generating action.",electric communication technique signalling computing; calculating; counting
US10726281_B2,2015-07-29,2020-07-28,2015-07-29,INVENSENSE,"GEORGY, JACQUESGOODALL, CHRISTOPHERELHOUSHI, MOSTAFAALI, ABDELRAHMANHESHMATI, ARDALANHOUSHOLDER, MICHAEL D.",57882474,gps,method and apparatus for user and moving vehicle detection,"An apparatus and method are disclosed for user and moving vehicle detection in which sensor data for a portable device is processed to determine whether the portable device is in a moving vehicle. Following a determination the portable device is in a moving vehicle, the sensor data is to characterize an association between the user and the portable device to determine whether the portable device is connected to the user. If the user is connected to the portable device, it is then determined if the portable device is being held in hand. If the portable device is held in hand, it is then determined if the user is operating the moving vehicle. Output from an image sensor of the portable device may be used in determining if the user is the operator.","1. A method for user and moving vehicle detection, the method comprising: obtaining sensor data for a portable device from sensors integrated with the portable device, including at least one motion sensor configured to output data representing motion of the portable device; performing an ordered sequence of determinations to improve resource conservation, wherein each determination is performed only if an immediately previous determination is confirmed, by: a) processing motion sensor data obtained from only the portable device to determine whether the portable device is in a moving vehicle; b) processing motion sensor data obtained from only the portable device to determine whether the portable device is connected to a user if the portable device is determined to be in the moving vehicle; and c) processing motion sensor data to determine whether the portable device is hand held if it is determined the portable device is connected to the user, wherein processing the motion sensor data to determine whether the portable device is hand held comprises at least one of: i) determining a device use case for the portable device by processing the motion sensor data; ii) applying a machine learning technique to the motion sensor data; and iii) applying a signal analysis technique to the motion sensor data.2. The method of claim 1, wherein processing motion sensor data to determine whether the portable device is in a moving vehicle comprises applying a machine learning technique.3. The method of claim 2, further comprising inputting features extracted from the processed motion sensor data to at least one stored classification model to determine a motion mode of the portable device.4. The method of claim 3, wherein the at least one stored classification model comprises extracted features developed during a training phase.5. The method of claim 1, wherein processing motion sensor data to determine whether the portable device is in a moving vehicle comprises applying a signal analysis technique.6. The method of claim 5, wherein the signal analysis technique comprises any one or any combination of the following: (i) a statistical analysis; (ii) a frequency-domain analysis; or (iii) a time-domain analysis.7. The method of claim 5, wherein the signal analysis technique comprises an analysis of at least one signal selected from the group consisting of an angular rotation signal, a signal derived from the angular rotation signal, an acceleration signal, and a signal derived from the acceleration signal.8. The method of claim 1, wherein the processing motion sensor data to determine whether the portable device is in a moving vehicle is based at least in part on motion sensor data that comprises inertial sensor data.9. The method of claim 8, further comprising obtaining supplemental sensor data for the portable device and processing the supplemental sensor data with the inertial sensor data.10. The method of claim 1, further comprising obtaining absolute navigational information for the portable device, wherein determining whether the portable device is in a moving vehicle is based at least in part on the absolute navigational information.11. The method of claim 1, wherein processing motion sensor data to determine whether the portable device is connected to the user comprises applying a machine learning technique.12. The method of claim 11, further comprising inputting features extracted from processed motion sensor data to at least one stored classification model to characterize if the portable device is connected to the user.13. The method of claim 12, wherein the at least one stored classification model comprises extracted features developed during a training phase.14. The method of claim 1, wherein processing motion sensor data to determine whether the portable device is connected to the user comprises applying a signal analysis technique.15. The method of claim 14, wherein the signal analysis technique comprises any one or any combination of the following: (i) a statistical analysis; (ii) a frequency-domain analysis; or (iii) a time-domain analysis.16. The method of claim 14, wherein the signal analysis technique comprises an analysis of at least one signal selected from the group consisting of an angular rotation signal, a signal derived from the angular rotation signal, an acceleration signal, and a signal derived from the acceleration signal.17. The method of claim 1, wherein determining whether the portable device is hand held comprises determining a device use case for the portable device.18. The method of claim 1, wherein processing the sensor data to determine whether the portable device is hand held comprises applying a machine learning technique.19. The method of claim 18, further comprising inputting features extracted from the processed sensor data to at least one stored classification model to determine whether the portable device is hand held.20. The method of claim 19, wherein the at least one stored classification model comprises extracted features developed during a training phase.21. The method of claim 1, wherein processing the sensor data to determine whether the portable device is hand held comprises applying a signal analysis technique.22. The method of claim 21, wherein the signal analysis technique comprises any one or any combination of the following: (i) a statistical analysis; (ii) a frequency-domain analysis; or (iii) a time-domain analysis.23. The method of claim 21, wherein the signal analysis technique comprises an analysis of at least one signal selected from the group consisting of an angular rotation signal, a signal derived from the angular rotation signal, an acceleration signal, and a signal derived from the acceleration signal.24. The method of claim 1, wherein the processing motion sensor data to determine whether the portable device is hand held is based at least in part on motion sensor data that comprises inertial sensor data.25. The method of claim 24, further comprising obtaining supplemental sensor data, wherein the determination of whether the portable device is hand held is further based at least in part on the supplemental sensor data.26. The method of claim 25, wherein the supplemental sensor data is obtained from at least one of an ambient light sensor and a proximity sensor.27. The method of claim 25, further comprising activating a source of the supplemental sensor data from a power save mode if it is determined the portable device is connected to the user.28. The method of claim 1, further comprising obtaining information regarding the portable device, wherein the determination of whether the portable device is hand held is further based at least in part on the information regarding the portable device.29. The method of claim 28, wherein the information regarding the portable device comprises a status of at least one application running on the portable device.30. The method of claim 1, further comprising obtaining image sensor data for the portable device if it is determined the portable device is hand held, processing the image sensor data, and determining whether the user of the portable device is operating the moving vehicle based at least in part on the processed image sensor data.31. The method of claim 30, wherein processing the image sensor data comprises extracting at least one of a feature and an object from the image sensor data.32. The method of claim 31, further comprising preprocessing the image sensor data before extracting the at least one of a feature and an object.33. The method of claim 32, wherein preprocessing the image comprises performing an operation selected from the group consisting of generating a full color image, generating a reduced dimension image, generating a grey scale image, generating a resized image, generating a normalized image, and generating a reduced noise image.34. The method of claim 31, wherein extracting the at least one of a feature and an object comprises a line detection operation.35. The method of claim 31, wherein extracting the at least one of a feature and an object comprises an edge detection operation.36. The method of claim 31, wherein extracting the at least one of a feature and an object comprises a circular shape detection operation.37. The method of claim 31, wherein extracting the at least one of a feature and an object comprises performing a Hough transform.38. The method of claim 31, wherein processing the image sensor data further comprises recognizing the at least one of a feature and an object.39. The method of claim 38, wherein the at least one of a feature and an object comprises an object associated with the user.40. The method of claim 39, wherein the object associated with the user is at least one of a head, a face and shoulders.41. The method of claim 38, wherein the at least one of a feature and an object comprises an object associated with the vehicle.42. The method of claim 41, wherein the object associated with the vehicle is at least one of a steering wheel, a mirror, a seat belt, a windshield and a window.43. The method of claim 38, further comprising recognizing a combination of objects associated with the user and the vehicle.44. The method of claim 30, wherein obtaining image sensor data further comprises activating an image sensor of the portable device from a power save mode.45. The method of claim 30, wherein image sensor data is obtained from multiple image sensors of the portable device.46. The method of claim 30, further comprising obtaining a coarse location for the portable device and determining an expected orientation of a vehicle operator with respect to the vehicle based on the coarse location, wherein determining whether the user of the portable device is operating the moving vehicle is based at least in part on the expected vehicle operator orientation.47. The method of claim 46, wherein the coarse location is obtained from any one or any combination of the following: (i) a global navigation satellite system (GNSS); (ii) cell-based positioning; (iii) WiFi-based positioning; or (iv) other wireless -based positioning.48. The method of claim 1, wherein the processing motion sensor data to determine whether the portable device is connected is based at least in part on motion sensor data that comprises inertial sensor data.49. A portable device for providing user and moving vehicle detection, the portable device comprising: a sensor assembly integrated with the portable device, including at least one motion sensor configured to output data representing motion of the portable device; a processor configured to receive and process sensor data for the portable device from sensors integrated with the portable device; and a detector configured to perform an ordered sequence of determinations to improve resource conservation, wherein each determination is performed only when an immediately previous determination is confirmed, by: a) processing motion sensor data obtained from only the portable device to determine whether the portable device is in a moving vehicle; b) processing motion sensor data obtained from only the portable device to determine whether the portable device is connected to a user if it is determined that the portable device is in the moving vehicle; and c) processing motion sensor data to determine whether the portable device is hand held if it is determined the portable device is connected to the user, wherein the detector is configured to process the motion sensor data to determine whether the portable device is hand held by at least one of: i) determining a device use case for the portable device by processing the motion sensor data; ii) applying a machine learning technique to the motion sensor data; and iii) applying a signal analysis technique to the motion sensor data.50. The portable device of claim 49, wherein the sensor assembly comprises an inertial sensor.51. The portable device of claim 50, further comprising a supplemental sensor selected from the group consisting of an ambient light sensor and a proximity sensor, wherein the detector is further configured to determine whether the portable device is hand held based at least in part on output from the supplemental sensor.52. The portable device of claim 49, further comprising an image sensor, wherein the detector is further configured to process image sensor data to determine whether the user of the portable device is operating the moving vehicle.53. The portable device of claim 52, further comprising a location module configured to obtain a coarse location for the portable device, wherein the detector is further configured to determine whether the user of the portable device is operating the moving vehicle based at least in part an expected orientation of a vehicle operator with respect to the vehicle based on the coarse location.54. The portable device of claim 53, wherein the location module comprises a source of absolute navigational information from any one or any combination of the following: (i) a global navigation satellite system (GNSS) receiver; (ii) cell-based positioning receiver; (iii) WiFi-based positioning receiver; or (iv) other wireless-based positioning receiver.55. The portable device of claim 49, wherein the sensor assembly includes an accelerometer and a gyroscope.56. The device of claim 55, wherein the sensor assembly comprises an inertial sensor implemented as a Micro Electro Mechanical System (MEMS).57. The method of claim 48, further comprising obtaining supplemental sensor data, wherein the determination of whether the portable device is connected is further based at least in part on the supplemental sensor data.",US10726281_B2.txt,"B60W30/08,B60W50/14,G01C21/12,G01S19/42,G06K9/00,G06K9/46,G06K9/62,G06T7/00,G06T7/13,G06T7/20,G06T7/246","{'measuring; testing', 'computing; calculating; counting', 'vehicles in general'}","['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general', 'image data processing or generation, in general', 'image data processing or generation, in general', 'image data processing or generation, in general']","method and apparatus for user and moving vehicle detection An apparatus and method are disclosed for user and moving vehicle detection in which sensor data for a portable device is processed to determine whether the portable device is in a moving vehicle. Following a determination the portable device is in a moving vehicle, the sensor data is to characterize an association between the user and the portable device to determine whether the portable device is connected to the user. If the user is connected to the portable device, it is then determined if the portable device is being held in hand. If the portable device is held in hand, it is then determined if the user is operating the moving vehicle. Output from an image sensor of the portable device may be used in determining if the user is the operator.",measuring; testing computing; calculating; counting vehicles in general
CN109493449_A,2018-11-23,2019-03-19,2018-11-23,BEIHANG UNIVERSITY,CHEN LIZHU TONGYU,65697642,gps,method for evaluating truck loading state based on truck gps track data and high-speed trading data,"The invention relates to a method for evaluating the truck loading state based on truck GPS track data and high-speed trading data. The method comprises the following steps: (1) matching truck GPS track data and high-speed trading data, namely performing truck trip information matching based on longitude and latitude timestamp information recorded in the truck GPS track and toll station accessingtime, toll station information and loading information recorded in the high-speed trading data; (2) extracting the trip characteristics corresponding to each trip of a truck, namely extracting the trip characteristics of current trip, adjacent previous trip and adjacent next trip of the truck based on the matching result data, and pre-processing the trip characteristics; (3) taking the trip characteristic capable of determining whether a truck is loaded or not as a training sample, and training a prediction model based on an xgboost method; and (4) taking the characteristic not capable of determining the truck loading state as a test set, inputting into the step (3) to obtain the prediction model, and predicting the loading state of the trip of the truck.","1. A freight train cargo state estimation method based on freight train GPS track data and high-speed transaction data is characterized by comprising the following steps: the method is realized by the following steps: (1) matching GPS track data with high-speed transaction data based on longitude and latitude, time stamp and speed information recorded in GPS data and toll station entering and exiting time, toll station time and load information recorded in the high-speed transaction data, and marking whether a primary journey of the truck passes through high speed or not and whether the current journey passes through the high speed or not carries cargo or not; (2) removing the initial travel and the final travel of the truck, and extracting travel characteristics for each travel, wherein the travel characteristics of the travel comprise the current travel, the adjacent previous travel, the start time and the end time of the adjacent next travel, the travel start place, the travel end place, the OD distance of the travel, the travel accumulated travel mileage, the travel speed of the travel at a non-high speed and the travel duration; (3) for the travel start time and the travel end time extracted in (2): because the starting time range and the ending time range of the truck are large, one day is divided into 4 time periods according to the historical travel of the truck and the distribution of the starting time and the ending time, the time periods are marked by 1, 2, 3 and 4, and then the time characteristics are marked according to the time periods to which the starting time and the ending time of the travel belong; (4) preprocessing the travel starting point and the travel ending point extracted in the step (2); clustering longitude and latitude information of a starting place and an ending place of a historical truck journey based on DBSCAN, selecting the optimal clustering number based on a contour coefficient method, and if the clustering number is K categories, considering that abnormal points exist after clustering, the K +1 categories are shared; then marking the types of the starting place and the ending place of the journey, wherein the starting place and the ending place of the journey of the truck are of a nominal type and have no sequence, and the starting place and the ending place of the journey of the truck need to be processed based on one-hot coding, and the processed starting place and the processed ending place of the journey are respectively used as K +1 characteristics; (5) based on the travel starting time and the travel ending time in the step (3), the travel starting place and the travel ending place in the step (4), the travel OD distance extracted in the step (2), travel accumulated travel mileage, travel speed at a non-high speed of the travel and travel duration, and the characteristics of determining whether goods are loaded or not of the truck are used as a training set, and a prediction model is trained based on an xgboost method; (6) and (5) inputting the travel characteristics corresponding to the travel where the cargo carrying state of the truck cannot be determined as a test set into the prediction model trained in the step (5), and predicting whether the current travel carries cargo to obtain the cargo carrying state of the travel. 2. The truck cargo state estimation method based on truck GPS trajectory data and high speed transaction data according to claim 1, characterized in that: in the step (1), the GPS track data is: the time stamp, longitude and latitude of the current position of the truck, the current running speed and running direction of the truck are acquired by a positioning device arranged on the truck. 3. The truck cargo state estimation method based on truck GPS trajectory data and high speed transaction data according to claim 1, characterized in that: in the step (1), the high-speed transaction number is the information, the timestamp, the model number and the load of the truck of the toll station, which are collected by equipment or staff of the toll station when the truck enters and exits the high-speed toll station. 4. The truck cargo state estimation method based on truck GPS trajectory data and high speed transaction data according to claim 1, characterized in that: in the step (1), the GPS track data is matched with the high-speed transaction data: when the longitude and latitude information recorded by the GPS is within 500 meters from the longitude and latitude of the toll station, the time difference between the time and the time recorded in the high-speed transaction data is within 3 minutes, and the truck is in a low-speed driving state at present, namely less than 5m/s, the matching is considered to be performed once. And if all track points of the current journey are traversed, marking whether the current journey of the truck passes through high speed or not and whether the truck carries cargo or not according to whether the track points are matched with the high-speed transaction data or not and the truck cargo information recorded in the high-speed transaction data. 5. The truck cargo state estimation method based on truck GPS trajectory data and high speed transaction data according to claim 1, characterized in that: in the step (3), one day is divided into 4 time periods, 24 hours of one day are divided into 4 time periods of 01: 00-03: 59,04: 00-12: 59,13: 00-19: 59 and 20: 00-00: 59 according to the travel starting time and the travel ending time of all trucks, and the time periods are respectively marked as 1, 2, 3 and 4 according to the travel starting time and the travel ending time. 6. The truck cargo state estimation method of truck GPS track data and high speed transaction data according to claim 1, characterized in that: the OD distance of the trip in the step (2) is a straight-line distance between a truck trip start point (origin) and a trip end point (destination). 7. The truck cargo state estimation method of truck GPS track data and high speed transaction data according to claim 1, characterized in that: the travel speed of the truck in the step (2) under the non-high speed travel refers to that the travel environments of the truck under the high speed travel and the non-high speed travel environments are different, the travel of the high speed travel part is not considered in the calculation range of the speed, and only the speed of the truck under the non-high speed travel is calculated. 8. The truck cargo state estimation method of truck GPS track data and high speed transaction data according to claim 1, characterized in that: the travel accumulated travel mileage in the step (2) is to sequentially accumulate the distances between two adjacent GPS track points in the travel to obtain the final travel accumulated travel mileage.",CN109493449_A.txt,"G06F16/2458,G07C5/08","{'computing; calculating; counting', 'checking-devices'}","['electric digital data processing (computer systems based on specific computational models g06n)', 'time or attendance registers; registering or indicating the working of machines; generating random numbers; voting or lottery apparatus; arrangements, systems or apparatus for checking not provided for elsewhere']","method for evaluating truck loading state based on truck gps track data and high-speed trading data The invention relates to a method for evaluating the truck loading state based on truck GPS track data and high-speed trading data. The method comprises the following steps: (1) matching truck GPS track data and high-speed trading data, namely performing truck trip information matching based on longitude and latitude timestamp information recorded in the truck GPS track and toll station accessingtime, toll station information and loading information recorded in the high-speed trading data; (2) extracting the trip characteristics corresponding to each trip of a truck, namely extracting the trip characteristics of current trip, adjacent previous trip and adjacent next trip of the truck based on the matching result data, and pre-processing the trip characteristics; (3) taking the trip characteristic capable of determining whether a truck is loaded or not as a training sample, and training a prediction model based on an xgboost method; and (4) taking the characteristic not capable of determining the truck loading state as a test set, inputting into the step (3) to obtain the prediction model, and predicting the loading state of the trip of the truck.",computing; calculating; counting checking-devices
WO2020069517_A2,2019-09-30,2020-04-02,2018-09-30,STRONG FORCE INTELLECTUAL CAPITAL,"CELLA, CHARLES HOWARD",69953381,road monitoring,intelligent transportation systems,"Transportation systems have artificial intelligence including neural networks for recognition and classification of objects and behavior including natural language processing and computer vision systems. The transportation systems involve sets of complex chemical processes, mechanical systems, and interactions with behaviors of operators. System-level interactions and behaviors are classified, predicted and optimized using neural networks and other artificial intelligence systems through selective deployment, as well as hybrids and combinations of the artificial intelligence systems, neural networks, expert systems, cognitive systems, genetic algorithms and deep learning.","1. A system for transportation, comprising:a vehicle having a vehicle operating state;an artificial intelligence system to execute a genetic algorithm to generate mutations from an initial vehicle operating state to determine at least one optimized vehicle operating state.2. The system for transportation of claim 1 wherein the vehicle operating state includes a set of vehicle parameter values and wherein the genetic algorithm is to:vary the set of vehicle parameter values for a set of corresponding time periods such that the vehicle operates according to the set of vehicle parameter values during the corresponding time periods;evaluate the vehicle operating state for each of the corresponding time periods according to a set of measures to generate evaluations; andselect, for future operation of the vehicle, an optimized set of vehicle parameter values based on the evaluations.3. The system for transportation of claim 2 wherein the vehicle operating state includes a state of a rider of the vehicle, wherein the at least one optimized vehicle operating state includes an optimized state of the rider wherein the genetic algorithm is to optimize the state of the rider, wherein the evaluating according to the set of measures is to determine the state of the rider corresponding to the vehicle parameter values.4. The system for transportation of claim 3 wherein the vehicle operating state includes a state of the rider of the vehicle, wherein the set of vehicle parameter values includes a set of vehicle performance control values, wherein the at least one optimized vehicle operating state includes an optimized state of performance of the vehicle wherein the genetic algorithm is to optimize the state of the rider and the state of performance of the vehicle, wherein the evaluating according to the set of measures is to determine the state of the rider and the state of performance of the vehicle corresponding to the vehicle performance control values.5. The system for transportation of claim 2 wherein the set of vehicle parameter values includes a set of vehicle performance control values, wherein the at least one optimized vehicle operating state includes an optimized state of performance of the vehicle, wherein the genetic algorithm is to optimize the state of performance of the vehicle, wherein the evaluating according to the set of measures is to determine the state of performance of the vehicle corresponding to the vehicle performance control values.6. The system for transportation of claim 2 wherein the set of vehicle parameter values includes a rider-occupied parameter value, and wherein the rider-occupied parameter value affirms a presence of a rider in the vehicle.7. The system for transportation of claim 6 wherein the vehicle operating state includes a state of a rider of the vehicle, wherein the at least one optimized vehicle operating state includes an optimized state of the rider wherein the genetic algorithm is to optimize the state of the rider, wherein the evaluating according to the set of measures is to determine the state of the rider corresponding to the vehicle parameter values.8. The system for transportation of claim 7 wherein the state of the rider includes a rider satisfaction parameter.9. The system for transportation of claim 7 wherein the state of the rider includes an input representative of the rider, wherein the input representative of the rider is selected from the group consisting of: a rider state parameter, a rider comfort parameter, a rider emotional state parameter, a rider satisfaction parameter, a rider goals parameter, a classification of trip, and combinations thereof.10. The system for transportation of claim 7 wherein the set of vehicle parameter values includes a set of vehicle performance control values, wherein the at least one optimized vehicle operating state includes an optimized state of performance of the vehicle wherein the genetic algorithm is to optimize the state of the rider and the state of performance of the vehicle, wherein the evaluating according to the set of measures is to determine the state of the rider and the state of performance of the vehicle corresponding to the vehicle performance control values.11. The system for transportation of claim 6 wherein the set of vehicle parameter values includes a set of vehicle performance control values, wherein the at least one optimized vehicle operating state includes an optimized state of performance of the vehicle, wherein the genetic algorithm is to optimize the state of performance of the vehicle, wherein the evaluating according to the set of measures is to determine the state of performance of the vehicle corresponding to the vehicle performance control values.12. The system for transportation of claim 11 wherein the set of vehicle performance control values are selected from the group consisting of:a fuel efficiency; a trip duration; a vehicle wear; a vehicle make; a vehicle model; a vehicle energy consumption profiles; a fuel capacity; a real-time fuel levels; a charge capacity; a recharging capability; a regenerative braking state; and combinations thereof.13. The system for transportation of claim 11 wherein at least a portion of the set of vehicle performance control values is sourced from at least one of an on-board diagnostic system, a telemetry system, a software system, a vehicle-located sensor, and a system external to the vehicle.14. The system for transportation of claim 2 wherein the set of measures relates to a set of vehicle operating criteria.15. The system for transportation of claim 2 wherein the set of measures relates to a set of rider satisfaction criteria.16. The system for transportation of claim 2 wherein the set of measures relates to a combination of vehicle operating criteria and rider satisfaction criteria.17. The system for transportation of claim 2 wherein each evaluation uses feedback indicative of an effect on at least one of a state of performance of the vehicle and a state of the rider.18. A system for transportation, comprising:an artificial intelligence system to process inputs representative of a state of a vehicle and inputs representative of a rider state of a rider occupying the vehicle during the state of the vehicle with a genetic algorithm to optimize a set of vehicle parameters that affects the state of the vehicle or the rider state, wherein the genetic algorithm is to perform a series of evaluations using variations of the inputs, wherein each evaluation in the series of evaluations uses feedback indicative of an effect on at least one of a vehicle operating state and the rider state.19. The system for transportation of claim 18 wherein the inputs representative of the rider state indicate that the rider is absent from the vehicle.20. The system for transportation of claim 18 wherein the state of the vehicle includes the vehicle operating state.21. The system for transportation of claim 18 wherein a vehicle parameter in the set of vehicle parameters includes a vehicle performance parameter.22. The system for transportation of claim 18 wherein the genetic algorithm is to optimize the set of vehicle parameters for the state of the rider.23. The system for transportation of claim 22 wherein optimizing the set of vehicle parameters is responsive to an identifying, by the genetic algorithm, of at least one vehicle parameter that produces a favorable rider state.24. The system for transportation of claim 18 wherein the genetic algorithm is to optimize the set of vehicle parameters for vehicle performance.25. The system for transportation of claim 18 wherein the genetic algorithm is to optimize the set of vehicle parameters for the state of the rider and is to optimize the set of vehicle parameters for vehicle performance.26. The system for transportation of claim 25 wherein optimizing the set of vehicle parameters is responsive to the genetic algorithm identifying at least one of a favorable vehicle operating state, and favorable vehicle performance that maintains the rider state.27. The system for transportation of claim 18 wherein the artificial intelligence system further includes a neural network selected from a plurality of different neural networks, wherein the selection of the neural network involves the genetic algorithm and wherein the selection of the neural network is based on a structured competition among the plurality of different neural networks.28. The system for transportation of claim 18 wherein the genetic algorithm facilitates training a neural network to process interactions among a plurality of vehicle operating systems and riders to produce the optimized set of vehicle parameters.29. The system for transportation of claim 18 wherein a set of inputs relating to at least one vehicle parameter are provided by at least one of an on-board diagnostic system, a telemetry system, a vehicle-located sensor, and a system external to the vehicle.30. The system for transportation of claim 18 wherein the inputs representative of the rider state comprise at least one of comfort, emotional state, satisfaction, goals, classification of trip, or fatigue.31. The system for transportation of claim 18 wherein the inputs representative of the rider state reflect a satisfaction parameter of at least one of a driver, a fleet manager, an advertiser, a merchant, an owner, an operator, an insurer, and a regulators.32. The system for transportation of claim 18 wherein the inputs representative of the rider state comprise inputs relating to a user that, when processed with a cognitive system yield the rider state.33. A system for transportation, comprising: a hybrid neural network for optimizing an operating state of a continuously variable powertrain of a vehicle wherein a portion of the hybrid neural network is to operate to classify a state of the vehicle thereby generating a classified state of the vehicle, and an other portion of the hybrid neural network is to operate to optimize at least one operating parameter of a transmission portion of the continuously variable powertrain.34. The system for transportation of claim 33 further comprising: an artificial intelligence system operative on at least one processor, the artificial intelligence system to operate the portion of the hybrid neural network to operate to classify the state of the vehicle and the artificial intelligence system to operate the other portion of the hybrid neural network to optimize the at least one operating parameter of the transmission portion of the continuously variable powertrain based on the classified state of the vehicle.35. The system for transportation of claim 34 wherein the vehicle comprises a system for automating at least one control parameter of the vehicle.36. The system for transportation of claim 35 wherein the vehicle is at least a semiautonomous vehicle.37. The system for transportation of claim 36 wherein the vehicle is to be automatically routed.38. The system for transportation of claim 37 wherein the vehicle is a self-driving vehicle.39. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle maintenance state.40. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle health state.41. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle operating state.42. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle energy utilization state.43. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle charging state.44. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle satisfaction state.45. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle component state.46. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle sub-system state.47. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle powertrain system state.48. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle braking system state.49. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle clutch system state.50. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle lubrication system state.51. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle transportation infrastructure system state.52. The system for transportation of claim 34 wherein the classified state of the vehicle is a vehicle rider state.53. The system for transportation of claim 34 wherein at least a portion of the hybrid neural network is a convolutional neural network.54. A method for optimizing operation of a continuously variable vehicle powertrain of a vehicle, the method comprising:executing a first network of a hybrid neural network on at least one processor, the first network classifying a plurality of operational states of the vehicle, wherein at least a portion of the operational states is based on a state of the continuously variable powertrain of the vehicle; andexecuting a second network of the hybrid neural network on the at least one processor, the second network processing inputs that are descriptive of the vehicle and of at least one detected condition associated with an occupant of the vehicle for at least one of the plurality of classified operational states of the vehicle, wherein the processing the inputs by the second network causes optimization of at least one operating parameter of the continuously variable powertrain of the vehicle for a plurality of the operational states of the vehicle.55. The method of claim 54 wherein the vehicle comprises an artificial intelligence system, the method further comprising automating at least one control parameter of the vehicle by the artificial intelligence system.56. The method of claim 55 wherein the vehicle is at least a semi-autonomous vehicle.57. The method of claim 56 wherein the vehicle is to be automatically routed.58. The method of claim 56 wherein the vehicle is a self-driving vehicle.59. The method of claim 55 further comprising optimizing, by the artificial intelligence system, an operating state of the continuously variable powertrain of the vehicle based on the optimized at least one operating parameter of the continuously variable powertrain by adjusting at least one other operating parameter of a transmission portion of the continuously variable powertrain.60. The method of claim 59 further comprising optimizing, by the artificial intelligence system, the operating state of the continuously variable powertrain by processing social data from a plurality of social data sources.61. The method of claim 59 further comprising optimizing, by the artificial intelligence system, the operating state of the continuously variable powertrain by processing data sourced from a stream of data from unstructured data sources.62. The method of claim 59 further comprising optimizing, by the artificial intelligence system, the operating state of the continuously variable powertrain by processing data sourced from wearable devices.63. The method of claim 59 further comprising optimizing, by the artificial intelligence system, the operating state of the continuously variable powertrain by processing data sourced from in-vehicle sensors.64. The method of claim 59 further comprising optimizing, by the artificial intelligence system, the operating state of the continuously variable powertrain by processing data sourced from a rider helmet.65. The method of claim 59 further comprising optimizing, by the artificial intelligence system, the operating state of the continuously variable powertrain by processing data sourced from rider headgear.66. The method of claim 59 further comprising optimizing, by the artificial intelligence system, the operating state of the continuously variable powertrain by processing data sourced from a rider voice system.67. The method of claim 55 further comprising operating, by the artificial intelligence system, a third network of the hybrid neural network to predict a state of the vehicle based at least in part on at least one of the classified plurality of operational states of the vehicle and at least one operating parameter of the transmission.68. The method of claim 55 wherein the first network of the hybrid neural network comprises a structure-adaptive network to adapt a structure of the first network responsive to a result of operating the first network of the hybrid neural network.69. The method of claim 55 wherein the first network of the hybrid neural network is to process a plurality of social data from social data sources to classify the plurality of operational states of the vehicle.70. The method of claim 55 wherein at least a portion of the hybrid neural network is a convolutional neural network.71. The method of claim 54 wherein at least one of the classified plurality of operational states of the vehicle is a vehicle maintenance state.72. The method of claim 54 wherein at least one of the classified plurality of operational states of the vehicle is a vehicle health state.73. The method of claim 54 wherein at least one of the classified states of the vehicle is a vehicle operating state.74. The method of claim 54 wherein at least one of the classified states of the vehicle is a vehicle energy utilization state.75. The method of claim 54 wherein at least one of the classified states of the vehicle is a vehicle charging state.76. The method of claim 54 wherein at least one of the classified states of the vehicle is a vehicle satisfaction state.77. The method of claim 54 wherein at least one of the classified states of the vehicle is a vehicle component state.78. The method of claim 54 wherein at least one of the classified states of the vehicle is a vehicle sub-system state.79. The method of claim 54 wherein at least one of the classified states of the vehicle is a vehicle powertrain system state.80. The method of claim 54 wherein at least one of the classified states of the vehicle is a vehicle braking system state.81. The method of claim 54 wherein at least one of the classified states of the vehicle is a vehicle clutch system state.82. The method of claim 54 wherein at least one of the classified states of the vehicle is a vehicle lubrication system state.83. The method of claim 54 wherein at least one of the classified states of the vehicle is a vehicle transportation infrastructure system state.84. The method of claim 54 wherein the at least one of classified states of the vehicle is a vehicle driver state.85. The method of claim 54 wherein the at least one of classified states of the vehicle is a vehicle rider state.86. A system for transportation, comprising:a cognitive system for routing at least one vehicle within a set of vehicles based on a routing parameter determined by facilitating a negotiation among a designated set of vehicles, wherein the negotiation accepts inputs relating to a value attributed by at least one user to at least one parameter of a route.87. A method of negotiation-based vehicle routing comprising:facilitating a negotiation of a route-adjustment value for a plurality of parameters used by a vehicle routing system to route at least one vehicle in a set of vehicles; anddetermining a parameter in the plurality of parameters for optimizing at least one outcome based on the negotiation.88. The method of claim 87 further comprising capturing, through a vehicle-based route negotiation interface, a set of user-indicated values for the plurality of parameters used by the vehicle routing system to route the at least one vehicle in the set of vehicles.872. The method of claim 866 wherein an advertisement is delivered from an advertiser who places a winning bid.873. The method of claim 866 wherein delivering an advertisement is based on a winning bid.874. The method of claim 866 further including resolution of payments as settlement for at least one of the delivering an advertisement and placing a winning bid for a placement opportunity.875. The method of claim 866 wherein the inputs relating to the at least one parameter of a vehicle include vehicle classification.876. The method of claim 866 wherein the inputs relating to the at least one parameter of a vehicle include display classification.877. The method of claim 866 wherein the inputs relating to the at least one parameter of a vehicle include audio system capability.878. The method of claim 866 wherein the inputs relating to the at least one parameter of a vehicle include screen size.879. The method of claim 866 wherein the inputs relating to the at least one parameter of a vehicle include route information.880. The method of claim 866 wherein the inputs relating to the at least one parameter of a vehicle include location information.881. The method of claim 866 wherein the inputs relating to the at least one parameter of a rider include rider demographic information.89. The method of claim 88 wherein a user is a rider of the at least one vehicle.882. The method of claim 866 wherein the inputs relating to the at least one parameter of a rider include rider emotional state.883. The method of claim 866 wherein the inputs relating to the at least one parameter of a rider include rider response to prior in-seat advertising.884. The method of claim 866 wherein the inputs relating to the at least one parameter of a rider include rider social media activity.885. An advertising system of vehicle in-seat advertising, the advertising system comprising: a cognitive system that takes inputs relating to at least one parameter of a vehicle and takes inputs relating to at least one parameter of a rider occupying the vehicle, and determines at least one of a price, classification, content and location of an advertisement to be delivered within an interface of the vehicle to a rider in a seat in the vehicle based on the vehiclerelated inputs and the rider-related inputs.886. The advertising system of claim 885 wherein the vehicle comprises a system for automating at least one control parameter of the vehicle.887. The advertising system of claim 886 wherein the vehicle is at least a semiautonomous vehicle.888. The advertising system of claim 887 wherein the vehicle is automatically routed. 889. The advertising system of claim 888 wherein the vehicle is a self-driving vehicle.890. The advertising system of claim 885 wherein the inputs relating to the at least one parameter of a vehicle include vehicle classification.891. The advertising system of claim 885 wherein the inputs relating to the at least one parameter of a vehicle include display classification.892. The advertising system of claim 885 wherein the inputs relating to the at least one parameter of a vehicle include audio system capability.90. The method of claim 88 wherein a user is an administrator for a set of roadways to be used by the at least one vehicle in the set of vehicles.893. The advertising system of claim 885 wherein the inputs relating to the at least one parameter of a vehicle include screen size.894. The advertising system of claim 885 wherein the inputs relating to the at least one parameter of a vehicle include route information.895. The advertising system of claim 885 wherein the inputs relating to the at least one parameter of a vehicle include location information.896. The advertising system of claim 885 wherein the inputs relating to the at least one parameter of a rider include rider demographic information.897. The advertising system of claim 885 wherein the inputs relating to the at least one parameter of a rider include rider emotional state.898. The advertising system of claim 885 wherein the inputs relating to the at least one parameter of a rider include rider response to prior in-seat advertising.899. The advertising system of claim 885 wherein the inputs relating to the at least one parameter of a rider include rider social media activity.900. The advertising system of claim 885, further to determine a vehicle operating state from the inputs related to at least one parameter of the vehicle, wherein the advertisement to be delivered is determined based at least in part on the determined vehicle operating state.901. The advertising system of claim 885, further to determine a rider state from the inputs related to at least one parameter of the rider, wherein the advertisement to be delivered is determined based at least in part on the determined rider state.902. A system for transportation, comprising: a hybrid cognitive system for managing an advertising market for in-seat advertising to riders of vehicles, wherein at least one part of the hybrid cognitive system processes inputs corresponding to at least one parameter of the vehicle to determine a vehicle operating state and at least one other part of the cognitive system processes inputs relating to a rider to determine a rider state, wherein the cognitive system determines a characteristic of an advertisement to be delivered within an interface to the rider in a seat of the vehicle, wherein the characteristic of the advertisement is selected from the group consisting of a price, a category, a location and combinations thereof.91. The method of claim 88 wherein a user is an administrator for a fleet of vehicles including the set of vehicles.903. An artificial intelligence system for vehicle in-seat advertising, comprising:a first portion of the artificial intelligence system that determines an operating state of the vehicle by processing inputs relating to at least one parameter of the vehicle;a second portion of the artificial intelligence system that determines a state of the rider of the vehicle by processing inputs relating to at least one parameter of the rider; and a third portion of the artificial intelligence system that determines at least one of a price, classification, content and location of an advertisement to be delivered within an interface of the vehicle to a rider in a seat in the vehicle based on the vehicle state and the rider state.904. The artificial intelligence system of claim 903 wherein the vehicle comprises a system for automating at least one control parameter of the vehicle.905. The artificial intelligence system of claim 904 wherein the vehicle is at least a semi-autonomous vehicle.906. The artificial intelligence system of claim 905 wherein the vehicle isautomatically routed.907. The artificial intelligence system of claim 906 wherein the vehicle is a self-driving vehicle.908. The artificial intelligence system of claim 903 wherein the cognitive system further determines at least one of a price, classification, content and location of an advertisement placement.909. The artificial intelligence system of claim 903 wherein an advertisement is delivered from an advertiser who places a winning bid.92. The method of claim 87 further comprising offering a set of offered user-indicated values for the plurality of parameters to users with respect to the set of vehicles.910. The artificial intelligence system of claim 903 wherein delivering anadvertisement is based on a winning bid.911. The artificial intelligence system of claim 903, further including resolution of payments as settlement for at least one of the delivering an advertisement and placing a winning bid for a placement opportunity.912. The artificial intelligence system of claim 903 wherein the inputs relating to the at least one parameter of a vehicle include vehicle classification.913. The artificial intelligence system of claim 903 wherein the inputs relating to the at least one parameter of a vehicle include display classification.914. The artificial intelligence system of claim 903 wherein the inputs relating to the at least one parameter of a vehicle include audio system capability.915. The artificial intelligence system of claim 903 wherein the inputs relating to the at least one parameter of a vehicle include screen size.916. The artificial intelligence system of claim 903 wherein the inputs relating to the at least one parameter of a vehicle include route information.917. The artificial intelligence system of claim 903 wherein the inputs relating to the at least one parameter of a vehicle include location information.918. The artificial intelligence system of claim 903 wherein the inputs relating to the at least one parameter of a rider include rider demographic information.93. The method of claim 92 wherein the route-adjustment value is based at least in part on the set of offered user-indicated values.919. The artificial intelligence system of claim 903 wherein the inputs relating to the at least one parameter of a rider include rider emotional state.920. The artificial intelligence system of claim 903 wherein the inputs relating to the at least one parameter of a rider include rider response to prior in-seat advertising.921. The artificial intelligence system of claim 903 wherein the inputs relating to the at least one parameter of a rider include rider social media activity.922. A method of in-vehicle advertising interaction tracking comprising:taking inputs relating to at least one parameter of a vehicle and inputs relating to at least one parameter of a rider occupying the vehicle;aggregating the inputs across a plurality of vehicles;using a hybrid cognitive system to determine opportunities for in-vehicle advertisement placement based on the aggregated inputs;offering the placement opportunities in an advertising network that facilitates bidding for the placement opportunities;based on a result of the bidding, delivering an advertisement for placement within a user interface of the vehicle; andmonitoring vehicle rider interaction with the advertisement presented in the user interface of the vehicle.94. The method of claim 92 wherein the route-adjustment value is further based on at least one user response to the offering.923. The method of claim 922 wherein the vehicle comprises a system for automating at least one control parameter of the vehicle.924. The method of claim 923 wherein the vehicle is at least a semi-autonomous vehicle.925. The method of claim 924 wherein the vehicle is automatically routed.926. The method of claim 925 wherein the vehicle is a self-driving vehicle.927. The method of claim 922 wherein a first portion of the hybrid cognitive system determines an operating state of the vehicle by processing inputs relating to at least one parameter of the vehicle.928. The method of claim 922 wherein a second portion of the hybrid cognitive system determines a state of the rider of the vehicle by processing inputs relating to at least one parameter of the rider.929. The method of claim 922 wherein a third portion of the hybrid cognitive system determines at least one of a price, classification, content and location of an advertisement to be delivered within an interface of the vehicle to a rider in a seat in the vehicle based on the vehicle state and the rider state.930. The method of claim 922 wherein an advertisement is delivered from an advertiser who places a winning bid.931. The method of claim 922 wherein delivering an advertisement is based on a winning bid.932. The method of claim 922, further including resolution of payments as settlement for at least one of the delivering an advertisement and placing a winning bid for a placement opportunity.95. The method of claim 92 further comprising monitoring responses by the users of the set of vehicles to the set of offered user-indicated values.933. The method of claim 922, further including resolution of payments as settlement for a transaction caused by the rider interaction with the advertisement.934. The method of claim 922, further including providing the monitored vehicle rider interaction information to an advertising network.935. The method of claim 922 wherein the monitored vehicle rider interaction information includes information for resolving click-based payments.936. The method of claim 922 wherein the monitored vehicle rider interaction information includes an analytic result of the monitoring.937. The method of claim 936 wherein the analytic result is a measure of interest in the advertisement.938. The method of claim 922 wherein the inputs relating to the at least one parameter of a vehicle include vehicle classification.939. The method of claim 922 wherein the inputs relating to the at least one parameter of a vehicle include display classificatio",WO2020069517_A2.txt,"B60W20/12,B60W30/08,B60W30/14,B60W40/08,B60W40/12,B60W50/08",{'vehicles in general'},"['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit']","intelligent transportation systems Transportation systems have artificial intelligence including neural networks for recognition and classification of objects and behavior including natural language processing and computer vision systems. The transportation systems involve sets of complex chemical processes, mechanical systems, and interactions with behaviors of operators. System-level interactions and behaviors are classified, predicted and optimized using neural networks and other artificial intelligence systems through selective deployment, as well as hybrids and combinations of the artificial intelligence systems, neural networks, expert systems, cognitive systems, genetic algorithms and deep learning.",vehicles in general
US10319094_B1,2016-05-20,2019-06-11,2016-05-20,CCC INFORMATION SERVICES,"CHEN, KEGEORGHIADES, ATHINODOROS S.HALLER, JOHN L.KANADE, TAKEO",66767637,vehicle classification,"technology for capturing, transmitting, and analyzing images of objects","A system for analyzing images of objects such as vehicles. According to certain aspects, the system includes a user interface device configured to capture a set of images depicting a target vehicle, and transfer the set of images to a server that stores a set of base image models. The server analyzes the set of images using a base image model corresponding to the target vehicle, a set of correlational filters, and a set of convolutional neural networks (CNNs) to determine a set of changes to the target vehicle as depicted in the set of images. The server further transmits, to the user interface device, information indicative of the set of changes for a user to view or otherwise access.","1. A system for capturing and analyzing images of vehicles, comprising: a user interface device comprising: a user interface, an image sensor, a device communication module, and a device processor interfacing with the user interface, the image sensor, and the device communication module, the device processor configured to execute a set of instructions to cause the processor to: cause the image sensor to capture a set of images depicting a target vehicle, and transmit the set of images via the device communication module; and a server comprising: a database storing (i) a set of base object models, (ii) a set of correlation filters, and (iii) a set of convolutional neural networks (CNNs), a memory storing an image processing routine, a communication module, and a processor interfacing with the database, the memory, and the communication module, and configured to execute the image processing routine to cause the processor to: receive, via the communication module, the set of images from the user interface device, access, from the database, (i) a base object model of the set of base object models corresponding to the target vehicle, (ii) at least a portion of the set of correlation filters corresponding to the base object model, and (iii) at least a portion of the set of CNNs corresponding to the base object model, each CNN included in at least the portion of the set of CNNs that correspond to the base object model trained on images of a respective set of components of the base object model, analyze the set of images using the base object model, the at least the portion of the set of correlation filters, and the at least the portion of the set of CNNs, to determine a set of changes to the target vehicle as depicted in the set of images, including: align, by utilizing the at least the portion of the set of correlation filters, the base object model with an orientation of the target vehicle depicted in the set of images based on one or more landmarks detected in the set of images; detect, by utilizing one or more CNNs that are included in the at least the portion of set of CNNs and that correspond to a respective component of the target vehicle, one or more changes to the respective component of the target vehicle, the respective component determined based upon the aligned base model and the depiction of the target vehicle in the set of images; transmit, to the user interface device via the communication module, information indicative of the set of changes, the user interface device configured to present, in the user interface, the information indicative of the set of changes.2. The system of claim 1, wherein the database of the server further stores a set of changed image files associated with the set of base object models, and a set of information files, wherein the set of changed image files illustrate a set of changes to a set of base objects represented in the set of base object models, and wherein the set of information files include information associated with the set of changed image files.3. The system of claim 2, wherein the memory of the server further stores an image processing training routine, and wherein the processor is further configured to execute the image processing training routine to cause the processor to: for each base object represented in the set of base object models, determine, based on at least a portion of the set of changed image files and at least a portion of the set of information files, a corresponding correlation filter and a corresponding convolutional neural network (CNN), and cause the database to store the corresponding correlation filter and the corresponding CNN.4. The system of claim 1, wherein to analyze the set of images to determine the set of changes, the processor is configured to: analyze the set of images using the base object model, at least the portion of the set of correlation filters, and at least the portion of the set of CNNs, to detect damage to the target vehicle and a quantification of the damage to the target vehicle.5. The system of claim 1, wherein the processor is configured to execute the image processing routine to further cause the processor to: determine that the analysis of the set of images is indeterminate, and transmit, to the user interface device via the communication module, an indication that the analysis of the set of images is indeterminate, the user interface device configured to present the indication in the user interface.6. The system of claim 1, wherein the processor is configured to execute the image processing routine to further cause the processor to: generate a heat map depicting the set of changes to the target vehicle, and transmit the heat map to the user interface device via the communication module, the user interface device configured to present the heat map in the user interface.7. The system of claim 1, wherein the processor is configured to execute the image processing routine to further cause the processor to: analyze at least a portion of the set of images to identify (i) the target vehicle that is depicted in at least the portion of the set of images, and (ii) a set of information associated with the target vehicle, determine, based at least in part on the set of information associated with the target vehicle, that the set of images does not meet a threshold criteria for further image analysis using the base image model; and in response to determining that the set of images does not meet the threshold criteria: generate a notification indicating that the set of images does not meet the threshold criteria, and transmit the notification to the user interface device via the communication module, the user interface device configured to present the notification in the user interface.8. The system of claim 7, wherein the device processor is configured to execute the set of instructions to further cause the processor to: cause the image sensor to capture an additional set of images depicting the target vehicle, and transfer the additional set of images via the device communication module; and wherein the processor is configured to execute the image processing routine to further cause the processor to: receive, via the communication module, the additional set of images from the user interface device, analyze at least a portion of the additional set of images to identify an additional set of information associated with the target vehicle, and determine, based at least in part on the additional set of information associated with the target vehicle, that a combination of the set of images and the additional set of images meets the threshold criteria for further image analysis using the base image model.9. The system of claim 7, wherein the processor is configured to execute the image processing routine to further cause the processor to: determine image information that is needed to meet the threshold criteria for further image analysis using the base image model, wherein to generate the notification, the processor is configured to: generate the notification indicating at least the image information that is needed to meet the threshold criteria for further image analysis using the base image model.10. The system of claim 1, wherein the processor is configured to execute the image processing routine to further cause the processor to: performing a multi-dimensional alignment between a base object of the base object model and the target object depicted in the set of images.11. A system for analyzing images of vehicles, comprising: a database storing (i) a set of base object models, (ii) a set of correlation filters, and (iii) a set of convolutional neural networks (CNNs); a memory storing an image processing routine; a communication module; and a processor interfacing with the database, the memory, and the communication module, and configured to execute the image processing routine to cause the processor to: receive, via the communication module, a set of images from a user interface device, the set of images depicting a target vehicle, access, from the database, (i) a base object model of the set of base object models corresponding to the target vehicle, (ii) at least a portion of the set of correlation filters corresponding to the base object model, and (iii) at least a portion of the set of CNNs corresponding to the base object model, each CNN included in at least the portion of the set of CNNs that correspond to the base object model trained on images of a respective set of components of the base object model, analyze the set of images using the base object model, at least the portion of the set of correlation filters, and at least the portion of the set of CNNs, to determine a set of changes to the target vehicle as depicted in the set of images, including: align, by utilizing the at least the portion of the set of correlation filters, the base object model with an orientation of the target vehicle depicted in the set of images based on one or more landmarks detected in the set of images; detect, by utilizing one or more CNNs that are included in the at least the portion of set of CNNs and that correspond to a respective component of the target vehicle, one or more changes to the respective component of the target vehicle, the respective component determined based upon the aligned base model and the depiction of the target vehicle in the set of images; and transmit, to the user interface device via the communication module, information indicative of the set of changes, the user interface device configured to present, in a user interface, the information indicative of the set of changes.12. The system of claim 11, further comprising: the user interface device comprising: the user interface, an image sensor, a device communication module, and a device processor interfacing with the user interface, the image sensor, and the device communication module, the device processor configured to execute a set of instructions to cause the processor to: cause the image sensor to capture the set of images depicting the target vehicle, and transmit the set of images via the device communication module for receipt by the communication module.13. The system of claim 12, wherein the processor is configured to execute the image processing routine to further cause the processor to: determine that the analysis of the set of images is indeterminate, and transmit, to the user interface device via the communication module, an indication that the analysis of the set of images is indeterminate, the user interface device configured to present the indication in the user interface.14. The system of claim 12, wherein the processor is configured to execute the image processing routine to further cause the processor to: generate a heat map depicting the set of changes to the target vehicle, and transmit the heat map to the user interface device via the communication module, the user interface device configured to present the heat map in the user interface.15. The system of claim 12, wherein the processor is configured to execute the image processing routine to further cause the processor to: analyze at least a portion of the set of images to identify (i) the target vehicle that is depicted in at least the portion of the set of images, and (ii) a set of information associated with the target vehicle, determine, based at least in part on the set of information associated with the target vehicle, that the set of images does not meet a threshold criteria for further image analysis using the base image model; and in response to determining that the set of images does not meet the threshold criteria: generate a notification indicating that the set of images does not meet the threshold criteria, and transmit the notification to the user interface device via the communication module, the user interface device configured to present the notification in the user interface.16. The system of claim 15, wherein the device processor is configured to execute the set of instructions to further cause the processor to: cause the image sensor to capture an additional set of images depicting the target vehicle, and transmit the additional set of images via the device communication module; and wherein the processor is configured to execute the image processing routine to further cause the processor to: receive, via the communication module, the additional set of images from the user interface device, analyze at least a portion of the additional set of images to identify an additional set of information associated with the target vehicle, and determine, based at least in part on the additional set of information associated with the target vehicle, that a combination of the set of images and the additional set of images meets the threshold criteria for further image analysis using the base image model.17. The system of claim 11, wherein the database further stores a set of changed image files associated with the set of base object models, and a set of information files, wherein the set of changed image files illustrate a set of changes to a set of base objects represented in the set of base object models, and wherein the set of information files include information associated with the set of changed image files.18. The system of claim 17, wherein the memory further stores an image processing training routine, and wherein the processor is further configured to execute the image processing training routine to cause the processor to: for each base object represented in the set of base object models, determine, based on at least a portion of the set of changed image files and at least a portion of the set of information files, a corresponding correlation filter and a corresponding convolutional neural network (CNN), and cause the database to store the corresponding correlation filter and the corresponding CNN.19. The system of claim 11, wherein to analyze the set of images to determine the set of changes, the processor is configured to: analyze the set of images using the base object model, at least the portion of the set of correlation filters, and at least the portion of the set of CNNs, to detect damage to the target vehicle and a quantification of the damage to the target vehicle.20. The system of claim 11, wherein the processor is configured to execute the image processing routine to further cause the processor to: perform a multi-dimensional alignment between a base object of the base object model and the target object depicted in the set of images.",US10319094_B1.txt,"G06F16/583,G06F3/0484,G06N3/08,G06T7/00",{'computing; calculating; counting'},"['electric digital data processing (computer systems based on specific computational models g06n)', 'electric digital data processing (computer systems based on specific computational models g06n)', 'computing arrangements based on specific computational models', 'image data processing or generation, in general']","technology for capturing, transmitting, and analyzing images of objects A system for analyzing images of objects such as vehicles. According to certain aspects, the system includes a user interface device configured to capture a set of images depicting a target vehicle, and transfer the set of images to a server that stores a set of base image models. The server analyzes the set of images using a base image model corresponding to the target vehicle, a set of correlational filters, and a set of convolutional neural networks (CNNs) to determine a set of changes to the target vehicle as depicted in the set of images. The server further transmits, to the user interface device, information indicative of the set of changes for a user to view or otherwise access.",computing; calculating; counting
US2015161796_A1,2014-06-19,2015-06-11,2013-12-09,HYUNDAI MOTOR COMPANY,"KIM, JAE KWANGLEE, WAN JAELEE, KANG HOONKIM, JIN HAKCHOI, EUN JIN",53271698,road monitoring,method and device for recognizing pedestrian and vehicle supporting the same,"A method and a device for recognizing a pedestrian and a vehicle supporting the same are provided. The method includes collecting, by a controller, a far-infrared image using a far-infrared imaging device and detecting a pedestrian candidate group from the far-infrared image. In addition, the method includes extracting, by the controller, pedestrian features based on previously normalized pedestrian database (DB) learning and comparing the pedestrian features with the pedestrian DB learning results to determine similarity. The controller is configured to perform pedestrian recognition based on the comparison result.","1. A device for recognizing a pedestrian, the device comprising: a far-infrared imaging device configured to collect a far-infrared image of a predetermined area; and a controller configured to: detect a pedestrian candidate group from the far-infrared image; and draw and compare pedestrian features based on primary features among features detected by a classifier, while learning normalized pedestrian database (DB), to perform pedestrian detection.2. The device according to claim 1, wherein the controller is configured to perform pedestrian candidate group detection based on temperature information and object information in the far-infrared image.3. The device according to claim 1, wherein the controller is configured to determine a surrounding area of the pedestrian candidate group detected from the far-infrared image, and normalize the surrounding area of the pedestrian candidate group to have a size of a pedestrian area in the normalized pedestrian DB.4. The device according to claim 3, wherein the controller is configured to normalize the surrounding area of the pedestrian candidate group to have a size with a ratio of 1:2 of width and height.5. The device according to claim 1, wherein the controller is configured to apply an Adv_HOG scheme in which the pedestrian candidate group area is divided into square blocks adjustable in size and an angle range of 360 degrees is divided into nine bins to express angles, or apply a local binary pattern (LBP) scheme in which a value obtained by pattern changes in a currnent pixel value and a neighbor pixel value is applied to each block having adjustable size in the pedestrian candidate group area to configure a histogram to draw features.6. The device according to claim 1, wherein the controller is configured to perform clustering on an area in which objects overlap in the pedestrian detection result image to determine whether a single pedestrian is present or a plurality of pedestrians are present.7. A vehicle supporting a pedestrian recognition function, the vehicle comprising: a far-infrared imaging device configured to collect a far-infrared image of a predetermined area; a controller configured to: detect a pedestrian candidate group from the far-infrared image; and draw and compare pedestrian features based on primary features among features detected by a classifier, while learning normalized pedestrian database (DB), to perform pedestrian detection; and an information output device configured to output the pedestrian detection result.8. The vehicle according to claim 7, wherein the controller is configured to perform pedestrian candidate group detection based on of temperature information and object information in the far-infrared image, determines a surrounding area of the pedestrian candidate group detected from the far-infrared image, and normalize the surrounding area of the pedestrian candidate group to match the surrounding area to a size of a pedestrian area in the normalized pedestrian DB, and have a size having a ratio of 1:2 of width and height.9. The vehicle according to claim 7, wherein the controller is configured to apply an Adv_HOG scheme in which the pedestrian candidate group area is divided into square blocks adjustable in size and an angle range of 360 degrees is divided into nine bins to express angles, or apply a local binary pattern (LBP) scheme in which a value obtained by pattern changes in a current pixel value and a neighbor pixel value is applied to each block having adjustable size in the pedestrian candidate group area to configure a histogram to draw features.10. The vehicle according to claim 7, wherein the controller is configured to perform clustering on an area in which objects overlap in the pedestrian detection result image to determine whether a single pedestrian is present or a plurality of pedestrians are present.11. The vehicle according to claim 7, wherein the information output device includes at least one of a group consisting of: an audio device configured to output an alarm sound based on at least one of a distance between the pedestrian and a vehicle and a position of the pedestrian and a video device configured to output the pedestrian detection image.12. The vehicle according to claim 7, further comprising: at least one of a group consisting of: a timer configured to determine a time at which the pedestrian recognition function is automatically applied; an luminance sensor configured to detect ambient intensity of illumination to automatically apply the pedestrian recognition function; and a temperature sensor configured to detect ambient temperature to automatically apply the pedestrian recognition function.13. A method for recognizing a pedestrian, the method comprising: collecting, by a controller, a far-infrared image captured by a far-infrared imaging device; detecting, by the controller, a pedestrian candidate group from the far-infrared image; extracting, by the controller, pedestrian features based on previously normalized pedestrian database (DB) learning; comparing, by the controller, the pedestrian features with the pedestrian DB learning results to determine similarity; and performing, by the controller, pedestrian recognition based on the comparison result.14. The method according to claim 13, wherein the detecting includes performing, by the controller, the pedestrian candidate group detection based on temperature information and object information from the far-infrared image.15. The method according to claim 13, further comprising: determining, by the controller, a surrounding area of the pedestrian candidate group detected from the far-infrared image; and normalizing, by the controller, the surrounding area of the pedestrian candidate group such to correspond to a size of a pedestrian area in a normalized pedestrian database and have a size with a ratio of 1:2 in width and length.16. The method according to claim 13, wherein the feature extraction includes: extracting, by the controller, primary features among features drawn by a classifier during the database learning.17. The method according to claim 16, wherein the feature extraction includes at least one of: applying by the controller, an Adv_HOG scheme in which the pedestrian candidate group region is divided into square blocks adjustable in size and an angle range of 360 degrees is configured as 9 bins to express angles; and applying, by the controller, a local binary pattern (LBP) scheme in which a value obtained by patterning changes in a current pixel value and a neighbor pixel value is applied to each block having adjustable size in the pedestrian candidate group area to configure a histogram to draw features.18. The method according to claim 13, further comprising: determining, by the controller, whether a single pedestrian is present or a plurality of pedestrians are present with respect to an area in which detection objects overlap in the pedestrian detection result image.19. The method according to claim 13, further comprising at least one of: outputting, by the controller, an alarm sound based on at least one of a distance between a pedestrian and a vehicle and a position of the pedestrian; and outputting, by the controller, the pedestrian detection image.20. The method according to claim 13, further comprising at least one of: automatically applying, by the controller, the pedestrian recognition function when a pre-set time is reached; automatically applying, by the controller, the pedestrian recognition function when an luminance sensor value is less than or grater than a predetermined value; and automatically applying, by the controller, the pedestrian recognition function when a temperature sensor value is less than or greater than a predetermined value.",US2015161796_A1.txt,"G06T7/00,H04N5/33,H04N7/18","{'electric communication technique', 'computing; calculating; counting'}","['image data processing or generation, in general', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television']","method and device for recognizing pedestrian and vehicle supporting the same A method and a device for recognizing a pedestrian and a vehicle supporting the same are provided. The method includes collecting, by a controller, a far-infrared image using a far-infrared imaging device and detecting a pedestrian candidate group from the far-infrared image. In addition, the method includes extracting, by the controller, pedestrian features based on previously normalized pedestrian database (DB) learning and comparing the pedestrian features with the pedestrian DB learning results to determine similarity. The controller is configured to perform pedestrian recognition based on the comparison result.",electric communication technique computing; calculating; counting
US2020380266_A1,2019-09-04,2020-12-03,2019-05-28,APPLE,"FRAIOLI, NICHOLAS M.NADATHUR, ANUSH G.RAUENBUEHLER, KEITH W.York, Kenneth A.Singh, Varinder",73549730,survillance,techniques for secure video frame management,"Embodiments of the present disclosure can provide devices, methods, and computer-readable medium for secure frame management. The techniques disclosed herein provide an intelligent method for detecting triggering items in one or more frames of streaming video from an Internet Protocol camera. Upon detection, the camera transmits one or more frames of the video over a network to a computing device. Upon detecting a triggering item in a frame of the video stream, the computing device can begin a streaming session with a server and stream the one or more frames of video and accompanying metadata to the server. The frames, metadata, and associated keys can all be encrypted prior to streaming to the server. For each subsequent segment of video frames that includes the triggering item, the server can append the frames of that segment to the video clip in an encrypted container. Once the triggering item is no longer detected, the streaming session can be closed.","1. A method, comprising: in response to detection of motion by a camera connected to a network, receiving, by a computing device, one or more frames of a video stream and accompanying metadata from the camera; in accordance with detection of a triggering item in at least one frame of the one or more frames of the video stream: encrypting, by the computing device, the one or more frames of the video stream and the accompanying metadata; transmitting, to the network computing device, the one or more frames of the video stream and the accompanying metadata, the network computing device configured to store the encrypted one or more frames of the video stream and the encrypted accompanying metadata in a container; encrypting, by the computing device, one or more subsequent frames of the video stream and subsequent metadata; and transmitting, to the network computing device, the encrypted one or more subsequent frames of the video stream and the encrypted subsequent metadata, the network computing device configured to append the one or more encrypted subsequent frames of the video stream onto the one or more frames of the video stream and to store the encrypted subsequent metadata in the container.2. The method of claim 1, further comprising classifying, by the computing device, the one or more frames of the video stream by at least comparing one or more aspects of the one or more frames of the video stream from the camera against one or more criteria.3. The method of claim 2, wherein the classifying comprises utilizing an image recognition technique to determine a classification of the one or more frames of the video stream.4. The method of claim 2, wherein the classifying comprises a plurality of different classes including at least one of a person, an animal, and a vehicle.5. The method of claim 1, further comprising reviewing, by the computing device, each frame of the one or more frames of the video stream for the triggering item.6. The method of claim 1, further comprising establishing, by the computing device, a connection with a network computing device.7. The method of claim 6, further comprising: in accordance with detection of an absence of the triggering item in at least one subsequent frame of the one or more subsequent frames of the video stream: terminating the connection with the network computing device.8. The method of claim 1, further comprising decrypting the one or more frames of the video stream from the camera.9. The method of claim 1, wherein the accompanying metadata identifies a start time and an end time of the one or more frames of the video stream.10. A computing device for a network, comprising, one or more memories; and one or more processors in communication with the one or more memories and configured to execute instructions stored in the one or more memories to: receiving one or more frames of a video stream and accompanying metadata from a camera; in accordance with detection of a triggering item in at least one frame of the one or more frames of the video stream: encrypting, by the computing device, the one or more frames of the video stream and the accompanying metadata; transmitting, to a network computing device over a connection, the one or more frames of the video stream and the accompanying metadata, the network computing device configured to store the encrypted one or more frames of the video stream and the encrypted accompanying metadata in a container; encrypting, by the computing device, one or more subsequent frames of the video stream and subsequent metadata; and transmitting, to the network computing device over the connection, the encrypted one or more subsequent frames of the video stream and the encrypted subsequent metadata, the network computing device configured to append the one or more encrypted subsequent frames of the video stream onto the one or more frames of the video stream and to store the encrypted subsequent metadata in the container.11. The computer device of claim 10, wherein the container is encrypted using a symmetric encryption key.12. The computer device of claim 11, wherein the symmetric encryption key is a 256-bit encryption key.13. The computer device of claim 10, wherein the one or more frames of the video stream comprise a fragment of live video stream data from the camera.14. The computer device of claim 10, wherein the computing device is configured to play the one or more frames of the video stream using an HTTP-based adaptive bitrate streaming communication protocol.15. The computer device of claim 10, further comprising providing instructions to a second computing device to balance a load between the computing device and the second computing device.16. The computer device of claim 10, further comprising generating a private key for decryption.17. A computer-readable medium storing a plurality of instructions that, when executed by one or more processors of a computing device, cause the one or more processors to perform operations comprising: receiving, by the computing device, one or more frames of a video stream and accompanying metadata from an Internet Protocol camera; reviewing, by the computing device, each frame of the one or more frames of the video stream for a triggering item; in accordance with detection of the triggering item in at least one frame of the one or more frames of the video stream: encrypting, by the computing device, the one or more frames of the video stream and the accompanying metadata; transmitting, to a network computing device over a connection, the one or more frames of the video stream and the accompanying metadata, the network computing device configured to store the encrypted one or more frames of the video stream and the encrypted accompanying metadata in a storage container; encrypting, by the computing device, one or more subsequent frames of the video stream and subsequent metadata; and transmitting, to the network computing device over the connection, the encrypted one or more subsequent frames of the video stream and the encrypted subsequent metadata, the network computing device configured to append the one or more encrypted subsequent frames of the video stream onto the one or more frames of the video stream and to store the encrypted subsequent metadata in the storage container18. The computer-readable medium of claim 17, further comprising performing balancing operations between one or more processors for a load of the one or more frames of the video stream being received from the Internet Protocol camera.19. The computer-readable medium of claim 18, wherein the balancing operations comprise reassigning operations to a second computing device based at least in part on bandwidth of the one or more processors of the computing device and network bandwidth.20. The computer-readable medium of claim 17, further comprising instructions assigning access control of a specific camera to a user outside a primary network.",US2020380266_A1.txt,"G06K9/00,H04L9/08,H04N5/14,H04N5/232","{'electric communication technique', 'computing; calculating; counting'}","['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'transmission of digital information, e.g. telegraphic communication ({coding or ciphering apparatus for cryptographic or other purposes involving the need for secrecy g09c;} arrangements common to telegraphic and telephonic communication h04m)', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television']","techniques for secure video frame management Embodiments of the present disclosure can provide devices, methods, and computer-readable medium for secure frame management. The techniques disclosed herein provide an intelligent method for detecting triggering items in one or more frames of streaming video from an Internet Protocol camera. Upon detection, the camera transmits one or more frames of the video over a network to a computing device. Upon detecting a triggering item in a frame of the video stream, the computing device can begin a streaming session with a server and stream the one or more frames of video and accompanying metadata to the server. The frames, metadata, and associated keys can all be encrypted prior to streaming to the server. For each subsequent segment of video frames that includes the triggering item, the server can append the frames of that segment to the video clip in an encrypted container. Once the triggering item is no longer detected, the streaming session can be closed.",electric communication technique computing; calculating; counting
US2020372791_A1,2020-05-15,2020-11-26,2019-05-24,E-Motion Inc.,"Li, LarryLi, Hannah",73457273,road monitoring,crowdsourced realtime traffic images and videos,"A traffic and road condition monitoring system utilizing a vehicle mounted camera recording images of traffic, road conditions ahead of the vehicle wherein the recorded images are uploaded to a component that can distribute the images in real time to remote display components, a transmission component that transmits the image to a remote server in real time as the image is recorded, an image storage component operable with the server, a transmission component operable with the server to transmit real time or stored images to a remote display monitor, and a data processor component to compute distance or speed of objects within the image. The remote display monitor may be positioned in a separate vehicle such as a following vehicle. The traffic images may be shared directly between vehicles in a peer to peer network.","What we claim are:1. A traffic and road condition monitoring system utilizing a vehicle mounted camera recording images of traffic, road conditions ahead of the vehicle wherein the recorded images are uploaded to a component that can distribute the images in real time to display components comprising: (a) a forward facing camera positioned within a vehicle wherein the camera records an image of traffic and road conditions in front of the vehicle; (b) a transmission component that transmits the image to a remote server in real time as the image is recorded; (c) an image storage component operable with the server; (d) a transmission component operable with the server to transmit real time or stored images to one or more remote display monitors; (e) a data processor component to compute distance or speed of objects within the image.2. The traffic and road condition monitoring system of claim 1 further comprising a computer processor and software configured for identification of objects appearing within the image.3. The traffic and road condition monitoring system of claim 2 wherein the object identification can be displayed within the remote display monitors.4. The traffic and road condition monitoring system of claim 1 wherein the speed or distance of objects is determined by a component within the vehicle.5. The traffic and road condition monitoring system of claim 3 wherein the objects are signs.6. The traffic and road condition monitoring system of claim 5 wherein the signs are traffic signs.7. The traffic and road condition monitoring system of claim 5 wherein the signs are vendor signs.8. The traffic and road condition monitoring system of claim 5 wherein the system reads a QR code within the sign.9. The traffic and road condition monitoring system of claim 8 wherein the system displays information from the QR code within the remote display monitors.10. The traffic and road condition monitoring system of claim 1 wherein the camera is a smart phone or tablet.11. The traffic and road condition monitoring system of claim 1 wherein the image display monitor is a smart phone or tablet.12. A traffic or road condition monitoring system providing vehicles with information of traffic or road conditions from images transmitted from other vehicles comprising: (a) a vehicle transmitting an image of traffic or road conditions as seen from an image capturing device observing conditions in front of the transmitting vehicle; (b) transmitting the image to a remote server in real time; (c) the remote server transmitting the image in real time to a requesting second vehicle for display on a monitor located within the second vehicle; (d) the image transmitted to the second vehicle augmented by analysis of the image to provide speed or distance information of an object within the transmitted image.13. A traffic or road condition monitoring system of claim 12 further comprising components within the vehicle to augment the transmitted image.14. A traffic or road condition monitoring system of claim 12 further comprising uploading images utilizing protocol such as RTMP.15. The traffic or road condition monitoring system of claim 12 further comprising transmitting object identification information to the second vehicle.16. The traffic or road condition monitoring system of claim 12 wherein speed, distance or object identification is performed by components of the remote server.17. The traffic or road condition monitoring system of claim 12 wherein the object is a vehicle.18. A method of evaluating traffic or road conditions utilizing information obtained from images of traffic displayed by one or more vehicles comprising: (a) operating a first vehicle; (b) requesting from a remote server at least one uploaded image from a camera within a separate second vehicle wherein the second vehicle is proximate to a location specified by a person occupying the first vehicle; (c) receiving in the first vehicle a downloaded real time image from a camera within the second vehicle wherein the image may contain traffic information including identified objects, road conditions, or weather ahead of the second vehicle and speed of the objects or location; and (d) utilizing the downloaded image to evaluate a route, direction or speed of the first vehicle.19. The method of evaluating traffic conditions of claim 18 further comprising initiating an additional request to the remote server or remote vehicles for one or more images from cameras within other vehicles.20. The method of evaluating traffic conditions of claim 18 further comprising requesting the remote server for suggested route or direction information.21. An anonymous vehicle to vehicle communication method allowing one vehicle receiving a real time image display to communicate with the vehicle providing the image comprising: (a) a first vehicle recording an image of traffic and road conditions and uploading the image to a remote server wherein the first vehicle possesses video or voice capability and an IP address; (b) a second vehicle initiating a request to the remote server to have video or voice communication with the first vehicle via a component having an IP address; (c) the remote server, having access to the IP addresses of both the first vehicle and second vehicle, creating a communication link between the first vehicle and second vehicle wherein identifying information of either the first vehicle or second vehicle is not disclosed.22. The anonymous vehicle to vehicle communication method of claim 21 utilizing WebRTC protocol.",US2020372791_A1.txt,"G06K9/00,G08G1/01,H04W4/44","{'electric communication technique', 'signalling', 'computing; calculating; counting'}","['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'wireless communication networks (broadcast communication h04h; communication systems using wireless links for non-selective communication, e.g. wireless extensions h04m1/72)']","crowdsourced realtime traffic images and videos A traffic and road condition monitoring system utilizing a vehicle mounted camera recording images of traffic, road conditions ahead of the vehicle wherein the recorded images are uploaded to a component that can distribute the images in real time to remote display components, a transmission component that transmits the image to a remote server in real time as the image is recorded, an image storage component operable with the server, a transmission component operable with the server to transmit real time or stored images to a remote display monitor, and a data processor component to compute distance or speed of objects within the image. The remote display monitor may be positioned in a separate vehicle such as a following vehicle. The traffic images may be shared directly between vehicles in a peer to peer network.",electric communication technique signalling computing; calculating; counting
US10467482_B2,2017-02-17,2019-11-05,2016-02-17,FORD GLOBAL TECHNOLOGIES,"SEEMANN, MICHAELLAKEHAL-AYAT, MOHSENPOTMA, JOOST",59560369,road monitoring,method and arrangement for assessing the roadway surface being driven on by a vehicle,"The disclosure relates to a method and an arrangement for assessing the roadway surface being driven on by a vehicle. In a method according to the disclosure, on the basis of at least one image recorded with a camera that is present on the vehicle, the roadway surface being driven on by the vehicle is classified using a classifier. The classifier is trained on the basis of image features that are extracted from the at least one image, wherein a plurality of image details are defined in the at least one image. The extraction of image features is performed independently for each of these image details.","1. A method for assessing a roadway surface for a vehicle comprising: capturing an image of a roadway with a vehicle-mounted camera, the image defining a plurality of image details; extracting image features independently for each of the plurality of image details, wherein at least some of the features are textural details based on an entropy of the image; classifying the roadway surface using a classifier to determine a roadway class, the classifier including the image features as input data, wherein the classifier is trained based on the extracted image features; determining weather conditions with at least one on-board vehicle sensor; and utilizing a driver-assistance system to control the vehicle according to a friction map that is based the roadway class and the weather conditions.2. The method as claimed in claim 1 further comprising weighing the image features extracted for each of the plurality of image details using different weights.3. The method as claimed in claim 1, wherein the classifying of the roadway surface takes place in real time.4. The method as claimed in claim 1 further comprising checking the classification of the roadway surface based on validation data.5. The vehicle as claimed in claim 1, wherein the friction map indicates a coefficient of tire friction between the roadway and tires of the vehicle.6. A vehicle arrangement for assessing the roadway surface comprising: a camera disposed on the vehicle and configured to capture at least one image of a roadway, the image defining a plurality of image details; and a processing unit configured to: extract image features independently for each of the plurality of image details, wherein at least some of the features are textural details based on an entropy of the image, classify the roadway surface using a classifier to determine a roadway class, the classifier including the image features as input data, determining weather conditions proximate the vehicle, and control a driver-assistance system of the vehicle according to a friction map that is based the roadway class and the weather conditions.7. The vehicle arrangement as claimed in claim 6, wherein the processing unit is further configured to weight each of the image features differently via the classifier.8. The vehicle arrangement as claimed in claim 6, wherein the processing unit classifies the roadway surface in real time.9. The vehicle as claimed in claim 6, wherein the camera is a black-and-white camera.10. The vehicle as claimed in claim 6, wherein the friction map indicates a coefficient of tire friction between the roadway and tires of the vehicle.11. The vehicle as claimed in claim 6, wherein at least some of the textural details are based on a gray-value matrix of the image.12. The vehicle as claimed in claim 6, wherein the classifier is trained based on the extracted image features.13. A vehicle comprising: a camera that records an image having image details of a roadway; and a processor configured to, extract image features for each of the image details, wherein at least some of the features are textural details based on a gray-value matrix of the image, classify the roadway surface using a classifier to determine a roadway class, the classifier including the image features as input data, determining weather conditions proximate the vehicle, and control a driver-assistance system of the vehicle according to a friction map that is based the roadway class and the weather conditions.14. The vehicle as claimed in claim 13, wherein at least some of the textural details are based on an entropy of the image.15. The vehicle as claimed in claim 13, the processor classifies the roadway in real time.16. The vehicle as claimed in claim 13, wherein the friction map indicates a coefficient of tire friction between the roadway and tires of the vehicle.17. The vehicle as claimed in claim 13, wherein the camera is a black-and-white camera.18. The vehicle as claim in claim 13, wherein the classifier is trained based on the extracted image features.",US10467482_B2.txt,"B60R11/04,G05D1/02,G06K9/00,G06K9/03,G06K9/46,G06K9/62,H04N7/18","{'controlling; regulating', 'electric communication technique', 'computing; calculating; counting', 'vehicles in general'}","['vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'systems for controlling or regulating non-electric variables (for continuous casting of metals b22d11/16; valves per se f16k; sensing non-electric variables, see the relevant subclasses of g01; for regulating electric or magnetic variables g05f)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'pictorial communication, e.g. television']","method and arrangement for assessing the roadway surface being driven on by a vehicle The disclosure relates to a method and an arrangement for assessing the roadway surface being driven on by a vehicle. In a method according to the disclosure, on the basis of at least one image recorded with a camera that is present on the vehicle, the roadway surface being driven on by the vehicle is classified using a classifier. The classifier is trained on the basis of image features that are extracted from the at least one image, wherein a plurality of image details are defined in the at least one image. The extraction of image features is performed independently for each of these image details.",controlling; regulating electric communication technique computing; calculating; counting vehicles in general
US2020342247_A1,2020-04-23,2020-10-29,2019-04-24,"CRC R&D, LLC","MAINS, JR., RONALD H.Guillot, JohnRoos, Matthew",70739159,vehicle classification,systems and methods for automatic recognition of vehicle information,"Disclosed systems and methods provide automatic recognition of information from stationary and/or moving vehicles. A disclosed system includes an image capture device that captures an image of a vehicle surface and thereby generates image data. A processor circuit receives the image data from the image capture device and may process the image data to determine a Department of Transportation (DOT) number. The processor circuit may control the image capture device to capture a plurality of images, to detect and recognize text characters in each of the plurality of images, and to compare probabilities of likely DOT numbers determined from each of the plurality of images. The processor circuit may be further configured to determine DOT numbers from captured images by processing image data using a machine learning algorithm. The system may be configured to be portable and to perform real-time analysis using an application specific integrated circuit (ASIC).","1. An information capture and recognition system, comprising: an image capture device configured to capture an image of a vehicle surface; and a processor circuit configured to perform operations including: controlling the image capture device to capture an image of a vehicle surface to thereby generate image data; and processing the image data to determine a Department of Transportation (DOT) number.2. The system of claim 1, wherein the processor circuit is further configured to perform operations comprising: performing a detection operation to generate box coordinates characterizing boxes that enclose sections of text found in the image data; performing a recognition operation to determine a sequence of text characters found within one or more boxes defined by the box coordinates; and performing a decision operation to identify the DOT number.3. The system of claim 2, wherein performing the detection operation further comprises generating box coordinates including numerical values for a horizontal center, a vertical center, a width, a height, and a rotation angle.4. The system of claim 2, wherein determining a sequence of text characters further comprises generating a set of probabilities that characterize uncertainties associated with determined text characters.5. The system of claim 2, wherein identifying the DOT number further comprises determining probabilistic locations of characters associated with the DOT number.6. The system of claim 1, wherein the processor circuit is further configured to perform operations comprising: controlling the image capture device to capture a plurality of images to generate image data based on the plurality of images; and determining the DOT number by performing operations including: detecting and recognizing text characters in each of the plurality of images; and comparing probabilities of likely DOT numbers determined from each of the plurality of images.7. The system of claim 1, wherein processing the image data further comprises performing a machine learning algorithm to determine the DOT number based on the image data.8. The system of claim 7, further comprising an application specific integrated circuit (ASIC) that is configured to perform the machine learning algorithm to determine the DOT number based on the image data.9. The system of claim 1, further comprising: a power source that includes one or more of a battery or a solar power generation device, wherein the system is configured to be a portable system.10. The system of claim 1, further comprising: a motion detection sensor that is configured to generate a signal characterizing motion of a vehicle, wherein the processor circuit is further configured to perform operations including: receiving the signal from the motion detection sensor; and controlling the image capture device to capture one or more images based on the received signal.11. The system of claim 1, wherein the processor circuit is further configured to determine a relationship between the determined DOT number and a license plate number of the vehicle.13. The system of claim 1, wherein the image capture device is further configured to capture a first image containing a license plate number and a second image containing a DOT number, and wherein the processor circuit is further configured to determine a license plate number from the first image and to determine a DOT number from the second image.14. The system of claim 13, further comprising: two or more image capture devices, wherein a first image capture device captures the first image containing the license plate number and a second image capture device captures the second image containing the DOT number.15. A processor-implemented method of capturing and recognizing information, the method comprising: controlling, by a processor circuit, an image capture device to capture an image of a vehicle surface to thereby generate image data; and processing the image data to determine a DOT number.16. The method of claim 15, further comprising: performing, by the processor circuit, a detection operation to generate box coordinates characterizing boxes that enclose sections of text found in the image data; performing a recognition operation to determine a sequence of text characters found within one or more boxes defined by the box coordinates; and performing a decision operation to identify the DOT number.17. The method of claim 15, further comprising: controlling, by the processor circuit, the image capture device to capture a plurality of images to generate image data based on the plurality of images; and determining the DOT number by performing operations including: detecting and recognizing text characters in each of the plurality of images; and comparing probabilities of likely DOT numbers determined from each of the plurality of images.18. The system of claim 15, wherein processing the image data further comprises performing a machine learning algorithm to determine the DOT number based on the image data.19. The system of claim 15, further comprising: determining, by the processor circuit, a relationship between the determined DOT number and a license plate number of the vehicle.20. A non-transitory computer-readable storage medium, having computer program instructions stored thereon that, when executed by a processor circuit, cause the processor circuit to perform operations comprising: receiving a signal from a motion detection sensor; controlling an image capture device to capture one or more images based on the received signal to thereby generate image date; and processing the image data to determine a DOT number.",US2020342247_A1.txt,"G06K9/32,G06K9/62,H04N7/18","{'electric communication technique', 'computing; calculating; counting'}","['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'pictorial communication, e.g. television']","systems and methods for automatic recognition of vehicle information Disclosed systems and methods provide automatic recognition of information from stationary and/or moving vehicles. A disclosed system includes an image capture device that captures an image of a vehicle surface and thereby generates image data. A processor circuit receives the image data from the image capture device and may process the image data to determine a Department of Transportation (DOT) number. The processor circuit may control the image capture device to capture a plurality of images, to detect and recognize text characters in each of the plurality of images, and to compare probabilities of likely DOT numbers determined from each of the plurality of images. The processor circuit may be further configured to determine DOT numbers from captured images by processing image data using a machine learning algorithm. The system may be configured to be portable and to perform real-time analysis using an application specific integrated circuit (ASIC).",electric communication technique computing; calculating; counting
CN104200668_B,2014-07-28,2017-07-11,2014-07-28,SICHUAN UNIVERSITY,WANG MAONINGQIU DUNGUO,52085952,survillance,image-analysis-based detection method for helmet-free motorcycle driving violation event,"The invention discloses an image-analysis-based detection method for a helmet-free motorcycle driving violation event. According to the detection method, a high-resolution camera is arranged on a motorcycle running lane, a virtual coil in which a motorcycle runs is set in a set lane range, a tail license plate image of the motorcycle is captured in the virtual coil according to the default width range of the motorcycle, an acquired image sequence is transmitted to a detection host through the high-resolution camera, and detection host is used for detecting the motorcycle in the current lane and performing helmet detection and license plate identification according to the received image. In helmet detection, a head top average saturation degree and an initial set value Sd are calculated and compared with each other, and a model identification technology based on a support vector machine is used for identifying the license plate. Field law enforcement and license plate identification can be performed without a large amount of armed police force, thereby effectively avoiding dangerous accidents caused by field law enforcement. The method can be implemented on the conventional electronic police system, and has the advantages of small investment, high feasibility and wide application foundation.","1. A motorcycle peccancy event detection method without wearing a helmet based on image analysis is characterized in that: installing a high-definition camera on a motorcycle driving lane, setting a virtual coil for motorcycle driving in a set lane range, capturing images of a tail license plate of a motorcycle in the virtual coil according to a default width range of the motorcycle, sending an acquired image sequence to a detection host by the high-definition camera, and performing motorcycle detection, helmet detection and license plate recognition on a current lane by the detection host according to the received images; 1) detection of motorcycles As the motorcycle is a moving target, in order to separate the moving target from the background, firstly, a Gaussian model background model is adopted according to an image sequence obtained from a camera to establish a background image, then the background image is subtracted from the moving target image to extract the moving target, a target tracking mode is adopted to detect the tracking target, the judgment is carried out according to various characteristics of the target size, the driving speed and the driving direction so as to determine whether the target is the motorcycle, a motorcycle image is extracted from the target judged as the motorcycle, and the region where the motorcycle is located in the image is obtained; 2) helmet detection Adopting an image analysis technology, and aiming at the color difference between the helmet and the hair, judging the saturation condition of the top color of the motorcycle target currently detected so as to judge whether the current motorcycle driver wears the helmet or not; the specific process of helmet detection is as follows: according to the extracted region of the motorcycle in the image when motorcycle detection is carried out, extracting a small region at the uppermost part of the motorcycle image region, wherein the small region is the top of the head of a motorcycle driver, converting the color of the top image from RGB into HSV, and calculating the average saturation value S of the partial region image at the top of the headaThe average saturation is compared with a set initial value SdMaking a comparison if Sa<SdJudging that the motorcycle driver does not wear a helmet, selecting an image, and generating an illegal video, wherein RGB is three colors of red, green and blue, and HSV is Hue chroma, Saturation and Value lightness; 3) motorcycle number plate recognition Adopting a mode identification technology based on a support vector machine to identify the number plate: firstly, establishing a character library of motorcycle Chinese characters, letters and numbers under various environmental conditions, searching a number plate area when identifying the number plate, then segmenting the area of each font in the number plate image according to the characteristics of the motorcycle number plate, then comparing the segmented font image with the character library, judging which type the font belongs to according to the degree of confidence, and finishing the identification of the number plate. 2.The image analysis-based motorcycle helmet-less violation event detection method of claim 1, wherein: the set initial value Sd=12; The default width range of the motorcycle is 1.2-1.8 meters; the step of selecting the image and generating the violation video is to select 2 frames of images with the interval larger than 1 meter and generate the violation video in 15 seconds as the evidence of motorcycle violation. 3. The image analysis-based motorcycle helmet-less violation event detection method of claim 1, wherein: the method comprises the steps that a high-definition camera is installed on a motorcycle driving lane to capture images of a tail plate of a motorcycle, a 200-ten-thousand-pixel high-definition camera is used for covering one lane, an LED light supplement lamp is matched with each lane to supplement light at night, the camera captures images of the tail plate of the motorcycle in a back capturing mode, a detection host is connected with 2 cameras, and detection of the two cameras covering the lanes is completed. 4. The image analysis-based motorcycle helmet-less violation event detection method of claim 1, wherein: the implementation process comprises the following steps: 1) parameter setting Capturing a lane image, and finishing the following initial setting according to the captured lane image: setting a lane line: setting a lane range in which the motorcycle runs in the image, setting a virtual coil in the lane range, carrying out tracking detection only on a target in the lane range by detection software, not considering other areas, determining the running direction of the vehicle by drawing the lane, and improving the detection accuracy; setting a detection area: two parallel lines are defined in the lower part area of the image, the lines are vertical to the lane, and the detection host machine only carries out motorcycle snapshot and recognition on the area between the two parallel lines; lane calibration: drawing a rectangular area on the lane image, setting the width and height of the rectangular area according to the actual value corresponding to the rectangular area, calibrating the road, and calculating the actual width and height represented by each pixel point on the video image, so that the size and speed of the target can be judged according to the position of the target, some interference targets are eliminated, and the accuracy of motorcycle detection is improved; setting the width range of the motorcycle target: setting a width range of a motorcycle target, wherein only the target in the range is identified as the motorcycle, and the default width range of the motorcycle is 1.2-1.8 m; setting a saturation threshold: setting the saturation SaInitial threshold value Sd,SdDefault value is 12, and saturation S isaAnd initial threshold value SdComparing and judging whether the motorcycle driver wears the helmet or not; 2) video image acquisition Collecting the video image of the motorcycle into a detection host computer in real time through a camera for analysis and detection; 3) generating a lane background image In the video vehicle image, the background image of the lane usually only changes along with the adjustment of the external illumination and the camera parameters, and the change is less, so the system obtains the background image of the lane by removing the moving target on the lane at intervals; 4) obtaining a moving object grayscale image Subtracting the gray value of each frame of vehicle image from the gray value of the background image to obtain a gray value image of the moving target; 5) moving object tracking Tracking a moving target by adopting a Kalman filtering tracking technology, and determining a vehicle target according to the size, the direction and the speed of the target; 6) motorcycle identification When the moving target reaches the virtual coil, determining whether the moving target accords with the characteristics of the motorcycle according to the size, the speed and the moving direction of the target, and finishing motorcycle identification; 7) motorcycle helmet detection According to the captured motorcycle image, the position of a motorcycle driver is determined, the image of the overhead area of the driver is obtained, and the color value of each pixel of the overhead image is converted from RGBIs HSV, and calculates the average saturation value S of the image of the head top regionaAverage saturation SaAnd initial threshold value SdComparison, if Sa<SdThe saturation degree of the head top area is small, and the motorcycle driver is judged not to wear the helmet; 8) identifying the number plate of the motorcycle: for the captured illegal motorcycle image, firstly, determining the motorcycle license plate position according to a motorcycle target area, cutting out a license plate area, cutting out each character, and identifying the license plate by using a mode identification technology based on a support vector machine; 9) obtaining picture video For detecting the violation behaviors without wearing a helmet, 2 violation pictures and a section of video image for 15 seconds are captured and stored in a hard disk; 10) data upload And transmitting the detected violation pictures, videos and detection information to an intelligent traffic management center system through a network.",CN104200668_B.txt,"G06K9/34,G08G1/017","{'signalling', 'computing; calculating; counting'}","['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})']","image-analysis-based detection method for helmet-free motorcycle driving violation event The invention discloses an image-analysis-based detection method for a helmet-free motorcycle driving violation event. According to the detection method, a high-resolution camera is arranged on a motorcycle running lane, a virtual coil in which a motorcycle runs is set in a set lane range, a tail license plate image of the motorcycle is captured in the virtual coil according to the default width range of the motorcycle, an acquired image sequence is transmitted to a detection host through the high-resolution camera, and detection host is used for detecting the motorcycle in the current lane and performing helmet detection and license plate identification according to the received image. In helmet detection, a head top average saturation degree and an initial set value Sd are calculated and compared with each other, and a model identification technology based on a support vector machine is used for identifying the license plate. Field law enforcement and license plate identification can be performed without a large amount of armed police force, thereby effectively avoiding dangerous accidents caused by field law enforcement. The method can be implemented on the conventional electronic police system, and has the advantages of small investment, high feasibility and wide application foundation.",signalling computing; calculating; counting
US2018074200_A1,2017-11-21,2018-03-15,2017-11-21,GM GLOBAL TECHNOLOGY OPERATIONS,"BRANSON, ELLIOTHARRIS, SEANLIU, MARK",61559785,survillance,systems and methods for determining the velocity of lidar points,"A processor-implemented method in a vehicle for detecting the motion of lidar points includes: constructing a sequence of voxel grids surrounding the vehicle at each of a plurality of successive time increments wherein the sequence of voxel grids includes a voxel grid for the current time and a voxel grid for each of a plurality of past time instances, tracing in each voxel grid in the sequence, lidar beams from a lidar system on the vehicle through the voxel grid, analyzing differences across the sequence of voxel grids to produce a motion score for a plurality of regions in the voxel grid for the current time that characterizes the degree of motion in the region over the successive time increments, summing the motion scores of the regions across columns to produce a summed motion score for each column of regions, and producing a 2D image from the summed motion scores.","1. A processor-implemented method in a vehicle for detecting the motion of lidar points and generating a two-dimensional (2D) top-down map that identifies moving objects, the method comprising: constructing, by the processor, a sequence of computer-generated voxel grids surrounding the vehicle at each of a plurality of successive time increments, the sequence of voxel grids including a voxel grid for the current time and a voxel grid for each of a plurality of past time instances; tracing, by the processor, in each voxel grid in the sequence, lidar beams from a lidar system on the vehicle through the voxel grid; analyzing, by the processor, differences across the sequence of voxel grids to produce a motion score for a plurality of regions in the voxel grid for the current time that characterizes the degree of motion in the region over the successive time increments; summing, by the processor, the motion scores of the regions across columns to produce a summed motion score for each column of regions; producing, by the processor, a 2D image from the summed motion scores.2. The method of claim 1, wherein the plurality of successive time increments comprises at least eight successive time increments.3. The method of claim 1, wherein constructing a sequence of computer-generated voxel grids comprises constructing a voxel grid for the current time by adding voxels to a front face of a voxel grid for a prior time instance wherein the number of voxels added corresponds to the amount of vehicle movement in the front face direction and removing voxels from a rear face of the voxel grid for the prior time instance wherein the number of voxels removed corresponds to the amount of vehicle movement in the direction opposite to the rear face direction.4. The method of claim 1, wherein analyzing differences across the sequence of voxel grids comprises applying a machine learning classifier to the successive images.5. The method of claim 4, wherein analyzing differences across the sequence of voxel grids comprises applying a random forest classifier to the successive images.6. The method of claim 1, wherein analyzing differences across the sequence of voxel grids to produce a motion score for a plurality of regions in the voxel grid comprises: sub-dividing the voxel grid for the current time into a plurality of regions; identifying the regions in the voxel grid for the current time that contain occupied voxels; and producing a motion score for each identified region that characterizes the degree of motion in the identified region over the successive time increments by analyzing differences between the voxels in the identified regions and the voxels in corresponding regions in the voxel grids for past time instances.7. The method of claim 1, wherein a region comprises a rectangular prism of voxels.8. The method of claim 1, wherein the 2D image identifies objects that are in motion.9. The method of claim 8, wherein the 2D image identifies the velocity of objects that are in motion.10. The method of claim 1, wherein an identified region comprises a region wherein a lidar beam terminates in the center voxel of the region.11. The method of claim 1, wherein tracing lidar beams through the voxel grid comprises: assigning a first characteristic to a voxel if a lidar beam travels through the voxel; assigning a second characteristic to a voxel if no lidar beam travels through voxel; and assigning a third characteristic to a voxel if a lidar beam terminates at that voxel.12. The method of claim 11, wherein the first characteristic is clear, the second characteristic is unknown, and the third characteristic is occupied.13. A processor-implemented method in a vehicle for determining the velocity of lidar points, the method comprising: constructing, by a processor, a voxel grid around the vehicle; identifying, by the processor, an object in the voxel grid; retrieving, by the processor, a sequence of camera images that encompass the object; matching, by the processor, pixels in the sequence of camera images that encompass the object to corresponding voxels in the voxel grid that encompass the object; determining, by the processor, the velocity of the pixels that encompass the object from the sequence of camera images; and inferring, by the processor, the velocity of the corresponding voxels that encompass the object based on the velocity of the pixels that encompass the object.14. The method of claim 13, wherein determining the velocity of the pixels comprises analyzing the movement of the object in successive images in the sequence of images.15. The method of claim 13, wherein identifying an object in the voxel grid comprises tracing lidar beams from a lidar system on the vehicle through the voxel grid.16. The method of claim 15, wherein tracing lidar beams through the voxel grid comprises: assigning a first characteristic to a voxel if a lidar beam travels through the voxel; assigning a second characteristic to a voxel if no lidar beam travels through voxel; and assigning a third characteristic to a voxel if a lidar beam terminates at that voxel.17. The method of claim 16, wherein the first characteristic is clear, the second characteristic is unknown, and the third characteristic is occupied.18. The method of claim 17, wherein identifying an object in the voxel grid comprises identifying voxels that have been assigned an occupied characteristic.19. The method of claim 13, wherein matching pixels in the sequence of camera images that encompass the object to corresponding voxels comprises synchronizing the position and time of the pixels with the position and time of the voxels.20. An autonomous vehicle comprising: an imaging system configured to generate image data; a lidar system configured to generate lidar data; and a velocity mapping system configured to infer from the image data the velocity of voxels that encompass an object based on the velocity of pixels in the image data, the velocity mapping system comprising one or more processors configured by programming instructions encoded in non-transient computer readable media, the velocity mapping system configured to: construct a voxel grid around the vehicle; identify an object in the voxel grid; retrieve a sequence of camera images that encompass the object; match pixels in the sequence of camera images that encompass the object to corresponding voxels in the voxel grid that encompass the object; determine the velocity of the pixels that encompass the object from the sequence of camera images; and infer the velocity of the corresponding voxels that encompass the object based on the velocity of the pixels that encompass the object.",US2018074200_A1.txt,"G01S17/02,G01S17/58,G01S17/89,G01S7/48",{'measuring; testing'},"['radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves']","systems and methods for determining the velocity of lidar points A processor-implemented method in a vehicle for detecting the motion of lidar points includes: constructing a sequence of voxel grids surrounding the vehicle at each of a plurality of successive time increments wherein the sequence of voxel grids includes a voxel grid for the current time and a voxel grid for each of a plurality of past time instances, tracing in each voxel grid in the sequence, lidar beams from a lidar system on the vehicle through the voxel grid, analyzing differences across the sequence of voxel grids to produce a motion score for a plurality of regions in the voxel grid for the current time that characterizes the degree of motion in the region over the successive time increments, summing the motion scores of the regions across columns to produce a summed motion score for each column of regions, and producing a 2D image from the summed motion scores.",measuring; testing
US10740891_B1,2016-05-20,2020-08-11,2016-05-20,CCC INFORMATION SERVICES,"GEORGHIADES, ATHINODOROS S.CHEN, KEHALLER, JOHN L.KANADE, TAKEO",71994217,vehicle classification,technology for analyzing images depicting vehicles according to base image models,"A method and system for analyzing images of a target vehicle according to a base image model corresponding to the target vehicle. According to certain aspects, the method and system receive a set of images from an electronic device and analyze the set of images to identify the target vehicle and a set of information associated with the target vehicle. The method and system may determine that the set of images does not meet a threshold criteria for further image analysis using a base image model corresponding to the target vehicle. In response, the method and system may generate and transmit a notification to the electronic device indicating that the set of images does not meet the threshold criteria.","1. A computer-implemented method in a server device of analyzing images, the method comprising: receiving an initial set of images from an electronic device via a network connection; analyzing at least a portion of the initial set of images to identify (i) a target vehicle that is depicted in the at least the portion of the initial set of images, and (ii) a set of information associated with the target vehicle; accessing a particular base image model corresponding to the target vehicle; determining, based at least in part on the set of information associated with the target vehicle, that the initial set of images does not collectively meet a threshold criteria corresponding to a completeness, among an entirety of the initial set of images, of a plurality of types of image perspectives required for the particular base image model for further image analysis using the particular base image model, wherein different pluralities of types of image perspectives are required for different base image models; in response to determining that the initial set of images does not collectively meet the threshold criteria, generating a notification indicating that the entirety of the initial set of images does not collectively meet the threshold criteria, and transmitting the notification to the electronic device via the network connection; receiving, via the network connection subsequent to the transmitted notification, an additional set of images of the target vehicle; and performing the further image analysis, using the particular base image model, on a combination of at least a part of the initial set of images and at least a part of the additional set of images to thereby detect damage to the target vehicle.2. The computer-implemented method of claim 1, further comprising: analyzing at least some of the additional set of images to identify an additional set of information associated with the target vehicle; and determining, based at least in part on the additional set of information associated with the target vehicle, that the combination of the at least the part of the initial set of images and the at least the part of the additional set of images collectively meets the threshold criteria for the further image analysis using the particular base image model.3. The computer-implemented method of claim 1, wherein: analyzing the at least the portion of the initial set of images to identify the set of information associated with the target the vehicle comprises analyzing the at least the portion of the initial set of images to identify a set of perspective views of the target vehicle depicted in the at least the portion of the initial set of images; and determining that the initial set of images does not collectively meet the threshold criteria for the further image analysis using the particular base image model comprises determining that a set of types of perspective views of the target vehicle included in the set of perspective views of the target vehicle excludes one or more types of image perspectives required for the particular base image model corresponding to the target vehicle.4. The computer-implemented method of claim 1, further comprising accessing telematics data associated with the target vehicle, the telematics data indicating damage to a portion of the target vehicle; and wherein: analyzing the at least the portion of the initial set of images comprises analyzing the at least the portion of the initial set of images to determine that the damage to the portion of the target vehicle is not depicted in the at least the portion of the initial set of images; and generating the notification comprises generating a notification to capture one or more images depicting the damage to the portion of the target vehicle.5. The computer-implemented method of claim 1, further comprising: accessing telematics data associated with the target vehicle, wherein the telematics data indicates a first notice of loss (FNOL) event associated with the target vehicle has occurred; and in response to accessing the telematics data, requesting the electronic device to transmit the initial set of images.6. The computer-implemented method of claim 1, wherein: analyzing the at least the portion of the initial set of images comprises analyzing the at least the portion of the initial set of images to identify a set of components of the target vehicle depicted in the at least the portion of the initial set of images; and determining that the initial set of images does not collectively meet the threshold criteria for the further image analysis using the particular base image model comprises determining that the set of components does not match a set of base vehicle components associated with the particular base image model.7. The computer-implemented method of claim 1, wherein determining that the initial set of images does not collectively meet the threshold criteria for the further image analysis using the particular base image model comprises at least one of: determining that the initial set of images is unable to be aligned with the particular base image model; or analyzing the initial set of images according to t the particular base image model, wherein the analyzing is indeterminate.8. The computer-implemented method of claim 1, wherein the particular base image model is three-dimensional, and wherein at least some of the images included in the initial set of images are two-dimensional.9. The computer-implemented method of claim 1, wherein performing the further image analysis, using the particular base image model, on the combination of the at least the part of the initial set of images and the at least the part of the additional set of images to thereby detect the damage to the target vehicle comprises aligning the at least the part of initial set of images and the at least the part of the additional set of images with the particular base image model, and applying one or more convolutional neural networks to one or more portions of the aligned images of the target vehicle to thereby detect the damage to the target vehicle.10. The computer-implemented method of claim 1, wherein the method further comprises determining the completeness of the entirety of the initial set of images; and the completeness of the entirety of the initial set of images further corresponds to at least one of: an amount of image perspectives of the target vehicle, an amount of identified vehicle components, or one or more types of identified vehicle components.11. A system for analyzing images, comprising: a communication module configured to communicate with an electronic device via a network connection; a memory storing a set of base images models corresponding to a set of vehicles; and a processor interfaced with the communication module and the memory, and configured to execute a set of instructions to cause the processor to: receive an initial set of images from an electronic device via the communication module; analyze at least a portion of the initial set of images to identify (i) a target vehicle that is depicted in the at least the portion of the initial set of images, and (ii) a set of information associated with the target vehicle; access, from the memory, a particular base image model corresponding to the target vehicle; determine, based at least in part on the set of information associated with the target vehicle, that the initial set of images does not collectively meet a threshold criteria corresponding to a completeness, among the entirety of the initial set of images, of a plurality of types of image perspective views required for the particular base image model for further image analysis using the particular base image model, wherein different pluralities of types of image perspectives are required for different base image models; in response to determining that the initial set of images does not collectively meet the threshold criteria, generate a notification indicating that the entirety of the initial set of images does not collectively meet the threshold criteria, and transmit the notification to the electronic device via the communication module; receive, via the communication module subsequent to the transmitted notification, an additional set of images of the target vehicle; and perform the further image analysis, using the particular base image model, on a combination of at least a part of the initial set of images and at least a part of the additional set of images to thereby detect damage to the target vehicle.12. The system of claim 11, wherein the processor is further configured to: analyze at least some of the additional set of images to identify an additional set of information associated with the target vehicle; and determine, based at least in part on the additional set of information associated with the target vehicle, that the combination of the at least the part of the initial set of images and the at least the part of the additional set of images collectively meets the threshold criteria for the further image analysis using the particular base image model.13. The system of claim 11, wherein: the set of information associated with the target vehicle includes a set of perspective views of the target vehicle depicted in at least the portion of the initial set of images; and the determination that the initial set of images does not collectively meet the threshold criteria for the further image analysis using the particular base image model includes a determination that a set of types of perspective views of the target vehicle included in the set of perspective views of the target vehicle excludes one or more types of image perspectives required for the particular base image model corresponding to the target vehicle.14. The system of claim 11, wherein: the processor is further configured to access telematics data associated with the target vehicle, the telematics data indicating damage to a portion of the target vehicle; the analysis of the at least the portion of the initial set of images includes an analysis of the at least the portion of the initial set of images to determine that the damage to the portion of the target vehicle is not depicted in the at least the portion of the initial set of images; and the generated notification includes an indication that a depiction of the damage to the portion of the target vehicle is not included in the initial set of images.15. The system of claim 11, wherein the processor is further configured to: access telematics data associated with the target vehicle, wherein the telematics data indicates that a first notice of loss (FNOL) event associated with the target vehicle has occurred, and in response to accessing the telematics data, request the electronic device to transmit the initial set of images.16. The system of claim 11, wherein: the analysis of the at least the portion of the initial set of images includes an analysis of the at least the portion of the initial set of images to identify a set of components of the target vehicle depicted in the at least the portion of the initial set of images; and the determination that the initial set of images does not collectively meet the threshold criteria for the further image analysis using the particular base image model includes a determination that the set of components does not match a set of base vehicle components associated with the particular base image model.17. The system of claim 11, wherein the determination that the initial set of images does not collectively meet the threshold criteria for the further image analysis using the particular base image model includes at least one of: a determination that the initial set of images is unable to be aligned with the particular base image model, or a determination that an analysis of the initial set of images according to the particular base image model is indeterminate.18. The system of claim 11, wherein the particular base image model is three-dimensional, and wherein at least some of the images included in the initial set of images are two-dimensional.19. The system of claim 11, wherein the performance of the further image analysis, using the particular base image model, on the combination of the at least the part of the initial set of images and the at least the part of the additional set of images to thereby detect the damage to the target vehicle comprises an alignment of the at least the part of the initial set of images and the at least the part of the additional set of images the particular base image model, and an application of one or more convolutional neural networks to one or more portions of the aligned images of the target vehicle to thereby detect the damage to the target vehicle.20. The system of claim 11, wherein the set of instructions, when executed by the processor, is further configured to determine the completeness among the entirety of the initial set of images; and the completeness of the entirety of the initial set of images further corresponds to at least one of: an amount of image perspectives of the target vehicle, an amount of identified vehicle components, or one or more types of identified vehicle components.",US10740891_B1.txt,"G06Q40/08,G06T7/00",{'computing; calculating; counting'},"['data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for', 'image data processing or generation, in general']","technology for analyzing images depicting vehicles according to base image models A method and system for analyzing images of a target vehicle according to a base image model corresponding to the target vehicle. According to certain aspects, the method and system receive a set of images from an electronic device and analyze the set of images to identify the target vehicle and a set of information associated with the target vehicle. The method and system may determine that the set of images does not meet a threshold criteria for further image analysis using a base image model corresponding to the target vehicle. In response, the method and system may generate and transmit a notification to the electronic device indicating that the set of images does not meet the threshold criteria.",computing; calculating; counting
US10223842_B1,2017-11-28,2019-03-05,2017-10-30,HYUNDAI MOTOR COMPANYKIA MOTORS CORPORATION,"LEE, DONG CHULJUNG, IN SOO",65495935,speed & trajectory,system for controlling remotely connected vehicle,"Disclosed is a system for controlling a vehicle using a remote artificial intelligence (AI) server. A vehicle communicates with an artificial intelligence server for noise, vibration and harshness (NVH) issue diagnosis. The vehicle controls its fuel combustion condition for improving NVH based on an NVH diagnosis result using the AI.","1. A vehicle control total management system comprising: a noise sensor configured to generate a noise data signal based on measured noise; a vibration sensor configured to generate a vibration data signal based on measured noise; and a diagnosis unit configured to analyze the noise data signal, the vibration data signal, and driving condition data by using artificial intelligence (AI)-learning, to diagnose a noise vibration harshness (NVH) problem of the vehicle and to control a combustion condition of the vehicle for addressing the NVH problem.2. The vehicle control total management system of claim 1, wherein: the diagnosis unit is configured to monitor the noise data signal, the vibration data signal, and a combustion pressure measurement result after controlling the combustion condition, and to control the combustion condition based on the monitored result, and AI-learns data based on the combustion condition control and the monitored result to construct AI for determining the combustion condition.3. The vehicle control total management system of claim 2, wherein: the diagnosis unit is further configured to transmit information on the noise data signal, the vibration data signal, the combustion pressure measurement result, and the combustion condition control to a central artificial intelligence server during the AI learning.4. The vehicle control total management system of claim 1, wherein: the diagnosis unit is further configured to constrict an artificial intelligence neural network for the NVH diagnosis by learning the noise data signal, the vibration data signal, and the driving condition data by a deep learning scheme and is further configured to perform the NVH diagnosis by the artificial intelligence neural network.5. The vehicle control total management system of claim 4, wherein: the diagnosis unit is further configured to covert the noise data signal and the vibration data signal into image data through digital signal processing and performs at least one of a time domain image analysis algorithm and a frequency domain image analysis algorithm by using a gabor filter with respect to the image data.6. The vehicle control total management system of claim 4, wherein: the diagnosis unit is further configured to apply a deep neural network (DNN) learning machine technique or a convolution neural network (CNN) learning machine technique to the noise data signal and the vibration data signal.7. The vehicle control total management system of claim 1, further comprising: a signal processing controller configured to control an engine, a transmission, and the like of a vehicle, wherein the diagnosis unit is configured to transmit an instruction for the combustion condition to the signal processing controller.8. The vehicle control total management system of claim 1, wherein: when the NVH diagnosis result is an abnormal state, reservation information in which a maintenance service is providable is received based on a GPS position of the vehicle, and a navigation interlocks with the reservation information.9. The vehicle control total management system of claim 1, wherein: classification categories according to the NVH diagnosis result include at least one selected from the group consisting of transmission noise, gasoline knocking, piston noise, oil pump noise, high pressure pump noise, vacuum pump noise, timing chain noise, timing belt noise, valve system noise, turbocharger noise, injector noise, crank system noise, fuel pulsation noise, alternator noise, auxiliary machinery belt noise, driveline noise, intake/exhaust system noise, water pump noise, power train related noise, and power train false noise.10. The vehicle control total management system of claim 1, wherein: parameters for the NVH diagnosis include whether the parameter is an engine state which is noise and vibration data measured in the engine room based on a position where noise and vibration are measured or whether the parameter is a vehicle state which is noise and vibration data measured in a vehicle interior, whether a temperature condition in which noise and vibration are measured is a cold or hot condition, and whether the parameter is an idle condition in a stopped state, an acceleration driving mode in a driving condition which is a driving state, a deceleration driving mode, or a constant speed driving mode.11. A central artificial intelligence server receiving NVH diagnosis results for multiple vehicles, comprising: a central diagnosis unit configured to classify the NVH diagnosis results received from the multiple vehicles for each vehicle type; and a database configured to store the classified NVH diagnosis results, wherein the central diagnosis unit configured to update parameter values for artificial intelligence of the multiple vehicles by learning data stored in the database.12. The central artificial intelligence server of claim 11, wherein: the central diagnosis unit is configured to determine and update parameter values for an artificial intelligence neural network by learning the data of the database by deep learning, and wherein the server is further configured to transmit the updated parameter values to a vehicle.13. The central artificial intelligence server of claim 11, wherein: an abnormality result is notified to a driver of a vehicle in which the diagnosis result is abnormal among the multiple vehicles and information on the abnormal vehicle is transmitted to a service network in order to provide a maintenance service.14. A vehicle control total management system comprising: a driving pattern database configured to store data on a driving pattern and a shift pattern for each road condition and data on an acceleration pedal usage pattern of the driver for each traffic situation; and a tone control unit configured to construct artificial intelligence by performing AI learning by inputting the data of the driving pattern database, recognizing the driving pattern based on a road type and real-time traffic information received by using the artificial intelligence, and configured to set a target tone according to the recognized driving pattern.15. The vehicle control total management system of claim 14, wherein: the tone control unit is c an artificial intelligence neural network by deep-learning the data of the database and recognizes the driving pattern based on the road type and the real-time traffic information by using the constructed artificial intelligence neural network.16. The vehicle control total management system of claim 14, wherein: the tone control unit is configured: to set the target tone so as to emphasize a quiet tone when the recognized driving pattern is a country road, to set the target tone so as to emphasize a grand tone when the recognized driving pattern is a tunnel, and to set the target tone so as to emphasize a sporty tone when the recognized driving pattern is a highway.17. The vehicle control total management system of claim 14, wherein: the tone control unit is configured to determine an engine order component for the target tone and controls a speaker tone of a vehicle to match the target tone by using the extracted engine order component.18. The vehicle control total management system of claim 17, wherein: the tone control unit is configured to determine a grade and a level of the engine order component generated by vibration of an engine of the vehicle, and the engine order component is a physical phenomenon occurring in a rotating engine.19. The vehicle control total management system of claim 18, wherein the tone control unit is configured to select and arrange an order influencing the target tone among the extracted engine vibration order components, wherein the tone control unit is configured to determine an amplification level of the arranged order component, to determine a characteristic of the engine tone, and to amplify an output amplitude of the order component through signal processing.20. The vehicle control total management system of claim 17, wherein when the target tone emphasizes the grand tone, tone control in which the engine order component in a low-frequency band is emphasized is performed and when the target tone emphasizes the sporty tone, tone control in which an engine integer order component is emphasized is performed.",US10223842_B1.txt,"B60K28/00,B60W40/09,G07C5/00,G07C5/08,G10K15/02,H04L29/08","{'electric communication technique', 'musical instruments; acoustics', 'checking-devices', 'vehicles in general'}","['arrangement or mounting of propulsion units or of transmissions in vehicles; arrangement or mounting of plural diverse prime-movers in vehicles; auxiliary drives for vehicles; instrumentation or dashboards for vehicles; arrangements in connection with cooling, air intake, gas exhaust or fuel supply of propulsion units in vehicles', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'time or attendance registers; registering or indicating the working of machines; generating random numbers; voting or lottery apparatus; arrangements, systems or apparatus for checking not provided for elsewhere', 'time or attendance registers; registering or indicating the working of machines; generating random numbers; voting or lottery apparatus; arrangements, systems or apparatus for checking not provided for elsewhere', 'sound-producing devices (sound-producing toys a63h5/00); methods or devices for protecting against, or for damping, noise or other acoustic waves in general; acoustics not otherwise provided for', 'transmission of digital information, e.g. telegraphic communication ({coding or ciphering apparatus for cryptographic or other purposes involving the need for secrecy g09c;} arrangements common to telegraphic and telephonic communication h04m)']","system for controlling remotely connected vehicle Disclosed is a system for controlling a vehicle using a remote artificial intelligence (AI) server. A vehicle communicates with an artificial intelligence server for noise, vibration and harshness (NVH) issue diagnosis. The vehicle controls its fuel combustion condition for improving NVH based on an NVH diagnosis result using the AI.",electric communication technique musical instruments; acoustics checking-devices vehicles in general
US2020216026_A1,2019-01-09,2020-07-09,2019-01-09,CAPITAL ONE SERVICES,"TANG, QIAOCHUWYLIE, STEPHENPRICE, MICAH",71404861,survillance,detecting an event and automatically obtaining video data,"A server device obtains, from a primary device associated with a first vehicle, event data concerning an event, video data concerning the event, and/or location data concerning a location of the first vehicle. The server device processes the event data to determine a type of the event, and determines, based on the type of the event and the location data, a proximity zone around the location of the first vehicle. The server device determines additional devices within the proximity zone, where each additional device is associated with a different vehicle. The server device sends, to the additional devices, a message requesting additional video data concerning the event, and obtains the additional video data from at least one additional device. The server device performs, based on relevant event information based on the event data, the video data, and the additional video data, an action concerning the event.","1. A method, comprising: obtaining, by a primary device and from a detecting device, event data concerning an event, wherein the primary device and the detecting device are associated with a first vehicle, and wherein the detecting device is associated with a sound level meter and the event data includes sound level data concerning the event; determining, by the primary device and based on the event data, a type of the event, the type of the event including at least one of: a broken window event type, an unauthorized entry event type, a physical damage event type, a vandalism event type, or a personal harm event type, and the type of the event being determined using a machine learning model, the machine learning model being trained with historical event data associated with unauthorized entry event types, physical damage event types, vandalism event types, and personal harm event types; obtaining, by the primary device and from a camera device, and based on the type of the event, video data concerning the event, wherein the camera device is associated with the first vehicle; obtaining, by the primary device and from a location device, and based on the type of the event, first location data concerning a location of the first vehicle, wherein the location device is associated with the first vehicle; determining, by the primary device, a proximity zone around the first vehicle, wherein a size of the proximity zone is selectively determined based on the type of the event, a first size of the proximity zone being selectively determined based upon a first type of event, or a second size of the proximity zone being selectively determined based upon a second, different type of event; sending, by the primary device, a location query to a plurality of additional devices; receiving, by the primary device and from a device of the plurality of additional devices, second location data associated with a second vehicle, the device being associated with the second vehicle; determining, by the primary device, that the second vehicle is within the proximity zone; sending, by the primary device and based on the type of the event, a message to the device associated with the second vehicle, wherein the message requests the device to: obtain at least one of: additional event data concerning the event, additional video data concerning the event, or additional location data concerning a location of the second vehicle, and send the at least one of the additional event data, the additional video data, or the additional location data concerning the location of the second vehicle to a server device; sending, by the primary device, the type of the event, the video data, and the first location data concerning the location of the first vehicle to the server device to cause the server device to process the event data, the video data, the first location data concerning the location of the first vehicle, and the at least one of the additional event data, the additional video data, or the additional location data concerning the location of the second vehicle to determine relevant event information; obtaining, by the primary device and from the server device, the relevant event information; causing, by the primary device, storage of the relevant event information wherein the relevant event information includes a first portion of the video data and a second portion of the additional video data; and causing, by the primary device, concurrent display of the first portion of the video data and the second portion of the additional video data.2. The method of claim 1, wherein the detecting device is a microphone and the event data includes audio data concerning the event.3. (canceled)4. (canceled)5. (canceled)6. The method of claim 1, wherein obtaining the video data concerning the event from the camera device comprises: determining, based on the event data, a start time of the event; collecting the video data from the camera device relating to a first interval of time before the start time of the event; collecting the video data from the camera device relating to a second interval of time after the start time of the event; and joining the video data relating to the first interval of time and the video data relating to the second interval of time.7. (canceled)8. (canceled)9. The method of claim 1, further comprising: determining a correspondence between the first portion of the video data and the second portion of the additional video data; synchronizing, based on the correspondence, the first portion of the video data and the second portion of the additional video data; and wherein causing concurrent display of the first portion of the video data and the second portion of the additional video data comprises: causing concurrent display of the first portion of the video data and the second portion of the additional video data based on synchronizing the first portion of the video data and the second portion of the additional video data.10. A server device, comprising: one or more memories; and one or more processors, communicatively coupled to the one or more memories, configured to: obtain event data concerning an event from a primary device associated with a first vehicle, wherein the primary device is associated with a sound level meter and the event data includes sound level data concerning the event; obtain video data concerning the event from the primary device associated with the first vehicle; obtain first location data concerning a location of the first vehicle from the primary device associated with the first vehicle; process the event data to determine a type of the event, the type of the event including at least one of: a broken window event type, an unauthorized entry event type, a physical damage event type, a vandalism event type, or a personal harm event type, and the type of the event being determined using a machine learning model, the machine learning model being trained with historical event data associated with unauthorized entry event types, physical damage event types, vandalism event types, and personal harm event types; determine, based on the type of the event and the first location data concerning the location of the first vehicle, a proximity zone around the location of the first vehicle, wherein a size of the proximity zone is selectively determined based on the type of the event, a first size of the proximity zone being selectively determined based upon a first type of the event, or a second size of the proximity zone being selectively determined based upon a second, different type of the event; send a location query to a plurality of additional devices; receive, from a device of the plurality of additional devices, second location data associated with a second vehicle, the device being associated with the second vehicle; determine that the second vehicle is within the proximity zone; send, to the device associated with the second vehicle, a message requesting additional video data concerning the event; obtain the additional video data concerning the event from the device associated with the second vehicle; process the event data, the video data, and the additional video data to determine relevant event information; and perform, based on the relevant event information, an action concerning the event.11. The server device of claim 10, wherein the one or more processors, when performing the action concerning the event, are configured to: initiate a phone call to an emergency services provider; and provide the relevant event information to the emergency services provider.12. The server device of claim 10, wherein the one or more processors, when performing the action concerning the event, are configured to: generate an event report based on the relevant event information; and send the event report to a client device.13. The server device of claim 10, wherein the one or more processors, when processing the event data to determine the type of the event using the machine learning model, are configured to: process the event data using an event classification algorithm; wherein the event classification algorithm is a machine learning algorithm.14. The server device of claim 10, wherein the one or more processors, when determining that the second vehicle is within the proximity zone, are configured to: obtain additional location data concerning the second vehicle; and compare the first location data concerning the location of the first vehicle and the additional location data concerning the second vehicle to determine that the second vehicle is within the proximity zone.15. The server device of claim 10, wherein the one or more processors, when processing the event data, the video data, and the additional video data to determine the relevant event information, are configured to: process the video data and the additional video data to identify one or more objects; process, based on identifying the one or more objects, the video data and the additional video data to determine motion of the one or more objects; and determine that the motion of the one or more objects caused the event.16. A non-transitory computer-readable medium storing instructions, the instructions comprising: one or more instructions that, when executed by one or more processors of a device associated with a vehicle, cause the one or more processors to: receive a first message from a primary device associated with a primary vehicle via a wireless communication protocol, wherein the primary device is associated with a sound level meter and the first message includes sound level data concerning an event; obtain, based on the first message, location data concerning a location of the vehicle from a location device associated with the vehicle; determine, based on the first message and the location data concerning the location of the vehicle, that the location of the vehicle is within a proximity zone of the primary vehicle, the proximity zone being selectively determined based on a type of event, the type of event including at least one of: a broken window event type, an unauthorized entry event type, a physical damage event type, a vandalism event type, or a personal harm event type, the type of event being determined using a machine learning model the machine learning model being trained with historical event data associated with unauthorized entry event types, physical damage event types, vandalism event types, and personal harm event types, a first size of the proximity zone being selectively determined based upon a first type of event, or a second size of the proximity zone being selectively determined based upon a second, different type of event; cause, based on determining that the location of the vehicle is within the proximity zone of the primary vehicle, a camera device associated with the vehicle to capture video data; send, to the primary device and via the wireless communication protocol, a second message that indicates that the location of the vehicle is within the proximity zone; receive, from the primary device and via the wireless communication protocol, a third message that indicates the type of event and an interval of time; obtain, based on the interval of time and from the camera device, the video data during the interval of time; and send the video data to a server device to cause the server device to perform an action concerning the type of event.17. The non-transitory computer-readable medium of claim 16, wherein the wireless communication protocol is a Bluetooth wireless communication protocol that supports mesh networking.18. The non-transitory computer-readable medium of claim 16, wherein the one or more instructions, that cause the one or more processors to cause the camera device associated with the vehicle to capture the video data, cause the one or more processors to: cause one or more cameras associated with the camera device to point toward the primary vehicle.19. The non-transitory computer-readable medium of claim 16, wherein the first message indicates a location of the primary vehicle and the proximity zone of the primary vehicle, wherein the one or more instructions, when executed by the one or more processors, that cause the one or more processors to determine that the location of the vehicle is within the proximity zone of the primary vehicle, cause the one or more processors to: determine, based on the first message and the location data concerning the location of the vehicle, a distance between the location of the vehicle and the location of the primary vehicle; and determine that the distance satisfies a threshold associated with the proximity zone.20. The non-transitory computer-readable medium of claim 16, wherein the one or more instructions further cause the one or more processors to: send, to the primary device and via the wireless communications protocol, a fourth message that indicates that the video data has been sent to the server device.21. The method of claim 1, wherein the proximity zone includes at least one of: a geographic area around the first vehicle, or a wireless communication range of the primary device associated with the first vehicle.22. The method of claim 1, further comprising: causing an alarm to go off on the first vehicle based on the relevant event information.23. The device of claim 10, wherein the one or more processors are further to: cause an alarm to go off on the first vehicle based on the relevant event information.24. The method of claim 1, further comprising: controlling the camera device to pivot on a pivot point to obtain the video data concerning the event.25. The method of claim 1, further comprising: controlling the camera device to adjust a focus to obtain the video data concerning the event.",US2020216026_A1.txt,"B60R11/04,B60R21/013,B60R25/102,B60R25/30,B60R25/31,B60R25/33,G06F11/07,G07C5/00,G07C9/00,G08B13/196,G08B25/00,G08B31/00,H04N21/472,H04N21/61,H04N5/232,H04N5/247,H04N7/18","{'electric communication technique', 'signalling', 'computing; calculating; counting', 'checking-devices', 'vehicles in general'}","['vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'electric digital data processing (computer systems based on specific computational models g06n)', 'time or attendance registers; registering or indicating the working of machines; generating random numbers; voting or lottery apparatus; arrangements, systems or apparatus for checking not provided for elsewhere', 'time or attendance registers; registering or indicating the working of machines; generating random numbers; voting or lottery apparatus; arrangements, systems or apparatus for checking not provided for elsewhere', 'signalling or calling systems; order telegraphs; alarm systems', 'signalling or calling systems; order telegraphs; alarm systems', 'signalling or calling systems; order telegraphs; alarm systems', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television']","detecting an event and automatically obtaining video data A server device obtains, from a primary device associated with a first vehicle, event data concerning an event, video data concerning the event, and/or location data concerning a location of the first vehicle. The server device processes the event data to determine a type of the event, and determines, based on the type of the event and the location data, a proximity zone around the location of the first vehicle. The server device determines additional devices within the proximity zone, where each additional device is associated with a different vehicle. The server device sends, to the additional devices, a message requesting additional video data concerning the event, and obtains the additional video data from at least one additional device. The server device performs, based on relevant event information based on the event data, the video data, and the additional video data, an action concerning the event.",electric communication technique signalling computing; calculating; counting checking-devices vehicles in general
US9165196_B2,2012-11-16,2015-10-20,2012-11-16,INTEL CORPORATION,"LORTZ VICTOR B.KESAVAN, VIJAY SARATHIRATHI, SOMYARANGARAJAN, ANAND P.",50727566,road monitoring,augmenting adas features of a vehicle with image processing support in on-board vehicle platform,"Systems and methods directed to augmenting advanced driver assistance systems (ADAS) features of a vehicle with image processing support in on-board vehicle platform are described herein. Images may be received from one or more image sensors associated with an ADAS of a vehicle. The received images may be processed. An action is determined based upon, at least in part, the processed images. A message is transmitted to an ADAS controller responsive to the determination.","1. A computer-implemented method comprising: receiving, by one or more processors of a vehicle on-board platform, images from one or more image sensors associated with an advanced driver assistance system (ADAS) of a vehicle; processing, by the one or more processors of the vehicle on-board platform, the received images to generate processed images; identifying a triggering event based on the processed images; determining, by the one or more processors of the vehicle on-board platform, an action based upon, at least in part, the processed images; and transmitting, by the one or more processors of the vehicle on-board platform, a message comprising the images from the one or more images sensors to an in-vehicle infotainment (IVI) system in response to the triggering event.2. The computer-implemented method of claim 1, wherein the images from the one or more image sensors associated with the ADAS are received from an ADAS image system on chip (SoC).3. The computer-implemented method of claim 1, wherein the images are received via a USB connection, a Bluetooth connection, a fiber optic connection, an Ethernet connection, a Wi-Fi connection, a cellular data connection, or a Media Oriented Systems Transport Bus.4. The computer-implemented method of claim 1, wherein processing the received images further comprises processing the received images based upon, at least in part, data from at least one of an Internet connection, one or more databases, a navigation system, or a context engine.5. The computer-implemented method of claim 1, wherein processing the received images further comprises at least one of zooming, cropping, image projection, linear filtering, pixelation, hidden Markov model (HMM), scene analysis, feature extraction, pattern recognition, or heuristic image analysis.6. The computer-implemented method of claim 1, wherein the received images further comprise additional data, wherein the additional data further includes at least one of a timestamp of when an image of the received images was shot or optical data of at least one of the one or more image sensors.7. The computer-implemented method of claim 1, further comprising generating a database of image features based upon, at least in part, machine learning techniques to classify images received from the one or more image sensors.8. A computer program product residing on a non-transitory computer readable medium comprising a plurality of instructions stored thereon, wherein the plurality of instructions are executed by a processor, causing the processor to perform operations comprising: receiving data captured by one or more vehicle sensors associated with an advanced driver assistance system (ADAS) of the vehicle; analyzing the received data from the one or more vehicle sensors to generate analyzed data; identifying a triggering event based on the analyzed data; and publishing the analyzed data to an in-vehicle infotainment (IVI) system of the vehicle in response to the triggering event.9. The computer program product of claim 8, wherein the data from the one or more vehicle sensors is received from an ADAS image system on chip (SoC).10. The computer program product of claim 8, wherein the data from the one or more vehicle sensors is received over a USB connection, a Bluetooth connection, a fiber optic connection, an Ethernet connection, a Wi-Fi connection, a cellular data connection, or a Media Oriented Systems Transport Bus.11. The computer program product of claim 8, further comprising processing the received data.12. The computer program product of claim 11, wherein processing the received data further comprising processing the received data based upon, at least in part, data received from at least one of an Internet connection of the vehicle, one or more databases, a navigation system, or a context engine.13. The computer program product of claim 8, further comprising generating a database based upon, at least in part, machine learning techniques to classify data received from the one or more vehicle sensors.14. The computer program product of claim 8, wherein the one or more vehicle sensors further comprise at least one of an ADAS camera, a microphone, or a GPS system.15. A computing system including a processor and memory configured to perform operations comprising: receiving a plurality of images taken by one or more cameras of an advanced driver assistance system (ADAS) of a vehicle; analyzing the plurality of images received from the ADAS of the vehicle to generate analyzed images; identifying a triggering event based on the analyzed images; identifying an action based upon, at least in part, the plurality of analyzed images; and transmitting the analyzed images to an in-vehicle infotainment (IVI) system in response to the triggering event.16. The computer system of claim 15, wherein the plurality of images are received from an ADAS image system on chip (SoC).17. The computer system of claim 15, further comprising transmitting the plurality of images via a USB connection, a Bluetooth connection, a fiber optic connection, an Ethernet connection, a Wi-Fi connection, a cellular data connection, or Media Oriented Systems Transport Bus.18. The computer system of claim 15, further comprising augmenting a database by classifying images received from the one or more cameras of the ADAS.19. The computer system of claim 15, wherein the plurality of images further comprise additional data, wherein the additional data includes at least one of a timestamp of when one of the plurality of images was created, focal length of the camera that took the image of the plurality of images, resolution of the image of the plurality of images, or identification of the type of camera used to take the image of the plurality of images.20. The computer system of claim 15, further comprising processing the plurality of the received images, wherein processing further comprises at least one of basic image processing, complex signal processing, scene analysis, feature extraction, pattern recognition, or heuristic analysis.21. The computer system of claim 15, further comprising receiving additional data from at least one of an Internet connection, one or more databases, a navigation system, and a context engine; and analyzing the plurality of received images based upon, at least in part, the additional data.",US9165196_B2.txt,"B60W50/00,G06K9/00","{'computing; calculating; counting', 'vehicles in general'}","['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']","augmenting adas features of a vehicle with image processing support in on-board vehicle platform Systems and methods directed to augmenting advanced driver assistance systems (ADAS) features of a vehicle with image processing support in on-board vehicle platform are described herein. Images may be received from one or more image sensors associated with an ADAS of a vehicle. The received images may be processed. An action is determined based upon, at least in part, the processed images. A message is transmitted to an ADAS controller responsive to the determination.",computing; calculating; counting vehicles in general
CN111209923_A,2020-04-23,2020-05-29,2020-04-23,BEIJING HUIZHI DATA TECHNOLOGY COMPANY,ZHANG JINGLEQI CONGYA,70785374,vehicle classification,method for identifying whether muck truck is covered based on deep learning technology,"The invention provides a method for identifying whether a muck truck is covered or not based on a deep learning technology. Through the deep learning technology, the problem of identifying whether themuck truck is covered or not is solved, real-time auditing is achieved, the working efficiency is improved, the generalization ability of the algorithm is improved through the secondary optimizationprocess, and the purpose of improving the identification accuracy of the algorithm in different application scenes is achieved. The beneficial effects of the invention are that the method can providebetter local features through employing the convolutional neural network, and well solves problems that the recognition difficulty is large and the classification is not easy because of the shooting angle, complex background and similar shapes. The method provided by the invention has relatively high detection accuracy in classification of whether the cover is closed or not, solves the problems oflow efficiency and result hysteresis of manual checking of illegal behaviors of the muck truck, and has important significance for establishment of an intelligent illegal system.","1. a method for identifying whether a muck truck is covered or not based on deep learning technology is characterized by comprising the following steps: s1, extracting the front shot picture of the vehicle from the road traffic electronic monitoring equipment; s2, inputting the picture into a vehicle detection model, and detecting the position of the vehicle in the picture through the vehicle detection model; s3, inputting the detected vehicle position into a muck vehicle classifier, wherein the muck vehicle classifier comprises three types of covered muck vehicles, uncovered muck and other trucks; s4, recognizing vehicle classification through the muck vehicle classifier, outputting a recognition result, and outputting early warning information if the recognition result is that the muck vehicle is not covered; if the result is that the cover is covered with the muck truck or other trucks, no early warning information is output, and the identification is finished; and s5, accumulating data, and performing secondary optimization on the muck car classifier to improve the classification accuracy. 2. the deep learning technology-based muck vehicle cover or not identification method according to claim 1, characterized in that: the vehicle detection model in step s2 is used to detect all vehicles appearing in the image, and is implemented by extracting the image features to be identified layer by layer and step by using a convolution kernel, downsampling and pooling method, and performing classification processing on the final features at the network output end. 3. the deep learning technology-based muck vehicle cover or not identification method according to claim 2, characterized in that: the detection process of the vehicle detection model comprises the following steps: s2-1 image preprocessing: labeling the sample image, and dividing the sample image into a training set and a testing set according to the proportion; carrying out geometric transformation of translation, transposition, mirroring, rotation and scaling on the collected sample image, and correcting a system error of an automatic shooting system of the bayonet camera and a random error caused by the position of an instrument; wherein the random error caused by the position of the instrument comprises an imaging angle, a perspective relation and even an error caused by the self reason of a lens; s2-2, target vehicle detection: adding a convolutional layer based on a resnet-50 network, wherein the size of the convolutional layer is gradually reduced; detecting an object by using the multi-scale features; and obtaining a candidate frame through convolution operation, then carrying out classification and regression, and finally obtaining a detection result. 4. the deep learning technology-based muck vehicle cover or not identification method according to claim 1, characterized in that: the process of identifying the vehicle classification by the muck vehicle classifier in the step s4 includes the steps of: s4-1, performing image enhancement on the vehicle position detected in the step s2, improving the image quality, enriching the information content, enhancing the image interpretation and identification effect, and achieving the purpose of meeting the special analysis requirement; the image enhancement mainly refers to improving the visual effect of an image, increasing the diversity of a sample, purposefully emphasizing the overall or local characteristics of the image, turning an unclear image into clear or emphasizing certain characteristics, and enlarging the difference between different object characteristics in the image; s4-2, decomposing the convolution by adopting an inclusion v3 neural network framework, and reducing the parameter number by using 1x3 convolution and 3x1 convolution under the condition of the same receptive field; s4-3, introducing a label smoothing rule, and averagely distributing the confidence degrees of the labels to labels of other categories; and s4-4, performing probabilistic output by using a softmax function, and performing loss calculation on the output result by using cross entropy cross-encopy to enable the recognition result to be closer to the real distribution. 5. the deep learning technology-based muck vehicle cover or not identification method according to claim 1, characterized in that: the secondary optimization process in step s5 includes the following steps: s5-1, recognizing the input picture, re-marking the wrong picture in the recognition result, and storing the wrong picture as training data; s5-2, inputting the collected training data into the muck car classifier, and optimizing the muck car classifier; and s5-3, updating the classification device of the muck car to achieve a better recognition result.",CN111209923_A.txt,"G06K9/46,G06K9/62",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']","method for identifying whether muck truck is covered based on deep learning technology The invention provides a method for identifying whether a muck truck is covered or not based on a deep learning technology. Through the deep learning technology, the problem of identifying whether themuck truck is covered or not is solved, real-time auditing is achieved, the working efficiency is improved, the generalization ability of the algorithm is improved through the secondary optimizationprocess, and the purpose of improving the identification accuracy of the algorithm in different application scenes is achieved. The beneficial effects of the invention are that the method can providebetter local features through employing the convolutional neural network, and well solves problems that the recognition difficulty is large and the classification is not easy because of the shooting angle, complex background and similar shapes. The method provided by the invention has relatively high detection accuracy in classification of whether the cover is closed or not, solves the problems oflow efficiency and result hysteresis of manual checking of illegal behaviors of the muck truck, and has important significance for establishment of an intelligent illegal system.",computing; calculating; counting
CN108446645_A,2018-03-26,2018-08-24,2018-03-26,TIANJIN UNIVERSITY,PANG YANWEIHE ERLUJI ZHONG,63197048,vehicle classification,vehicle face recognition method based on deep learning,"The present invention provides a vehicle face recognition method based on deep learning. The method comprises the steps of: obtaining an image, and constructing a driver data set; constructing a model: extracting visual features and semantic features, and forming a feature processing model; training the feature processing model; optimizing an experiment result according to a test result, and transmitting the optimized feature processing model to a total control terminal; and installing a warning lamp and a camera on a vehicle, and uploading a driver's operation condition to the total control terminal in real time, wherein the total control terminal determines whether the drive breaks driving rules or not according to the optimized feature processing model, when the drive breaks driving rules, the total control terminal emits signals to excite the warning lamp to prompt the driver to perform legal driving. Through real-time analysis of face change, the vehicle face recognition method extracts face features to detect whether there is violation operation or not and perform analysis and comparison with a data set, once behaviors of violating laws and rules are discovered, such as fatigue driving and call answering, watching of a mobile phone, alarm can be automatically emitted to timely stop the drivers' illegal behaviors so as to reduce the possibility of generation of traffic accidents.","1. A vehicle-mounted face recognition method based on deep learning is characterized by comprising the following steps: 1) acquiring an image and constructing a driver data set; 2) constructing a model, comprising: (1) respectively extracting visual features and semantic features from the driver data set through a convolutional neural network and an LSTM network; (2) inputting the extracted visual features and semantic features into an LSTM network with an attention mechanism to form a feature processing model; 3) training a feature processing model, wherein 60% of images in a driver data set are used for training, 20% of images are used for verification, and 20% of images are used for testing; 4) according to the test result, respectively aligning the parameters WzWrW, fine adjustment is carried out, an experimental result is optimized, and the optimized characteristic processing model is transmitted to a master control end; 5) install warning light and camera on the car, reach total control end in real time with driver's operational aspect, total control end judges whether the driver has the driving of violating the rules according to the feature handling model after optimizing, and when the driving of violating the rules, total control end signals, arouses the warning light, reminds the driver civilized travel. 2. The deep learning-based vehicle-mounted face recognition method according to claim 1, wherein the step 1) comprises the steps of acquiring different driver images through the internet by using a python-based network picture acquisition script, labeling the images, and summarizing the images as a driver data set, wherein the image contents comprise: the driver drives normally, looks down at the mobile phone, watches everywhere, chats and drives tiredly. 3. The deep learning-based vehicle-mounted face recognition method according to claim 1, wherein the step (1) in the step 2) comprises: extracting 14 multiplied by 512 dimensional visual characteristics of the driver data set by using conv5_3 layer of VGG-19 network in convolutional neural network to obtain characteristic vector aiGenerating a visual information context vector z by an attention mechanismvt(ii) a Semantic features of a driver data set are extracted through an LSTM network and a semantic context vector z is obtainedst 4. The deep learning-based vehicle-mounted face recognition method according to claim 1, wherein the step (2) in the step 2) comprises: (1) context vector z of visual informationvtAnd semantic contextVector zstForming a context vector z capable of more fully expressing image information by affine transformationt; (2) The obtained context vector ztInput into LSTM network with attention mechanism, and analyze driver's behavior. 5. The deep learning-based vehicle-mounted face recognition method according to claim 4, wherein the feature processing model comprises the following steps: et=fatt(ai,ht-1) zt=(Wz[ht-1,xt]) rt=(Wr[ht-1,xt]) wherein z isvtRepresenting a context vector of visual information, aiRepresenting visual feature vectors, tRepresents a weight, ztRepresenting a context vector, xtInput representing the current time, htAnd ht-1Respectively representing the hidden layer states at the current time and the previous time,for candidate states of the hidden layer at the current moment, WzWrW isAnd (4) parameters.",CN108446645_A.txt,G06K9/00,{'computing; calculating; counting'},['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers'],"vehicle face recognition method based on deep learning The present invention provides a vehicle face recognition method based on deep learning. The method comprises the steps of: obtaining an image, and constructing a driver data set; constructing a model: extracting visual features and semantic features, and forming a feature processing model; training the feature processing model; optimizing an experiment result according to a test result, and transmitting the optimized feature processing model to a total control terminal; and installing a warning lamp and a camera on a vehicle, and uploading a driver's operation condition to the total control terminal in real time, wherein the total control terminal determines whether the drive breaks driving rules or not according to the optimized feature processing model, when the drive breaks driving rules, the total control terminal emits signals to excite the warning lamp to prompt the driver to perform legal driving. Through real-time analysis of face change, the vehicle face recognition method extracts face features to detect whether there is violation operation or not and perform analysis and comparison with a data set, once behaviors of violating laws and rules are discovered, such as fatigue driving and call answering, watching of a mobile phone, alarm can be automatically emitted to timely stop the drivers' illegal behaviors so as to reduce the possibility of generation of traffic accidents.",computing; calculating; counting
US10475335_B1,2017-11-30,2019-11-12,2017-11-30,STATE FARM MUTUAL AUTOMOBILE INSURANCE COMPANY,"DAHIYA, ANUJDRINKWATER, LEE MICHAELCHAE, PAUL CHANG HOONKOLLI, SAHITI",68466558,speed & trajectory,systems and methods for visualizing telematics data,"Systems and methods are described for the visualization of vehicular-based telematics data. In various aspects, telematics data may be aggregated for a plurality of vehicles where the telematics data can include telematics data observation(s) for each vehicle. Each observation can indicate a coordinate value of the vehicle and a timestamp for the observation, and can further indicate any of a device identifier for a telematics device associated with the vehicle, a speed value of the vehicle, a g-force value of the vehicle, a trip identifier associated with the vehicle, a distance value of the vehicle, or a stop indicator value of the vehicle. A visualization may also be generated based on at least a subset of the telematics data such that the visualization can indicate one or more image features associated with the one or more of the plurality of vehicles.","1. An imaging system configured to visualize vehicular-based telematics data, the imaging system comprising: one or more processors, configured to: aggregate telematics data for a plurality of vehicles, the telematics data including one or more observations for each vehicle, each observation indicating at least a coordinate value of the vehicle and a timestamp for each observation; generate a visualization based on at least a subset of the telematics data, wherein the subset of the telematics data defines a hazardous driving area, and wherein the visualization indicates one or more image features associated with one or more of the plurality of vehicles at the hazardous driving area, the image features determined from the one or more observations from the subset of telematics data; and determine a risk profile for a new vehicle based on the visualization.2. The imaging system of claim 1, wherein each observation further indicates one or more of the following: a device identifier for a telematics device associated with the vehicle, a speed value of the vehicle, a g-force value of the vehicle, a trip identifier associated with the vehicle, a distance value of the vehicle, or a stop indicator value of the vehicle.3. The imaging system of claim 1, wherein the visualization is a cluster-based visualization.4. The imaging system of claim 3, wherein the one or more image features include a stops-per-mile value, a move-time-percentage value, and a city-miles-per-total-miles value.5. The imaging system of claim 3, wherein the cluster-based visualization is a three dimensional cluster-based visualization defining a pattern between at least two image features along two respective axes of the three dimensional cluster-based visualization.6. The imaging system of claim 1, wherein the visualization is an extreme driving visualization, wherein the extreme driving visualization is operable to identify one or more extreme driving events that occurred at one or more corresponding locations.7. The imaging system of claim 6, wherein the extreme driving visualization is transmitted to a municipality associated with the one or more corresponding locations.8. The imaging system of claim 1, wherein the visualization is any one of the following: a choropleth map-based visualization, a heat map visualization, a heat table visualization, or a trip path visualization.9. The imaging system of claim 1, wherein the visualization corresponds to a particular vehicle, the particular vehicle corresponding to one or more drivers associated with the vehicle, and wherein the visualization is transmitted to the one or more drivers.10. The imaging system of claim 1 further configured to determine a risk profile using the visualization, wherein the risk profile corresponds to a particular vehicle, the particular vehicle corresponding to one or more drivers associated with the vehicle.11. A computer-implemented imaging method of visualizing vehicular-based telematics data using one or more processors, the imaging method comprising: aggregating telematics data, using one or more processors, for a plurality of vehicles, the telematics data including one or more observations for each vehicle, each observation indicating at least a coordinate value of the vehicle and a timestamp for each observation; generating a visualization, using one or more processors, based on at least a subset of the telematics data, wherein the subset of the telematics data defines a hazardous driving area, and wherein the visualization indicates one or more image features associated with one or more of the plurality of vehicles, the image features determined from the one or more observations from the subset of telematics data; and determining a risk profile for a new vehicle based on the visualization.12. The imaging method of claim 11, wherein each observation further indicates one or more of the following: a device identifier for a telematics device associated with the vehicle, a speed value of the vehicle, a g-force value of the vehicle, a trip identifier associated with the vehicle, a distance value of the vehicle, or a stop indicator value of the vehicle.13. The imaging method of claim 11, wherein the visualization is a cluster-based visualization.14. The imaging method of claim 13, wherein the one or more image features include a stops-per-mile value, a move-time-percentage value, and a city-miles-per-total-miles value.15. The imaging method of claim 13, wherein the cluster-based visualization is a three dimensional cluster-based visualization defining a pattern between at least two image features along two respective axes of the three dimensional cluster-based visualization.16. The imaging method of claim 11, wherein the visualization is an extreme driving visualization, wherein the extreme driving visualization is operable to identify one or more extreme driving events that occurred at one or more corresponding locations.17. The imaging method of claim 16, wherein the extreme driving visualization is transmitted to a municipality associated with the one or more corresponding locations.18. The imaging method of claim 11, wherein the visualization is any one of the following: a choropleth map-based visualization, a heat map visualization, a heat table visualization, or a trip path visualization.19. The imaging method of claim 11, wherein the visualization corresponds to a particular vehicle, the particular vehicle corresponding to one or more drivers associated with the vehicle, and wherein the visualization is transmitted to the one or more drivers.20. The imaging method of claim 11 further comprising determining a risk profile using the visualization, wherein the risk profile corresponds to a particular vehicle, the particular vehicle corresponding to one or more drivers associated with the vehicle.",US10475335_B1.txt,"G08G1/01,G08G1/052,G08G1/09,G08G1/0969",{'signalling'},"['traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})']","systems and methods for visualizing telematics data Systems and methods are described for the visualization of vehicular-based telematics data. In various aspects, telematics data may be aggregated for a plurality of vehicles where the telematics data can include telematics data observation(s) for each vehicle. Each observation can indicate a coordinate value of the vehicle and a timestamp for the observation, and can further indicate any of a device identifier for a telematics device associated with the vehicle, a speed value of the vehicle, a g-force value of the vehicle, a trip identifier associated with the vehicle, a distance value of the vehicle, or a stop indicator value of the vehicle. A visualization may also be generated based on at least a subset of the telematics data such that the visualization can indicate one or more image features associated with the one or more of the plurality of vehicles.",signalling
US2017017846_A1,2015-07-15,2017-01-19,2015-07-15,UMM AL-QURA UNIVERSITY,"FELEMBAN, EMADSAQIB, MUHAMMADSHEIKH, ADIL A.",57775135,road monitoring,crowd and traffic monitoring apparatus and method,"An apparatus and method for monitoring mixed pedestrian and motor vehicle traffic. The apparatus include an optical camera, an infrared camera, and a wireless-device counter, each obtaining complementary information regarding the crowd density and flow of the traffic. Performing heat signature analysis and blob detection of the infrared image data to discriminate pedestrians from motor vehicles. Performing optical flow computations with Gaussian pyramids, Histogram of Oriented Gradients calculations, support vector machine (SVM) classifications, and correlation-based clustering using the optical images to determine the crowd density and flow of pedestrians. Incorporating the (SVM) classifications with the blob detection to differentiate pedestrians from motor vehicles. Obtaining an estimate of the number of wireless devices using the wireless-device counter as an additional indicator of the number of pedestrians and motor-vehicles. Finally, combining the above-identified regarding the crowd density and flow to create an improved traffic estimate.","1. A traffic-monitoring controller, comprising: an interface configured to receive optical-image data and device-count data; and processing circuitry configured to obtain, using the interface, the optical-image data representing an optical image of the first area, obtain, using the interface, the device-count data representing device-identification information corresponding to wireless devices detected by a receiver, wherein the detected wireless devices are within a second area corresponding to an area within a wireless communication range of the receiver, and the device-count data further represents time information corresponding to when the wireless devices were respectively detected, determine, using the optical-image data, a first pedestrian estimate indicating a number of pedestrians in the first area, determine, using the device-count data, a pedestrian-and-motor-vehicle estimate indicating a number of pedestrians and motor vehicles in the second area, and determine, using the first pedestrian estimate and the pedestrian-and-motor-vehicle estimate, a first motor-vehicle estimate of a number of motor vehicles in the first area, wherein the second area overlaps the first area.2. The traffic-monitoring controller according to claim 1, wherein the processing circuitry is further configured to obtain, using the interface, thermal-image data representing an infrared image of the first area, determine, using the thermal-image data, a second pedestrian estimate of the number of pedestrians in the first area, determine, using the thermal-image data, a second motor-vehicle estimate of the number of motor-vehicles in the first area, determine a combined pedestrian estimate of the number of pedestrians in the first area by using the first pedestrian estimate and the second pedestrian estimate, and determine a combined motor-vehicle estimate of the number of motor vehicles in the first area by using the first motor-vehicle estimate and the second motor-vehicle estimate.3. The traffic-monitoring controller according to claim 1, wherein the processing circuitry is further configured to determine, using the optical-image data, a pedestrian flow estimate of a flow of the pedestrians within the first area.4. The traffic-monitoring controller according to claim 1, wherein the processing circuitry is further configured to determine, using the wireless-device data, a wireless-device flow estimate of a flow of the wireless devices through the second area.5. The traffic-monitoring controller according to claim 2, wherein the processing circuitry is further configured to discriminate, using the thermal-image data, between regions in the first area corresponding to pedestrians and regions in the first area corresponding to motor vehicles.6. The traffic-monitoring controller according to claim 5, wherein the regions of the thermal-image data are determined using blob detection, and the processing circuitry is further configured to process each region of the thermal-image by comparing a heat signatures of the region to known heat signatures of pedestrians and to known heat signatures of motor vehicles, wherein the comparison of heat signatures is performed using a support vector machine (SVM) classifier.7. The traffic-monitoring controller according to claim 1, wherein the first pedestrian estimate is performed by discriminating, using the optical-image data, descriptor blocks within the first area using a Histogram of Oriented Gradients (HOG) algorithm to generate HOG data, and classifying, using an SVM classifier, whether each descriptor block is classified as a pedestrian or as another classification to generate classified descriptor blocks, and determining, using the classified descriptor blocks, the first pedestrian estimate.8. The traffic-monitoring controller according to claim 7, wherein the processing circuitry is further configured to determine the pedestrian flow estimate by calculating, using the optical-image data, an optical flow of the optical image to generate optical-flow data, and clustering the optical flow data to generate the pedestrian flow estimate according to flow regions, wherein the clustering is performed using the classified descriptor blocks corresponding to pedestrians, and the pedestrian flow estimate represents, for each flow region of the optical image, a number of the pedestrians and a rate of movement of the pedestrians in the respective flow region.9. The traffic-monitoring controller according to claim 8, wherein the calculation of the optical flow is performed using a Horn and Schunck optical-flow method combined with a Gaussian pyramid method.10. The traffic-monitoring controller according to claim 8, wherein the clustering of the optical flow is performed by sorting the classified descriptor blocks into k clusters that minimize a within-cluster sum of squares.11. The traffic-monitoring controller according to claim 1, wherein the processing circuitry is further configured to obtain a received signal strength indicator (RSSI) corresponding to a wireless device, compare the RSSI to an RSSI threshold, and excluding, from the pedestrian-and-motor-vehicle estimate, the device-count data corresponding to wireless devices having the RSSI less than the RSSI threshold.12. The traffic-monitoring controller according to claim 1, wherein the second area is within the first area.13. The traffic-monitoring controller according to claim 1, further comprising: a transmitter and the receiver, wherein the transmitter and the receiver are configured to transmit and to receive communication signals from another traffic-monitoring controller, wherein communication signals from the another traffic-monitoring controller include wireless-device information of the another traffic-monitoring controller, and the processing circuitry is further configured to compare the wireless-device information of the another traffic-monitoring controller with the wireless-device information of the traffic-monitoring controller.14. The traffic-monitoring controller according to claim 4, further comprising: a transmitter and the recieiver, wherein the transmitter and the receiver are respectively configured to transmit and to receive communication signals from another traffic-monitoring controller, wherein the communication signals from the another traffic-monitoring controller include wireless-device information of the another traffic-monitoring controller, and the processing circuitry is further configured to compare the wireless-device information of the another traffic-monitoring controller with the wireless-device information of the traffic-monitoring controller, determine the wireless devices that were detected by both the traffic-monitoring controller and the another traffic-monitoring controller, and compare, for each wireless device detected by both traffic-monitoring controllers, the time information from the traffic-monitoring controller with the time information of the another traffic-monitoring controller to generate time difference data, generate, using the time difference data, an average speed estimate for wireless devices migrating between the wireless communication ranges of corresponding to the traffic-monitoring controller and the another traffic-monitoring controller, and the flow estimate of the wireless devices through the second area is estimated using the average speed estimate of wireless devices.15. The traffic-monitoring controller according to claim 1, wherein the device-identification information includes a MAC address of each detected wireless device.16. A traffic-monitoring apparatus, comprising: an optical camera configured to image a first area and to generate optical-image data representing an optical image of the first area; an infrared camera configured to image the first area and generate thermal-image data representing an infrared image of the first area; a receiver configured to receive device-identification information corresponding to wireless devices within a second area, the second area being within a wireless communication range of the receiver, and generate wireless-device data representing the device-identification information and representing a time that the device-identification information was received; and processing circuitry configured to determine, using the optical-image data, a first pedestrian estimate indicating a number of pedestrians in the first area, determine, using the wireless-device data, a pedestrian-and-motor-vehicle estimate indicating a number of pedestrians and motor vehicles in the second area, and determine, using the a first pedestrian estimate and the pedestrian-and-motor-vehicle estimate, a first motor-vehicle estimate of a number of motor vehicles in the first area, determine, using the thermal-image data, a second pedestrian estimate of the number of pedestrians in the first area, determine, using the thermal-image data, a second motor-vehicle estimate of the number of motor-vehicles in the first area, determine a combined pedestrian estimate of the number of pedestrians in the first area by using the first pedestrian estimate and the second pedestrian estimate, and determine a combined motor-vehicle estimate of the number of motor vehicles in the first area by using the first motor-vehicle estimate and the second motor-vehicle estimate.17. A traffic-monitoring method, comprising: imaging a first area, using an optical camera, to generate optical-image data representing an optical image of the first area; imaging a first area, using an infrared camera, to generate thermal-image data representing an infrared image of the first area; receiving, using a receiver, device-identification information corresponding to wireless devices within a second area in a wireless communication range of the receiver; generating device data representing the device-identification information and representing a time that the device-identification information was received; determining, via the processing circuitry and using the optical-image data, a first pedestrian estimate indicating a number of pedestrians in the first area; determining, via the processing circuitry and using the wireless-device data, a pedestrian-and-motor-vehicle estimate indicating a number of pedestrians and motor vehicles in the second area; and determining, via the processing circuitry and using the a first pedestrian estimate and the pedestrian-and-motor-vehicle estimate, a first motor-vehicle estimate of a number of motor vehicles in the first area, wherein the second area overlaps the first area.18. The traffic-monitoring method according to claim 17, further comprising: imaging a first area, using an infrared camera, to generate thermal-image data representing an infrared image of the first area; determining, using the thermal-image data, a second pedestrian estimate of the number of pedestrians in the first area; determining, using the thermal-image data, a second motor-vehicle estimate of the number of motor-vehicles in the first area; determining a combined pedestrian estimate of the number of pedestrians in the first area by using the first pedestrian estimate and the second pedestrian estimate; determining a combined motor-vehicle estimate of the number of motor vehicles in the first area by using the first motor-vehicle estimate and the second motor-vehicle estimate; and comparing heat signatures of regions of the thermal-image data to known heat signatures of pedestrians and to known heat signatures of motor vehicles, wherein the comparison of heat signatures is performed using a support vector machine (SVM) classifier.19. The traffic-monitoring method according to claim 17, further comprising: determining, using the optical-image data, a pedestrian flow estimate of a flow of the pedestrians within the first area a first area; discriminating, using the optical-image data, descriptor blocks within the first area using a Histogram of Oriented Gradients (HOG) algorithm to generate HOG data; discriminating, using the SVM classifier, descriptor blocks that correspond to the pedestrians to generate classified descriptor blocks; determining, using the classified descriptor blocks, the first pedestrian estimate; calculating, using the optical-image data, an optical flow of the optical image to generate optical-flow data; and clustering the optical flow data to generate the pedestrian flow estimate, wherein the clustering is performed by sorting the classified descriptor blocks into k clusters that minimize the within-cluster sum of squares, and the pedestrian flow estimate represents for each of the regions of the optical image a number of the pedestrians and a rate of movement of the pedestrians in the respective region.20. The traffic-monitoring method according to claim 17, further comprising: receiving communication signals from another traffic-monitoring apparatus, wherein communication signals from the another traffic-monitoring apparatus include wireless-device information of the another traffic-monitoring apparatus; and comparing the wireless-device information of the another traffic-monitoring apparatus with the wireless-device information of the traffic-monitoring apparatus.",US2017017846_A1.txt,G06K9/00,{'computing; calculating; counting'},['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers'],"crowd and traffic monitoring apparatus and method An apparatus and method for monitoring mixed pedestrian and motor vehicle traffic. The apparatus include an optical camera, an infrared camera, and a wireless-device counter, each obtaining complementary information regarding the crowd density and flow of the traffic. Performing heat signature analysis and blob detection of the infrared image data to discriminate pedestrians from motor vehicles. Performing optical flow computations with Gaussian pyramids, Histogram of Oriented Gradients calculations, support vector machine (SVM) classifications, and correlation-based clustering using the optical images to determine the crowd density and flow of pedestrians. Incorporating the (SVM) classifications with the blob detection to differentiate pedestrians from motor vehicles. Obtaining an estimate of the number of wireless devices using the wireless-device counter as an additional indicator of the number of pedestrians and motor-vehicles. Finally, combining the above-identified regarding the crowd density and flow to create an improved traffic estimate.",computing; calculating; counting
US2019384993_A1,2019-06-07,2019-12-19,2018-06-13,ROBERT BOSCH,"JANSSEN, HOLGER",68724555,road monitoring,detecting driving-relevant situations at a larger distance,"A method for detecting a relevant region in the surroundings of an ego vehicle, in which a situation exists which is relevant to the driving and/or safety of the ego vehicle, from measurement data of a sensor which observes at least a portion of the surroundings, the measurement data being discretized into pixels or voxels and/or are suitably represented in some other way, the existence of the relevant situation being dependent on the presence of at least one characteristic object in the surroundings, and the resolution of the pixels, voxels and/or the other representation being insufficient for directly detecting the characteristic object, the measurement data being analyzed for the presence of a grouping of objects which contains the characteristic object, the resolution of the pixels, voxels and/or the other representation being sufficient for detecting the grouping. A region in which the grouping is detected is classified as a relevant region.","1. A method for detecting at least one relevant region in surroundings of an ego vehicle, in which a situation exists which is relevant to the driving and/or safety of the ego vehicle from relevant measurement data of at least one sensor, which observes at least a portion of the surroundings, the method comprising: discretizing the measurement data onto pixels or voxels and/or representing the measurement data in some other way, wherein an existence of the situation is dependent on a presence of at least one characteristic object in the surroundings, and wherein a resolution of the pixels, and/or the voxels, and/or the other representation being insufficient for directly detecting the characteristic object; analyzing the measurement data to detect a presence of a grouping of objects which contains the characteristic object, wherein the resolution of the pixels, and/or the voxels and/or the other representation is sufficient for detecting the grouping; and classifying a region in which the grouping is detected as a relevant region.2. The method as recited in claim 1, wherein another vehicle is selected as the characteristic object, and the grouping encompasses as a further object, in addition to the other vehicle, at least one further other vehicle and/or at least one road boundary and/or at least one lane marking and/or at least one additional object characteristic of the grouping.3. The method as recited in claim 1, wherein a vehicle component which is separated from a vehicle or a vehicle cargo item which is separated from the vehicle is selected as the characteristic object, and the grouping encompasses, as the further object, in addition to the component or the cargo item, at least one road boundary and/or at least one lane marking.4. The method as recited in claim 1, further comprising: evaluating, from the measurement data, a change in size of the grouping over time, and/or a change in density of the grouping over time, and/or a relative speed of the grouping relative to the ego vehicle, and/or a relative speed of the grouping relative to a road, and wherein an assessment as to whether a region containing the grouping is a relevant region additionally depends on the change in size over time, and/or the change in density, and/or the relative speed.5. The method as recited in claim 1, wherein, in response to a region having been classified as a relevant region, a sensor and/or a device which supports acquisition of the measurement data by the sensor, is actuated with a control variable to change a physical parameter of the measurement data acquisition, and subsequently further measurement data relating to the relevant region are acquired.6. The method as recited in claim 1, wherein, in response to a region having been classified as a relevant region, a further sensor is actuated and/or incorporated to acquire further measurement data from the relevant region.7. The method as recited in claim 5, wherein such a change in the physical parameter of the measurement data acquisition is selected that, after the change, the resolution of the pixels, and/or the voxels and/or the other representation is sufficient to detect the characteristic object, and/or a further sensor is selected whose resolution of the pixels, and/or the voxels and/or the other representation is sufficient to detect the characteristic object.8. The method as recited in claim 1, wherein, to check for the presence of the grouping, a subset of the measurement data relating to a region of the surroundings of the ego vehicle that is located at a predefined minimum distance from the ego vehicle is pre-selected.9. The method as recited in claim 7, wherein, from a plurality of objects and/or groupings detected on the basis of the measurement data, a portion of the measurement data that is attributed to the detected objects and/or groupings is ascertained on the basis of a model of a contrast mechanism, and/or further physical imaging models, for the acquisition of the measurement data.10. The method as recited in claim 1, wherein, in response to a region having been classified as a relevant region, a physical warning mechanism which is perceptible to a driver of the ego vehicle is actuated, and/or a drive system is actuated, and/or a steering system is actuated, and/or a braking system is actuated, for avoiding negative consequences for the ego vehicle, for the driver or for other road users, and/or for adapting a speed of the ego vehicle and/or a trajectory of the ego vehicle.11. A classifier module for detecting objects in measurement data which have been obtained from surroundings of an ego vehicle, the classifier module configured to receive the measurement data as input and to deliver, as output, probabilities that the measurement data indicate a presence of one or multiple examples of one or multiple entities from a predefined set of sought entities, the sought entities including: at least one grouping of multiple vehicles, and/or at least one grouping of one or multiple vehicles together with at least one road boundary and/or with at least one lane marking, and/or at least one grouping of at least one component separated from a vehicle and/or a cargo item of the vehicle together with at least one road boundary and/or with at least one lane marking and/or with at least one additional object characteristic of the grouping.12. The classifier module as recited in claim 11, wherein the classifier module is trained or is trainable using learning input data and respectively associated learning output data indicating the sought entities which are discernible in the learning input data.13. The classifier module as recited in claim 12, wherein the classifier modules includes at least one artificial neural network.14. A data set with learning input data and associated learning output data for a classifier module for detecting objects in measurement data which have been obtained from surroundings of an ego vehicle, the classifier module configured to receive the measurement data as input and to deliver, as output, probabilities that the measurement data indicate a presence of one or multiple examples of one or multiple entities from a predefined set of sought entities, the data set including (i) at least one grouping of multiple vehicles, and/or (ii) at least one grouping of one or multiple vehicles together with at least one road boundary and/or with at least one lane marking, and/or (iii) at least one grouping of at least one component separated from a vehicle and/or a cargo item of the vehicle together with at least one road boundary and/or with at least one lane marking and/or with at least one additional object characteristic of the grouping, as the sought entities which are represented in the learning input data and learning output data.15. A non-transitory machine-readable storage medium on which is stored a computer program containing machine-readable instructions which, when executed on a computer and/or on a control unit and/or on a classifier module, prompt the computer and/or the control device to: (i) upgrade the classifier module to a classifier module as for detecting objects in measurement data which have been obtained from surroundings of an ego vehicle, the classifier module configured to receive the measurement data as input and to deliver, as output, probabilities that the measurement data indicate a presence of one or multiple examples of one or multiple entities from a predefined set of sought entities, the sought entities including (a) at least one grouping of multiple vehicles, and/or (b) at least one grouping of one or multiple vehicles together with at least one road boundary and/or with at least one lane marking, and/or (c) at least one grouping of at least one component separated from a vehicle and/or a cargo item of the vehicle together with at least one road boundary and/or with at least one lane marking and/or with at least one additional object characteristic of the grouping, and/or (ii) perform a method for detecting at least one relevant region in surroundings of an ego vehicle, in which a situation exists which is relevant to the driving and/or safety of the ego vehicle from relevant measurement data of at least one sensor, which observes at least a portion of the surroundings, the method comprising: discretizing the measurement data onto pixels or voxels and/or representing the measurement data in some other way, wherein an existence of the situation is dependent on a presence of at least one characteristic object in the surroundings, and wherein a resolution of the pixels, and/or the voxels, and/or the other representation being insufficient for directly detecting the characteristic object; analyzing the measurement data to detect a presence of a grouping of objects which contains the characteristic object, wherein the resolution of the pixels, and/or the voxels and/or the other representation is sufficient for detecting the grouping; and classifying a region in which the grouping is detected as a relevant region.",US2019384993_A1.txt,"B60R1/00,G06K9/00,G06K9/34,G06K9/62,H04N13/204,H04N13/271,H04N13/275","{'electric communication technique', 'computing; calculating; counting', 'vehicles in general'}","['vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television']","detecting driving-relevant situations at a larger distance A method for detecting a relevant region in the surroundings of an ego vehicle, in which a situation exists which is relevant to the driving and/or safety of the ego vehicle, from measurement data of a sensor which observes at least a portion of the surroundings, the measurement data being discretized into pixels or voxels and/or are suitably represented in some other way, the existence of the relevant situation being dependent on the presence of at least one characteristic object in the surroundings, and the resolution of the pixels, voxels and/or the other representation being insufficient for directly detecting the characteristic object, the measurement data being analyzed for the presence of a grouping of objects which contains the characteristic object, the resolution of the pixels, voxels and/or the other representation being sufficient for detecting the grouping. A region in which the grouping is detected is classified as a relevant region.",electric communication technique computing; calculating; counting vehicles in general
US10867180_B2,2019-03-21,2020-12-15,2017-05-16,GOOGLE,"BAKIR, GOKHANGRIMSMO, NILSBADR, IBRAHIM",64271738,road monitoring,resolving automated assistant requests that are based on image(s) and/or other sensor data,"Methods, apparatus, and computer readable media are described related to causing processing of sensor data to be performed in response to determining a request related to an environmental object that is likely captured by the sensor data. Some implementations further relate to determining whether the request is resolvable based on the processing of the sensor data. When it is determined that the request is not resolvable, a prompt is determined and provided as user interface output, where the prompt provides guidance on further input that will enable the request to be resolved. In those implementations, the further input (e.g., additional sensor data and/or the user interface input) received in response to the prompt can then be utilized to resolve the request.","1. A method implemented by one or more processors, comprising: receiving at least one image captured by a camera of a client device; determining that the at least one image relates to a request related to an object captured by the at least one image; in response to determining that the at least one image relates to the request related to the object: causing image processing to be performed on the at least one image; determining, based on the image processing of the at least one image, that at least one parameter, of the object, necessary for resolving the request is not resolvable based on the image processing of the at least one image, wherein the at least one parameter is dependent on a classification of the object; determining, based on at least one of the request and the image processing of the at least one image, an additional attribute for an additional parameter that is necessary for resolving the request; in response to determining that the at least one parameter necessary for resolving the request is not resolvable: providing, for presentation via the client device or an additional client device, a prompt that is tailored to the at least one parameter; receiving, in response to the prompt, at least one of: an additional image captured by the camera, and spoken voice input; resolving a given attribute for the at least one parameter based on at least one of: the additional image received in response to the prompt, and the spoken voice input received in response to the prompt; and resolving the request based on the given attribute and the additional attribute, wherein resolving the request based on the given attribute and the additional attribute comprises: issuing a query based on the additional attribute and based on the resolved given attribute for the at least one parameter; receiving one or more results that are responsive to the issued query; and causing at least one result, of the received one or more results, to be presented, via a user interface of the client device, as a response to the request.2. The method of claim 1, wherein determining that the at least one image relates to a request related to an object captured by the at least one image is based on a user context determined based on one or more signals from the client device or the additional client device.3. The method of claim 2, wherein the one or more signals include at least one location signal.4. The method of claim 1, wherein determining that the at least one image relates to a request related to an object captured by the at least one image is based on natural language input received via a user interface input device of the client device or the additional client device.5. The method of claim 1, further comprising: determining a classification attribute of the object; determining a plurality of parameters necessary for resolving the request based on the classification attribute of the object, the at least one parameter being one of the plurality of parameters; and wherein determining that the at least one parameter is not resolved comprises: determining that the image processing of the at least one image fails to define the given attribute for the at least one parameter.6. The method of claim 5, wherein issuing the query based on the object and based on the resolved given attribute for the at least one parameter comprises: transmitting, to an agent over one or more networks, the issued query based on the object and based on the given attribute for the at least one parameter; and receiving the one or more results that are responsive to the query from the agent in response to transmitting the agent request.7. The method of claim 6, further comprising: selecting the agent from a plurality of available agents based on natural language input provided by the user via the client device or the additional client device, the natural language input being spoken or typed user interface input; wherein transmitting the request to the agent is based on selecting the agent from the plurality of available agents.8. The method of claim 1, wherein the additional image is received in response to the prompt, and wherein resolving the given attribute is based on the additional image.9. The method of claim 1, wherein the spoken voice input is received in response to the prompt, and wherein resolving the given attributed in based on the spoken voice input.10. A method implemented by one or more processors, comprising: processing at least one image captured by a camera of an electronic device to resolve one or more attributes for an object captured in the at least one image; selecting one or more fields, for the object and dependent on a classification of the object, that are undefined by the attributes resolved by the processing of the at least one image; determining whether the selected one or more fields are necessary for resolving a request related to the object captured by the at least one image; in response to determining that the selected one or more fields are necessary for resolving the request, providing, via the electronic device or an additional electronic device, a prompt that is tailored to at least one of the selected one or more fields; receiving, in response to the prompt, at least one of: an additional image captured by the camera, and user interface input; resolving a given attribute for the selected one or more fields based on the at least one of the additional image and the user interface input; determining additional content based on the resolved given attribute and based on at least one of the attributes resolved by the processing of the at least one image, wherein determining the additional content comprises: issuing a query based on the at least one of the attributes resolved by the processing of the at least one image and based on the resolved given attribute for the selected one or more fields; receiving one or more results that are responsive to the issued query; and providing, via the electronic device, the additional content for presentation to the user, wherein providing the additional content for presentation to the user comprises: causing at least one result, of the received one or more results, to be presented, via a user interface of the client device, as a response to the request.11. The method of claim 10, wherein the one or more attributes resolved by the processing include a classification for the object, and further comprising: determining the fields based on the fields being defined for the classification.12. The method of claim 10, wherein the additional image is received in response to the prompt, and further comprising: selecting a subset of available image processing engines for processing the at least one additional image, wherein the available image processing engines of the subset are selected based on being associated with resolution of the one or more fields; wherein resolving the given attribute for the selected one or more fields is based on application of the at least one additional image to the selected subset of the available image processing engines, wherein resolving the given attribute occurs without any application of the at least one additional image to other of the available image processing engines not included in the selected subset.13. A system, comprising: memory storing instructions; one or more processors executing the instructions stored in the memory, wherein in executing the instructions the one or more processors are to: receive at least one image captured by a camera of a client device; determine that the at least one image relates to a request related to an object captured by the at least one image; in response to determining that the at least one image relates to the request related to the object: cause image processing to be performed on the at least one image; determine, based on the image processing of the at least one image, that at least one parameter, of the object, necessary for resolving the request is not resolvable based on the image processing of the at least one image, wherein the at least one parameter is dependent on a classification of the object; determine, based on at least one of the request and the image processing of the at least one image, an additional attribute for an additional parameter that is necessary for resolving the request; in response to determining that the at least one parameter necessary for resolving the request is not resolvable: provide, for presentation via the client device or an additional client device, a prompt that is tailored to the at least one parameter; receive, in response to the prompt, a spoken voice input; resolve a given attribute for the at least one parameter based on the spoken voice input received in response to the prompt; and resolve the request based on the given attribute and the additional attribute, wherein in resolving the request based on the given attribute and the additional attribute one or more of the processors are to: issue a query based on the additional attribute and based on the resolved given attribute for the at least one parameter; receive one or more results that are responsive to the issued query; and cause at least one result, of the received one or more results, to be presented, via a user interface of the client device, as a response to the request.14. The system of claim 13, wherein in determining that the at least one image relates to a request related to an object captured by the at least one image, one or more of the processors are to determine that the at least one image relates to a request related to an object captured by the at least one image based on a user context determined based on one or more signals from the client device or the additional client device.15. The system of claim 14, wherein the one or more signals include at least one location signal.16. The system of claim 13, wherein in determining that the at least one image relates to a request related to an object captured by the at least one image, one or more of the processors are to determine that the at least one image relates to a request related to an object captured by the at least one image based on natural language input received via a user interface input device of the client device or the additional client device.17. The system of claim 13, wherein in executing the instructions one or more of the processors are to: determine a classification attribute of the object; determine a plurality of parameters necessary for resolving the request based on the classification attribute of the object, the at least one parameter being one of the plurality of parameters; and wherein in determining that the at least one parameter is not resolved one or more of the processors are to: determine that the image processing of the at least one image fails to define the given attribute for the at least one parameter.",US10867180_B2.txt,"G06F16/583,G06F16/9032,G06F3/16,G06K9/00,H04L12/58","{'electric communication technique', 'computing; calculating; counting'}","['electric digital data processing (computer systems based on specific computational models g06n)', 'electric digital data processing (computer systems based on specific computational models g06n)', 'electric digital data processing (computer systems based on specific computational models g06n)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'transmission of digital information, e.g. telegraphic communication ({coding or ciphering apparatus for cryptographic or other purposes involving the need for secrecy g09c;} arrangements common to telegraphic and telephonic communication h04m)']","resolving automated assistant requests that are based on image(s) and/or other sensor data Methods, apparatus, and computer readable media are described related to causing processing of sensor data to be performed in response to determining a request related to an environmental object that is likely captured by the sensor data. Some implementations further relate to determining whether the request is resolvable based on the processing of the sensor data. When it is determined that the request is not resolvable, a prompt is determined and provided as user interface output, where the prompt provides guidance on further input that will enable the request to be resolved. In those implementations, the further input (e.g., additional sensor data and/or the user interface input) received in response to the prompt can then be utilized to resolve the request.",electric communication technique computing; calculating; counting
US2020278423_A1,2019-03-01,2020-09-03,2019-03-01,GM GLOBAL TECHNOLOGY OPERATIONS,"RITTBERG, EYALROZENZAFT, OMRI",72046509,road monitoring,removing false alarms at the beamforming stage for sensing radars using a deep neural network,"Processor-implemented methods and systems that perform target verification on a spectral response map to remove false alarm detections at the beamforming stage for sensing radars (i.e., prior to performing peak response identification) using a convolutional neural network (CNN) are provided. The processor-implemented methods include: generating a spectral response map from the radar data; and, executing the CNN to determine whether the response map represents a valid target detection and to classify the response map as a false alarm when the response map does not represent a valid target detection. Subsequent to the execution of the CNN, only response maps with valid targets are processed to generated therefrom a direction of arrival (DOA) command.","1. A processor-implemented method for using radar data to generate a direction of arrival (DOA) command using a convolutional neural network (CNN), the method comprising: generating a response map from the radar data; processing, in the CNN, the response map to determine whether the response map represents a valid target detection; classifying, by the CNN, the response map as a false alarm when the response map does not represent a valid target detection; and identifying a maximum value in the response map when the response map does represent a valid target detection.2. The method of claim 1, wherein the response map is a Bartlett beamformer spectral response map.3. The method of claim 2, wherein the CNN has been trained using training data generated in an anechoic chamber.4. The method of claim 3, wherein the response map is a three-dimensional tensor of dimensions 15203.5. The method of claim 4, wherein the CNN is trained using back propagation.6. The method of claim 5, wherein the CNN comprises a plurality of hidden layers.7. The method of claim 6, wherein each of the hidden layers comprise a convergent layer with a rectified linear unit (ReLU) activation function.8. The method of claim 7, wherein each of the hidden layers further comprise Batch Normalization layers, MaxPooling layers, and Dropout layers.9. The method of claim 8, wherein the CNN comprises at least one fully connected layer (FC) with a sigmoid activation function.10. A processor-implemented method for removing false alarms at the beamforming stage for sensing radars using a convolutional neural network (CNN), the method comprising: receiving a response map generated from radar data; processing, in the CNN, the response map to determine whether the response map represents a valid target detection; classifying, by the CNN, the response map as a false alarm when the response map does not represent a valid target detection; and classifying, by the CNN, the response map as a valid response map when the response map does represent a valid target detection.11. The method of claim 10, wherein the response map is a Bartlett beamformer spectral response map.12. The method of claim 11, wherein the CNN has been trained using training data generated in an anechoic chamber and validation data generated in the anechoic chamber.13. The method of claim 12, wherein the CNN is trained using back propagation.14. The method of claim 13, wherein the response map is a three-dimensional tensor of dimensions 15203, and the CNN comprises a number, N, of hidden layers, wherein N is a function of at least the dimensions of the response map.15. The method of claim 14, wherein each of the N hidden layers comprise a convergent layer with a rectified linear unit (ReLU) activation function.16. The method of claim 15, wherein the N hidden layers are interspersed with Batch Normalization layers, MaxPooling layers, and Dropout layers.17. The method of claim 16, wherein the CNN comprises at least one fully connected layer (FC) with a sigmoid activation function.18. A system for generating a direction of arrival (DOA) command for a vehicle comprising one or more processors programmed to implement a convolutional neural network (CNN), the system comprising: a radar transceiver providing radar data; a processor programmed to receive the radar data and generate therefrom a Bartlett beamformer response map; and wherein the CNN is trained to process the response map to determine whether the response map represents a valid target detection, and classify the response map as a false alarm when the response map does not represent a valid target detection; and wherein the processor is further programmed to generate the DOA command when the response map does represent a valid target detection.19. The system of claim 18, wherein the processor is further programmed to identify a peak response in the response map when the response map does represent a valid target detection.20. The system of claim 19, wherein the processor is further programmed to train the CNN using back propagation and using a training data set and a validation data set that are each generated in an anechoic chamber.",US2020278423_A1.txt,"G01S13/04,G01S7/41",{'measuring; testing'},"['radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves']","removing false alarms at the beamforming stage for sensing radars using a deep neural network Processor-implemented methods and systems that perform target verification on a spectral response map to remove false alarm detections at the beamforming stage for sensing radars (i.e., prior to performing peak response identification) using a convolutional neural network (CNN) are provided. The processor-implemented methods include: generating a spectral response map from the radar data; and, executing the CNN to determine whether the response map represents a valid target detection and to classify the response map as a false alarm when the response map does not represent a valid target detection. Subsequent to the execution of the CNN, only response maps with valid targets are processed to generated therefrom a direction of arrival (DOA) command.",measuring; testing
US2020027333_A1,2018-07-17,2020-01-23,2018-07-17,DENSO INTERNATIONAL AMERICA,"AKELLA, RAVIBANDO, TAKASHIVOLOS, HARISXU, YUNFEI",69162465,survillance,automatic traffic incident detection and reporting system,"An automatic traffic incident detection and reporting system for a vehicle may include at least one of a plurality of cameras and a plurality of proximity sensors, an incident determination unit, a remote vehicle position determination unit, and a communication unit. The incident determination unit is configured to receive signals from the at least one of the plurality of cameras and the plurality of proximity sensors, detect whether incident involving at least one remote vehicle has occurred, and categorize the incident. The remote vehicle position determination unit is configured to receive signals from the at least one of the plurality of cameras and the plurality of proximity sensors and determine a location of the incident. The communication unit is configured to transmit data related to the incident to at least one of a cloud server and an emergency service provider.","1. An automatic traffic incident detection and reporting system for a vehicle, comprising: at least one of a plurality of cameras and a plurality of proximity sensors; an incident determination unit configured to receive signals from the at least one of the plurality of cameras and the plurality of proximity sensors, detect whether an incident involving at least one remote vehicle has occurred, and categorize the incident; a remote vehicle position determination unit configured to receive signals from the at least one of the plurality of cameras and the plurality of proximity sensors and determine a location of the incident; and a communication unit configured to transmit data related to the incident to at least one of a cloud server and an emergency service provider.2. The system of claim 1, wherein the incident determination unit is configured to analyze images from the plurality of cameras to determine whether the incident is a traffic accident.3. The system of claim 1, wherein the incident determination unit is configured to either compare images from the plurality of cameras with known images of an acceptable roadway to determine whether there are potholes or debris on a roadway, or utilize machine learning methods and models to automatically identify a road surface and any debris or damage to the road surface.4. The system of claim 1, wherein the incident is one of a vehicle accident, an immobilized vehicle, a broken-down vehicle, a major pothole, and debris on a road.5. The system of claim 1, further comprising a vehicle position determination unit configured to determine a current position of the vehicle.6. The system of claim 1, further comprising a driver alert system configured to alert a driver when an incident is detected by the incident determination unit.7. The system of claim 1, further comprising a data transmission system configured to transmit data to the cloud server, wherein the communication unit is configured to transmit the location of the incident and incident category to the data transmission system, and the data transmission system is configured to communicate the location of the incident and incident category to the cloud server.8. The system of claim 1, wherein the data includes at least one of image data, video data, global positioning data, and weather data.9. A method for automatically detecting and reporting traffic incidents, comprising: receiving, by an incident determination unit and a remote vehicle position determination unit, a plurality of signals from at least one of a plurality of cameras and a plurality of proximity sensors disposed on a vehicle; detecting, by the incident determination unit, whether an incident involving at least one remote vehicle has occurred; categorizing, by the incident determination unit, a type of the incident from data gathered by the at least one of the plurality of cameras and the plurality of proximity sensors; determining, by the remote vehicle position determination unit, an incident location from the data gathered by the at least one of the plurality of cameras and the plurality of proximity sensors; and transmitting, by a communication unit, data related to the incident to at least one of a cloud server and an emergency service provider.10. The method of claim 9, further comprising: analyzing, by the incident determination unit, images from the plurality of cameras; and determining, by the incident determination unit, whether there has been a traffic accident from the images.11. The method of claim 9, further comprising: either comparing, by the incident determination unit, images from the plurality of cameras with known images of an acceptable roadway, or employing, by the incident determination unit, scene analysis algorithms; and determining, by the incident determination unit, whether there are potholes or debris on a current roadway.12. The method of claim 9, wherein the incident is one of a vehicle accident, an immobilized vehicle, a broken-down vehicle, a major pothole, and debris on a road.13. The method of claim 9, further comprising determining, by a vehicle position determination unit, a current position of the vehicle.14. The method of claim 9, further comprising alerting, by a driver alert system, a driver when an incident is detected by the incident determination unit.15. The method of claim 9, further comprising transmitting, by the communication unit, the incident location and type of incident to a data transmission system on the vehicle, and communicating, by the data transmission system, the incident location and type of incident to the cloud server.16. A method of detecting and handling incident reports at a cloud server, the method comprising: monitoring, by a cloud server, a communications network for at least one incident report including data for a traffic incident; verifying, by the cloud server, a validity of the at least one incident report; notifying, by the cloud server, an emergency services provider upon receiving and verifying the at least one incident report; reconstructing, by the cloud server, a scene of the traffic incident from the at least one incident report; packaging the scene and the at least one incident report into a data package; and sending, by the cloud server, the data package to a service provider for analysis.17. The method of claim 16, further comprising sorting, by the cloud server, the data from the at least one incident report into chronological order to reconstruct the scene of the traffic incident, wherein the data is sorted according to timestamp information.18. The method of claim 17, further comprising: evaluating the data, by the cloud server, to determine whether gaps exist in the data; and requesting, by the cloud server, additional data from vehicles local to the traffic incident if gaps exist in the data.19. The method of claim 16, further comprising comparing and corroborating, by the cloud server, a plurality of incident reports from multiple sources to verify the at least one incident report.20. The method of claim 16, wherein the at least one incident report includes data from a traffic accident, a broken-down vehicle, an immobilized vehicle, a pothole, or debris in a road.",US2020027333_A1.txt,"G06K9/00,G06T7/292,G06T7/70,G08B25/00,G08G1/16,H04L29/08,H04W4/44","{'electric communication technique', 'signalling', 'computing; calculating; counting'}","['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general', 'image data processing or generation, in general', 'signalling or calling systems; order telegraphs; alarm systems', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'transmission of digital information, e.g. telegraphic communication ({coding or ciphering apparatus for cryptographic or other purposes involving the need for secrecy g09c;} arrangements common to telegraphic and telephonic communication h04m)', 'wireless communication networks (broadcast communication h04h; communication systems using wireless links for non-selective communication, e.g. wireless extensions h04m1/72)']","automatic traffic incident detection and reporting system An automatic traffic incident detection and reporting system for a vehicle may include at least one of a plurality of cameras and a plurality of proximity sensors, an incident determination unit, a remote vehicle position determination unit, and a communication unit. The incident determination unit is configured to receive signals from the at least one of the plurality of cameras and the plurality of proximity sensors, detect whether incident involving at least one remote vehicle has occurred, and categorize the incident. The remote vehicle position determination unit is configured to receive signals from the at least one of the plurality of cameras and the plurality of proximity sensors and determine a location of the incident. The communication unit is configured to transmit data related to the incident to at least one of a cloud server and an emergency service provider.",electric communication technique signalling computing; calculating; counting
EP3499473_A1,2018-12-10,2019-06-19,2017-12-15,SIEMENS MOBILITY,"SCHMIDT, ANDREAS HEIKO",64664136,survillance,automated detection of hazardous situations,"In the method for automatically detecting a dangerous situation in a surveillance area (B), video data (VD) from the surveillance area (B) is continuously recorded. The captured video data (VD) is further converted into optical flow data (OFD). A classifier (K) trained on the basis of relevant movement patterns is then applied to the optical flow data (OFD) in real time. Furthermore, a dangerous situation (GS) is determined on the basis of a classification result (KE). An alerting procedure is also described. A monitoring system (10) is also described. An alarm system (1) is also described. Furthermore, a passenger transport vehicle is described.","1. Method for automatically detecting a dangerous situation in a surveillance area (B), comprising the steps of: - continuously acquiring video data (VD) from the monitor region (B), - Converting the video data (VD) in the Optical-flow data (OFD), - Applying a trained classifier based on relevant patterns (K) on the Optical-flow data (OFD) in real time, - Determining on the basis of a classification result (KE), whether a dangerous situation (GS) is present. 2. Method according to claim 1, wherein the classifier (K) is trained by the following steps: - Acquiring video data (VDB) with exemplary movement patterns, - Converting the video data (VDB) in Optical flow data (OFDB), - Training the classifier (K) with the aid of an automated learning method on the basis of the movement of the patterned Optical-flow data with exemplary (OFDB). 3. Method according to claim 2, wherein the automated machine learning method includes a learning method. 4. Method according to claim 3, wherein the machine Learning method a Deep-Learning method includes. 5. Method according to one of the preceding claims, wherein for the case, that a danger situation (GS) is determined, on the basis of the classification result (KE) a type of hazardous situation is determined. 6. Alerting method, comprising the steps of: - Carrying out the method according to any one of claims 1 to 5, - automated triggering of an alarm (AL) for the case, that a danger situation (GS) has been detected. 7. Method according to claim 6, wherein the type of alarm to be triggered (AL) and the recipient of the alarm function of the determined type of the hazard situation (GS) can be selected. 8. Monitoring (10) system, comprising: - a video recording unit (11) for continuously acquiring video data (VD) from the monitor region (B), - a conversion unit (12) for converting the video data (VD) in the Optical-flow data (OFD), - a classification unit (13) for applying a trained classifier based on relevant patterns (K) on the Optical-flow data (OFD) in real time, - a danger determining unit (14) for determining a dangerous situation (GS) on the basis of a classification result (KE). 9. , Alarm (1) system comprising: - a monitoring (10) system according to claim 8 and - an alarming unit (15) for automated triggering of an alarm (AL) as a function of a detected hazardous situation by the monitoring (10) system (GS). 10. Passenger transport vehicle, comprising an alarm (1) system according to claim 9. 11. Computer program product having a computer program, loadable directly into a memory unit of a computer unit of a (10) monitoring system which is, with program sections, all steps of a method according to any one of claims 1 to 5 to perform, when the computer program in the computer unit is performed. 12. Computer-readable medium, on which a computer executable program portions stored therein, to carry out all steps of the method according to any one of claims 1 to 5 to perform, when the program portions of the computer unit are executed.",EP3499473_A1.txt,G08B13/196,{'signalling'},['signalling or calling systems; order telegraphs; alarm systems'],automated detection of hazardous situations Bei dem Verfahren zum automatisierten Detektieren einer Gefahrensituation in einem berwachungsbereich (B) werden kontinuierlich Videodaten (VD) von dem berwachungsbereich (B) erfasst. Die erfassten Videodaten (VD) werden weiterhin in Optical-Flow-Daten (OFD) umgewandelt. Anschlieend wird ein auf Basis relevanter Bewegungsmuster trainierter Klassifikator (K) auf die Optical-Flow-Daten (OFD) in Echtzeit angewendet. Weiterhin wird eine Gefahrensituation (GS) auf Basis eines Klassifikationsergebnisses (KE) ermittelt. Es wird auch ein Alarmierungsverfahren beschrieben. Es wird ferner ein berwachungssystem (10) beschrieben. Zudem wird ein Alarmsystem (1) beschrieben. berdies wird ein Personentransportfahrzeug beschrieben.,signalling
CN111077517_A,2019-12-09,2020-04-28,2019-12-09,WUHAN KOTEI INFORMATICS COMPANY,WANG TINGWEI YANGDAOXIA LIANGJUNCHENG DEXIN,70313446,road monitoring,vehicle detection tracking method and device,"The embodiment of the invention provides a vehicle detection tracking method and device. The method comprises the steps: acquiring a surrounding environment image, detecting surrounding vehicles basedon a trained vehicle detection model, and acquiring position information and size information of the surrounding vehicles; and acquiring relative position information with the surrounding vehicles based on a camera imaging principle and the position information and the size information of the surrounding vehicles, and correcting the relative position information based on millimeter-wave radar ranging. Surrounding vehicles are detected through a deep learning algorithm, the position and size information of the surrounding vehicles is obtained; the foreground proportion is increased and environmental interference is eliminated by ways of data enhancement, foreground and background separation and the like on images to improve the tracking effect; detected objects are subjected to image enhancement, so that the sample size is increased and the tracking precision is improved; meanwhile, the distance calculated by the image and the distance measured by the millimeter-wave radar are fused, so that the advantages of high accuracy of the millimeter-wave radar and high frame rate of the high-speed camera are achieved.","1. a vehicle detection tracking method, comprising: acquiring surrounding environment images, detecting and tracking surrounding vehicles based on a trained vehicle detection model, and acquiring position information and size information of the surrounding vehicles; and acquiring relative position information of the peripheral vehicle based on a camera imaging principle and the position information and the size information of the peripheral vehicle, and correcting the relative position information based on millimeter wave radar ranging. 2. the vehicle detection and tracking method according to claim 1, wherein before detecting and tracking the nearby vehicle based on the trained vehicle detection model, further comprising: acquiring image information of surrounding vehicles, carrying out background segmentation, segmenting a vehicle region image, and carrying out foreground and background separation processing on the vehicle region image to obtain a vehicle body part image; and performing image enhancement processing on the vehicle body part image to expand a vehicle body part image sample, and performing machine learning based on the vehicle body part image sample to obtain a vehicle detection model. 3. the vehicle detection tracking method of claim 2, wherein the image enhancement processing includes translation, rotation, and scaling. 4. the vehicle detection and tracking method according to claim 1, wherein acquiring the relative position information with respect to the nearby vehicle based on the camera imaging principle and the position information and the size information of the nearby vehicle specifically comprises: and calculating the relative position information of the surrounding vehicle by a small hole imaging principle based on the acquired position information, size information and camera parameter information of the surrounding vehicle. 5. the vehicle detection and tracking method according to claim 1, wherein the correcting the relative position information based on millimeter wave radar ranging specifically comprises: correcting the relative position information of the surrounding vehicles acquired through the surrounding environment image at the same time by taking the position data acquired by the millimeter wave radar as a reference; and if the millimeter wave radar at the current moment has no position data, correcting by taking the relative position information acquired by the surrounding environment image at the previous moment as a reference. 6. the vehicle detection and tracking method according to claim 1, further comprising: and analyzing and judging the relative position relation and the relative speed change of the own vehicle and the surrounding vehicle based on the relative position information within a plurality of set frames, and sending the relative position relation and the relative speed change to the vehicle control unit. 7. the vehicle detection and tracking method according to claim 1, further comprising: and acquiring the surrounding environment image at set time intervals to detect whether a newly added surrounding vehicle appears. 8. a vehicle detection and tracking device, comprising: the vehicle detection module is used for acquiring images of the surrounding environment, detecting and tracking surrounding vehicles based on the trained vehicle detection model and acquiring position information and size information of the surrounding vehicles; and the vehicle tracking module is used for acquiring the relative position information of the vehicle and the surrounding vehicle based on the camera imaging principle and the position information and the size information of the surrounding vehicle, and correcting the relative position information based on the millimeter wave radar ranging. 9. an electronic device comprising a memory, a processor and a computer program stored on the memory and executable on the processor, wherein the steps of the vehicle detection and tracking method according to any one of claims 1 to 7 are implemented when the program is executed by the processor. 10. a non-transitory computer readable storage medium having a computer program stored thereon, wherein the computer program when executed by a processor implements the steps of the vehicle detection and tracking method according to any one of claims 1 to 7.",CN111077517_A.txt,"G01S13/66,G01S13/86",{'measuring; testing'},"['radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves']","vehicle detection tracking method and device The embodiment of the invention provides a vehicle detection tracking method and device. The method comprises the steps: acquiring a surrounding environment image, detecting surrounding vehicles basedon a trained vehicle detection model, and acquiring position information and size information of the surrounding vehicles; and acquiring relative position information with the surrounding vehicles based on a camera imaging principle and the position information and the size information of the surrounding vehicles, and correcting the relative position information based on millimeter-wave radar ranging. Surrounding vehicles are detected through a deep learning algorithm, the position and size information of the surrounding vehicles is obtained; the foreground proportion is increased and environmental interference is eliminated by ways of data enhancement, foreground and background separation and the like on images to improve the tracking effect; detected objects are subjected to image enhancement, so that the sample size is increased and the tracking precision is improved; meanwhile, the distance calculated by the image and the distance measured by the millimeter-wave radar are fused, so that the advantages of high accuracy of the millimeter-wave radar and high frame rate of the high-speed camera are achieved.",measuring; testing
CN110189317_A,2019-05-30,2019-08-30,2019-05-30,SHANGHAI CAROL NETWORK TECHNOLOGY COMPANY,WANG JIAN,67718789,road monitoring,road image intelligent acquisition and identification method based on deep learning,"The invention provides a road image intelligent acquisition and identification method based on deep learning. The road image intelligent acquisition and identification method comprises the following steps: shooting a road video image through road image acquisition hardware, extracting key frames in the video image through a program, detecting and identifying the key frames through an image intelligent identification algorithm, and determining a disease position, a disease category and a disease grade in the road image. According to the invention, positioning of the road disease area and classification tasks of types and grades can be completed, and manual intervention is not needed, and unmanned operation is completely realized, and the automation degree is high, and the fault tolerance ishigh, and the road maintenance patrol efficiency can be greatly improved, and the intelligent management level of the industry is improved.","1. a road image intelligent acquisition and identification method based on deep learning is characterized by comprising the following steps: (1) the method comprises the steps that road video images are collected through vehicle-mounted video collection equipment in a high-speed driving state, and key video frames are extracted through an intelligent frame extraction algorithm to obtain road images, so that no repetition or loss is guaranteed; (2) pre-training a classification convolutional neural network model (vggnet) for distinguishing asphalt roads, cement roads and non-motor vehicles by using sufficient road image samples, classifying and identifying the extracted road images, and determining the class attributes of the road images; (3) respectively training a target detection model (fast-rcnn) for detecting three road diseases by using sufficient asphalt road disease images, cement road disease images and non-motor vehicle road disease images, carrying out target detection on the classified road images, and determining the road disease category and the position of the road disease category in the images; (4) intercepting a disease area corresponding to the road image as a subgraph, inputting the subgraph to a pre-trained image classification neural network model (vggnet), calculating the probability that the subgraph belongs to three grades of light, medium and heavy, and identifying the disease grade of each road disease; (5) cascading the classified convolutional neural network model (vggnet) in the step (2), the target detection model (faster-rcnn) in the step (3) and the image classified neural network model (vggnet) in the step (4) to form a road disease detection and identification system in a complex scene; (6) and respectively storing the structures and the weight parameters of the three groups of models as model files, and respectively loading the three groups of model files through tensorflow in the model prediction process, thereby finishing the cascade connection of the classification model and the target detection model. 2. the method for intelligently acquiring and identifying road images based on deep learning of claim 1, wherein the classified convolutional neural network model (vggnet) pre-trained in the step (2) is a vggnet network model, and the specific training process of the vggnet network model is as follows: preprocessing the marked road disease training sample image, inputting the preprocessed image into the vggnet network model, adding a known road attribute class label, using cross entropy (cross entropy) as a loss function, calculating a back propagation gradient in a random gradient descent (sgd) mode, and optimizing network parameters according to the gradient to gradually reduce the loss function, wherein the loss function is close to 0 finally. 3. the method as claimed in claim 1, wherein the pre-training target detection model in step (3) is a fast-rcnn network model using resnet101 as a feature extraction network, and the specific training process of the fast-rcnn network model is as follows: inputting a training sample image into the fast-rcnn network model, generating l1-smooth function loss of a frame by the frame predicted by the rpn and a known labeling frame, generating category cross entropy loss by the category predicted by the rpn and the known labeling category, generating second loss on the frame and the category after a full connection layer and regression calibration, and optimizing the loss by using a random gradient descending mode, wherein in the optimization process, the gradient generated by the loss in the second step is blocked behind the rpn layer, and the rpn and the network parameters in front of the rpn layer are continuously optimized. 4. the method for intelligently acquiring and identifying road images based on deep learning of claim 1, wherein in the step (4), the classified convolutional neural network is trained in the same manner as in the step (2) to complete the prediction of road disease grades. 5. the method for intelligently acquiring and identifying road images based on deep learning of claim 1, wherein in the step (5), the classified convolutional neural network model (vggnet), the target detection model (faster-rcnn) and the image classified neural network model (vggnet) are cascaded in a manner that the tensorflow respectively derives the three models as a frozen model pb file.",CN110189317_A.txt,"G06K9/62,G06T7/00",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general']","road image intelligent acquisition and identification method based on deep learning The invention provides a road image intelligent acquisition and identification method based on deep learning. The road image intelligent acquisition and identification method comprises the following steps: shooting a road video image through road image acquisition hardware, extracting key frames in the video image through a program, detecting and identifying the key frames through an image intelligent identification algorithm, and determining a disease position, a disease category and a disease grade in the road image. According to the invention, positioning of the road disease area and classification tasks of types and grades can be completed, and manual intervention is not needed, and unmanned operation is completely realized, and the automation degree is high, and the fault tolerance ishigh, and the road maintenance patrol efficiency can be greatly improved, and the intelligent management level of the industry is improved.",computing; calculating; counting
US8229170_B2,2008-07-31,2012-07-24,2008-07-31,"GE (GENERAL ELECTRIC COMPANY)TU, PETER HENRYSEBASTIAN, THOMAS, BABY","TU, PETER HENRYSEBASTIAN, THOMAS, BABY",41608404,road monitoring,method and system for detecting a signal structure from a moving video platform,"The present invention aims at providing a method for detecting a signal structure from a moving vehicle. The method for detecting signal structure includes capturing an image from a camera mounted on the moving vehicle. The method further includes restricting a search space by predefining candidate regions in the image, extracting a set of features of the image within each candidate region and detecting the signal structure accordingly.","1. A method for detecting a signal structure from a moving vehicle comprising: capturing an image from a camera mounted on the moving vehicle; restricting a search space by predefining candidate regions in the image based on locations of signal structures along railway tracks stored in a memory; extracting a set of features of the image within each candidate region; and detecting the signal structure; wherein restricting the search space by predefining candidate regions in the image comprises mapping a set of measurement in world coordinates to a set of measurement in image coordinates based on a projection matrix.2. The method of claim 1, wherein the moving vehicle comprises a railroad train.3. The method of claim 1, wherein restricting the search space comprises detecting a path of the vehicle.4. The method of claim 1, wherein the candidate region is provided by calibrating the camera.5. The method of claim 1, wherein the set of measurement comprises the height of the signal structure with respect to the ground plane.6. The method of claim 4, wherein the candidate region comprises a candidate box.7. The method of claim 6, wherein the candidate box size is approximately equivalent to the signal structure size.8. The method of claim 1, wherein detecting the signal structure comprises classifying the set of features as a signal structure image region or a non-signal structure image region.9. The method of claim 8, comprising classifying extracted features using a machine learning technique.10. The method of claim 9, wherein the machine learning technique comprises support vector machine learning technique or boosting technique.11. The method of claim 1, further comprising tracking the signal structure in the image to eliminate false detections.12. A method for detecting a signal structure from a moving vehicle comprising: capturing a video from a camera mounted on the moving vehicle; extracting an image from the video; restricting a search space by predefining candidate regions in the image based on locations of signal structures along railway tracks stored in a memory; extracting a set of features of the image within each candidate region; detecting the signal structure; and tracking the detection of the signal structure to eliminate a false detection; wherein restricting the search space by predefining candidate regions in the image comprises mapping a set of measurement in world coordinates to a set of measurement in image coordinates based on a projection matrix.13. A system for signal structure detection comprising: a camera configured to capture video or images from a moving vehicle; a descriptor configured to extract a set of features from at least one predefined candidate region in the image; and a classifier configured to classify the set of features as a signal structure image or a non-signal structure image; wherein the descriptor is further configured to map a set of measurement in world coordinates to a set of measurement in image coordinates based on a projection matrix.14. The system of claim 13, wherein the camera comprises a video camera or a high-speed digital camera.15. The system of claim 13, wherein the camera comprises a pan-tilt-zoom camera.16. The system of claim 13, wherein the moving vehicle comprises a railroad train.17. The system of claim 13, wherein the candidate region is provided by camera calibration.18. The system of claim 13, wherein the descriptor and the classifier are implemented by appropriate programming of a digital processor.19. The system of claim 13, wherein the classifier comprises a machine learning technique algorithm.20. The system of claim 19, wherein the machine learning technique algorithm comprises adaptive boosting algorithm.21. The system of claim 19, wherein the machine learning technique algorithm comprises support vector machine algorithm.22. A non-transitory computer-readable storage medium comprising computer-readable instructions of a computer program that, when executed by a processor, cause the processor to perform a method for detecting a signal structure in an image comprising: restricting a search space in the image by predefining candidate regions in the image based on locations of signal structures along railway tracks stored in a memory; extracting a set of features of the image within the candidate region; and detecting a signal structure; wherein restricting the search space by predefining candidate regions in the image comprises mapping a set of measurement in world coordinates to a set of measurement in image coordinates based on a projection matrix.23. The non-transitory computer-readable storage medium of claim 22, further comprising a code for classifying the set of features as a signal structure image or a non-signal structure image.",US8229170_B2.txt,G06K9/00,{'computing; calculating; counting'},['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers'],"method and system for detecting a signal structure from a moving video platform The present invention aims at providing a method for detecting a signal structure from a moving vehicle. The method for detecting signal structure includes capturing an image from a camera mounted on the moving vehicle. The method further includes restricting a search space by predefining candidate regions in the image, extracting a set of features of the image within each candidate region and detecting the signal structure accordingly.",computing; calculating; counting
US2020191590_A1,2016-12-28,2020-06-18,2016-12-28,HONDA MOTOR COMPANY,"KAWASAKI, SHINJISAITO, SUSUMUITO, KAZUHIROYAMAGISHI, YUSUKEMOTEGI, YOJI",62711069,gps,information processing system and information processing device,"An information processing system and an information processing device that are capable of reflecting an emotion of a user and performing route search. In an information processing system including a search unit that performs the route search on the basis of route search information that is input, to output a result searched by the search unit, the search unit performs the route search on the basis of the route search information, and emotion information obtained from an artificial intelligence module that generates an emotion of the same quality as in an emotion of a searcher.","1. An information processing system comprising a search unit that performs route search on the basis of route search information that is input, to output a result searched by the search unit, wherein the search unit performs the route search on the basis of the route search information, and emotion information obtained from artificial intelligence that generates an emotion of the same quality as in an emotion of a searcher.2. The information processing system according to claim 1, wherein the emotion generated by the artificial intelligence is generated on the basis of at least information that is input by the searcher and information based on an activity of the searcher.3. The information processing system according to claim 1, wherein the search unit performs the route search on the basis of selection information on a route selected as a route to be utilized by the searcher in the past, in addition to the route search information and the emotion information.4. The information processing system according to claim 1, wherein a target of a route to be searched by the search unit includes a route to move by a vehicle.5. The information processing system according to claim 4, wherein the artificial intelligence generates the emotion of the searcher to the movement by the vehicle.6. The information processing system according to claim 1, wherein the route search information includes information on a departure point and information on a destination, and additionally includes information on a transit point when the transit point to pass before reaching the destination is set, and the search unit reflects movement to the transit point and movement from the transit point and performs the route search, when the route search information includes the information on the transit point.7. An information processing device comprising an input unit into which route search information is input, an output unit that outputs a result of route search, and a control unit that controls the input unit and the output unit, wherein the control unit controls the output unit to output the result of the route search performed on the basis of the route search information input into the input unit and emotion information obtained from artificial intelligence that generates an emotion of the same quality as in an emotion of a searcher.",US2020191590_A1.txt,"G01C21/34,G06N20/00,G06N5/04","{'measuring; testing', 'computing; calculating; counting'}","['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models']","information processing system and information processing device An information processing system and an information processing device that are capable of reflecting an emotion of a user and performing route search. In an information processing system including a search unit that performs the route search on the basis of route search information that is input, to output a result searched by the search unit, the search unit performs the route search on the basis of the route search information, and emotion information obtained from an artificial intelligence module that generates an emotion of the same quality as in an emotion of a searcher.",measuring; testing computing; calculating; counting
CN109829400_A,2019-01-18,2019-05-31,2019-01-18,QINGDAO UNIVERSITY,"PAN ZHENKUANWANG, LIANGLIANGWANG GUODONGXU JIELI NINGXIAOHU SHIYUWANG YANJIE",66861735,vehicle classification,rapid vehicle detection method,"The invention belongs to the technical field of video detection in deep learning. The invention relates to a vehicle detection method, in particular to a rapid vehicle detection method based on windowcharacteristics. According to the method, a vehicle window is used for replacing a vehicle body to serve as a target object for detection, a residual module of a ResNet network and a multi-scale feature extraction method of an SSD algorithm are combined, a network structure of YOLOv3 is used for reference, and a full convolution detection method with only 24 convolution layers is constructed; Under the condition that the traffic flow is large, during batch testing, the average detection precision is close to 100%, the average detection rate reaches 90%, the detection speed reaches 22 milliseconds per frame, real-time detection of vehicles in a road high-definition monitoring video is achieved, the detection rate of the vehicles in the large traffic flow is effectively increased, and the method has important application value.","1. A rapid vehicle detection method, characterized by comprising the steps of: (1) collecting data: capturing a large number of pictures under different conditions from a real-time traffic monitoring video of vehicles in a road, wherein the size of the pictures is 1920px multiplied by 1080px, and the pictures are used as data; (2) data annotation: performing data labeling on the picture acquired in the step (1) based on the car window characteristics, framing a front car window and a rear car window of the car as target objects by using rectangular frames in the picture by adopting a labelimg labeling tool, and labeling the target objects by attaching a class label of the car, and storing pixel point coordinate information of each drawn rectangular frame on the picture in an xml data format; (3) constructing a basic network: under the data labeling mode in the step (2), performing feature point extraction on an input picture layer by layer according to the pixel size of the feature layer scale by using residual connection of a ResNet network, sequentially forming four parts with feature layer scales of 208px  208px, 104px  104px, 52px  52px and 26px  26px, fusing information of different feature layers with the same scale, and constructing a 13-layer full-convolution small-sized basic network; (4) the construction detection method comprises the following steps: by using a multi-scale feature extraction method of the SSD for reference, on the basis of the basic network constructed in the step (3), by utilizing a multi-layer feature extraction technology, feature layer information is respectively extracted from residual connecting layers with the scales of 104px  104px, 52px  52px and 26px  26px in the basic network and fused to the detection network, and the feature layer information is fused with feature information of corresponding feature layers in the detection network after upsampling; then, convolving by 1  1 to generate a tensor corresponding to the number of the detected target species, namely: a (26  26+52  52+104  104)  (3  (4+1+ C)) dimensional tensor; screening by a sigmoid function and a 0.5 threshold value, and selecting the type with the highest confidence coefficient as a prediction type; obtaining a final prediction frame by carrying out non-maximum suppression on the plurality of prediction frames; then obtaining the category and position information of the predicted target vehicle to form a detection method; (5) training data; (6) and (5) applying a test. 2. The rapid vehicle detection method according to claim 1, wherein the different conditions in step (1) refer to different road sections, different road conditions, different weather conditions or different time; the category labels of the vehicles in the step (2) are divided into two categories of 'bus' and 'car', wherein the 'bus' category comprises buses and buses, and the 'car' category comprises cars of small cars and SUV (sports utility vehicles). 3. The rapid vehicle detection method according to claim 1-2, wherein the forming process of the portion with the feature layer dimension of 208px x 208px in step (3) is: firstly, adjusting the size of a picture pixel to 416px multiplied by 416px as data to be input into a network, and deepening and extracting features of a feature layer by using 16 convolutions with the size of 3 multiplied by 3 with the stride of 1; then, 32 convolutions with the size of 3  3 are used, the step length is 2, the characteristic layer is further deepened, the characteristic graph dimension of the layer is reduced to 208px  208px, and characteristic extraction is carried out, so that a first layer under the dimension of 208px  208px is obtained; then, performing convolution by 1 multiplied by 1, wherein the stride is 1, and fusing the extracted features to obtain a second layer under the scale of 208px multiplied by 208 px; performing convolution by 3 multiplied by 3, increasing the depth of the feature layer to 32 feature maps with the size of 208px multiplied by 208px by 1 step to obtain a third layer under the scale of 208px multiplied by 208 px; finally, by taking residual connection of the ResNet network as a reference, the first layer under the scale of 208px  208px is connected with the characteristic residual of the third layer under the scale of 208px  208px to form a residual connection layer under the scale of 208px  208px as a characteristic map. 4. The rapid vehicle detection method according to claim 1-2, wherein the forming process of the portion with the feature layer dimension of 104px x 104px in step (3) is: performing feature extraction on the feature map of the residual connection layer under the scale of 208px  208px by using 64 convolutions of 3  3 with the step size of 2, reducing the scale of the feature map of the layer to 104px  104px, and performing feature extraction to obtain a first layer under the scale of 104px  104 px; then, the features extracted are fused by using 1  1 convolution with the stride of 1 to obtain a second layer under the scale of 104px  104 px; then, performing convolution by 3 multiplied by 3, and deepening the feature layer to 64 feature maps with the size of 104px multiplied by 104px by 1 with the step length of 1 to obtain a third layer under the scale of 104px multiplied by 104 px; and residual error connecting the characteristics of the first layer under the 104px  104px scale and the third layer under the 104px  104px scale to form a residual error connecting layer under the 104px  104px scale of the characteristic map. 5. The rapid vehicle detection method according to claim 1-2, wherein the forming process of the portion with the feature layer dimension of 52px x 52px in step (3) is: performing feature extraction on the feature map of the residual error connection layer under the scale of 104px  104px by using 128 convolutions of 3  3 with the step size of 2, reducing the scale of the feature map of the layer to 52px  52px, and performing feature extraction to obtain a first layer under the scale of 52px  52 px; then, the features extracted are fused by using 1  1 convolution with the stride of 1 to obtain a second layer under the scale of 52px  52 px; then, performing convolution by 3 multiplied by 3, and deepening the feature layer to 128 feature maps with the size of 52px multiplied by 52px by 1 step to obtain a third layer under the scale of 52px multiplied by 52 px; and residual connecting the characteristics of the first layer at the 52px  52px scale and the third layer at the 52px  52px scale to form a residual connecting layer with the characteristic map of 52px  52 px. 6. The rapid vehicle detection method according to claim 1-2, wherein the forming process of the part with the characteristic layer dimension of 26px x 26px in step (3) is: performing feature extraction on the feature map of the residual connection layer under the scale of 52px  52px by using 256 convolutions of 3  3 with the step size of 2, reducing the scale of the feature map of the layer to 26px  26px, and performing feature extraction to obtain a first layer under the scale of 26px  26 px; then, 1  1 convolution is used, the step is 1, characteristics are fused, and a second layer under the scale of 26px  26px is obtained; then, performing convolution by 3 multiplied by 3, and deepening the feature layer to 256 feature maps with the size of 26px multiplied by 26px by 1 step to obtain a third layer under the scale of 26px multiplied by 26 px; and residual connecting the characteristics of the first layer under the 26px  26px scale and the third layer under the 26px  26px scale to form a residual connecting layer with the characteristic map of 26px  26 px. 7. The rapid vehicle detection method according to claim 1-2, wherein the 13-layer full-convolution small-scale base network in step (3) has a specific structure as follows: 8. the rapid vehicle detection method according to claim 1, wherein the training data in step (5) is operated by: selecting 80% of the pictures marked in the step (2) as training data, and using the rest 20% as test pictures; training the training data by using the detection method in the step (4), and stopping training when the loss value is smaller than a set value; get a weight model with "". weights"" as suffix. 9. The rapid vehicle detection method according to claim 1, wherein the application test in step (6) is performed by: the detection method in the step (4) takes the weight model generated in the step (5) as a standard, predicts a test picture, and checks the test precision and the test speed; when the testing precision and the testing speed meet the requirements, the constructed detection method meets the requirements; and then detecting the picture or the road traffic video with the target vehicle by using the constructed detection method and the trained weight model, identifying the vehicle in real time, and completing the detection of the moving vehicle. 10. The application of the rapid vehicle detection method in real-time detection of vehicles in high-definition monitoring videos of roads as claimed in any one of claims 1 to 9, wherein the average detection precision of the rapid vehicle detection method is close to 100%, the average detection rate reaches 90%, and the detection speed reaches 22 milliseconds/frame.",CN109829400_A.txt,G06K9/00,{'computing; calculating; counting'},['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers'],"rapid vehicle detection method The invention belongs to the technical field of video detection in deep learning. The invention relates to a vehicle detection method, in particular to a rapid vehicle detection method based on windowcharacteristics. According to the method, a vehicle window is used for replacing a vehicle body to serve as a target object for detection, a residual module of a ResNet network and a multi-scale feature extraction method of an SSD algorithm are combined, a network structure of YOLOv3 is used for reference, and a full convolution detection method with only 24 convolution layers is constructed; Under the condition that the traffic flow is large, during batch testing, the average detection precision is close to 100%, the average detection rate reaches 90%, the detection speed reaches 22 milliseconds per frame, real-time detection of vehicles in a road high-definition monitoring video is achieved, the detection rate of the vehicles in the large traffic flow is effectively increased, and the method has important application value.",computing; calculating; counting
CN107729818_B,2017-09-21,2020-09-22,2017-09-21,BEIHANG UNIVERSITY,JIANG NALIU JUNQISUN CHENXINWU WEIZHOU ZHONG,61207252,vehicle classification,multi-feature fusion vehicle re-identification method based on deep learning,"The invention discloses a multi-feature fusion vehicle re-identification method based on deep learning. The method comprises five parts of model training, license plate identification, vehicle identification, similarity measurement and visualization. The method comprises the steps that a large-scale vehicle data set is used for model training, and a multi-loss function-phased joint training policyis used for training; license plate identification is carried out on each vehicle image, and a license plate identification feature vector is generated according to the license plate recognition condition; a trained model is used to extract the vehicle descriptive features and vehicle attribute features of an image to be analyzed and an image in a query library, and the vehicle descriptive features and the license plate identification vector are combined with the unique re-identification feature vector of each vehicle image; in the stage of similarity measurement, similarity measurement is carried out on the image to be analyzed and the re-identification feature vector of the image in the query library; and a search result which meets requirements is locked and visualized.","1. A multi-feature fusion vehicle re-identification method based on deep learning is characterized in that: the method comprises five parts of training a model, recognizing a license plate, recognizing a vehicle, measuring similarity and visualizing: step (m1) training a model, namely completing the staged training of the feature extraction model by using a large-scale vehicle data set, a deep learning framework and a training strategy; step (m2) license plate recognition, namely, taking the license plate information of the vehicle as the important characteristic of the vehicle recognition, dividing the stage into three parts, namely preprocessing, license plate judgment and license plate content recognition, and storing the license plate recognition result as a license plate identification vector; step (m3), vehicle identification, namely, extracting the expressive feature vector and the vehicle attribute feature of the vehicle identification by using the feature extraction model trained in the step (m 1); step (m4) similarity measurement, namely respectively extracting license plate identification vectors and vehicle identification expression characteristic vectors of the image to be analyzed and the image in the vehicle query library, and performing similarity measurement on the license plate identification vectors and the vehicle identification expression characteristic vectors to obtain a vehicle re-identification result; and (m5) visualizing, namely visualizing the re-recognition result obtained in the step (4) in a form which is consistent with the human visual understanding mode. 2. The deep learning-based multi-feature fusion vehicle re-identification method according to claim 1, characterized in that: the step (m1) comprises the following steps: (m1.1) loading a vehicle classification network model (vehicle _ classification) trained in advance as a pre-training model; (m1.2) preprocessing the images rI of the vehicles participating in the trainingiIntercepting each image interest area as the basic input of local feature extraction; (m1.3) realizing interest area pooling according to the interest area position information extracted in the step (m1.2), so as to extract local features containing vehicle-specific information such as interior decoration and environmental protection marks; (m1.4) extracting a framework backbone network according to deep learning, and extracting global vehicle features; simultaneously extracting vehicle attribute features according to the branch network; (m1.5) staged training strategy depends on a plurality of loss functions, and the multi-classification loss functions respectively have local characteristic errors loss1Global feature error loss2And vehicle attribute feature error loss3(ii) a First stage calculation of joint Loss error Union _ Loss1Completing global and local training; and a global error, a local error and a vehicle attribute characteristic error are jointly trained in the second stage, and a joint loss error calculation formula in the stage is detailed as follows: <mrow> <mi>U</mi> <mi>n</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>_</mo> <msub> <mi>Loss</mi> <mn>1</mn> </msub> <mo>=</mo> <mfrac> <mn>1</mn> <mi>n</mi> </mfrac> <munderover> <mo></mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>n</mi> </munderover> <mrow> <mo>(</mo> <msub> <mi></mi> <mn>1</mn> </msub> <msub> <mi>loss</mi> <mn>1</mn> </msub> <mo>+</mo> <msub> <mi></mi> <mn>2</mn> </msub> <msub> <mi>loss</mi> <mn>2</mn> </msub> <mo>)</mo> </mrow> </mrow> <mrow> <mi>U</mi> <mi>n</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>_</mo> <msub> <mi>Loss</mi> <mn>2</mn> </msub> <mo>=</mo> <mfrac> <mn>1</mn> <mi>n</mi> </mfrac> <munderover> <mo></mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>n</mi> </munderover> <mrow> <mo>(</mo> <msub> <mi>loss</mi> <mn>1</mn> </msub> <mo>+</mo> <msub> <mi>loss</mi> <mn>2</mn> </msub> <mo>+</mo> <msub> <mi>loss</mi> <mn>3</mn> </msub> <mo>)</mo> </mrow> </mrow> wherein the weight parameters in the above two formulas are set as lambda according to experiment1=0.2,2=0.8;=0.05,=0.8,=0.15; (m1.6) before the iteration termination condition is met, updating the network model parameters through back propagation of errors in each cycle until the termination condition is met, and obtaining a final feature extraction model. 3. The deep learning-based multi-feature fusion vehicle re-identification method according to claim 1, characterized in that: the step (m2) comprises the following steps: (m2.1) the pre-treatment stage comprises the following steps: (r1) first, the image rI of the vehicle is subjected to Gaussian blur processingiThen it is changed to gIi; (r2) for Gaussian processed image gIiGraying, extracting edge by Sobel operator, and image gIiThen is changed to sIi; (r3) carrying out binarization, corrosion expansion and other mathematical form processing on the obtained edge image, and extracting a contour to obtain input for license plate judgment; (m2.2) the license plate distinguishing stage comprises the following steps: (d1) all the rectangular frames extracted in the preprocessing stage are candidate license plates, the size of the candidate license plates is judged, and the candidate license plates which do not accord with the aspect ratio and the conventional area are deleted from the candidates; (d2) judging the angles of the candidate license plates left after the judgment in the step (d1) by virtue of the rectangular inclined angle, and selecting the candidate license plate with the highest score as the license plate of the current vehicle image by combining the size judgment score in the step (d 1); (m2.3) the license plate content recognition stage comprises the following steps: (i1) carrying out gray scale, binarization and other processing on the recognized license plate so as to segment all characters C in the license plate7={c1,c2,,c7}; (i2) Recognizing the segmented characters by using a trained character recognition BP neural network, carrying out binary coding, and connecting into a license plate identification feature vector; if no license plate exists or the detection fails, occupying by using a special identifier to generate a special feature vector; the first bit of the generated feature vector represents whether the license plate information is successfully acquired, 1 represents success, and 0 represents failure; image vehicle with successfully extracted license plateU (c) for brand identity feature vector1),U(c2),,U(c7) And representing that the license plate identification feature vectors of the images with failed license plate extraction are occupied by 0. 4. The deep learning-based multi-feature fusion vehicle re-identification method according to claim 1, characterized in that: the step (m3) comprises the following steps: (m3.1) extracting global features, local features and vehicle attribute features by using the feature extraction model extracted by the training in the step (m 1); (m3.2) fusing the global features and the local features extracted in the step (m3.1) and the license plate identification feature vector extracted in the step (m2) into a vehicle re-identification feature vector; (m3.3) mapping the vehicle attribute features extracted in the step (m3.1) to corresponding vehicle types and brand information for subsequent similarity measurement and visualization in the step (m 5). 5. The deep learning-based multi-feature fusion vehicle re-identification method according to claim 1, characterized in that: the step (m4) comprises the following steps: (m4.1) identifying whether the license plate identification feature vector extracted in the step (m2) contains license plate information; (m4.2) taking the image pair as similarity measurement input, and when the license plates exist, preferentially judging whether the license plates are matched, and if not, considering the license plates as different vehicles; if the license plates are matched, judging whether attributes such as brands, vehicle types and the like of the matched vehicle feature mapping are consistent, if the attributes are inconsistent, judging the vehicles to be different, and judging the illegal conditions of different vehicles with the same license plate; if the vehicle type is judged to pass through, calculating the distance between the vehicle re-identification feature vectors, determining that the same vehicle is identified when the distance is smaller than a preset threshold value epsilon 1, determining that different vehicles have the same license plate when the distance is larger than the preset threshold value epsilon 1, and marking as an illegal vehicle; but images of vehicles identified as violations store information and serve as input for visual alerts; (m4.3) if one or two pieces of license plate identification feature information do not exist in the input pair, firstly, whether the vehicle attributes of the matched vehicle feature mapping are consistent or not is judged, and if the vehicle attributes are not consistent, the vehicle attributes are considered to be different vehicles; if the two vehicles are consistent, the similarity of the representative features of the vehicle identification is used as a basis for judging whether the two vehicles belong to the same vehicle; and when the similarity is smaller than a preset threshold epsilon 2, entering a to-be-sorted list, and selecting the front N images meeting the social conditions and the physical conditions as final retrieval results. 6. The deep learning-based multi-feature fusion vehicle re-identification method according to claim 1, characterized in that: the step (m5) comprises the following steps: (m5.1) immediately visualizing all dynamic information of the relevant vehicle in the current query library and giving a striking warning prompt in case that the warning information is provided in the step (m 4.2); (m5.2) once visualizing the search result after all similarity measurements are finished, supporting the search result to extend query to the database, and visualizing the query result: picture I to be analyzedAIs the image set IsetA,IsetAThe vehicle attribute information corresponding to the image and the associated information stored in the database can be further visualized by clicking any one retrieval result according to different sorting modes such as similarity, time sequence and the like.",CN107729818_B.txt,"G06K9/00,G06K9/34,G06K9/62",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']","multi-feature fusion vehicle re-identification method based on deep learning The invention discloses a multi-feature fusion vehicle re-identification method based on deep learning. The method comprises five parts of model training, license plate identification, vehicle identification, similarity measurement and visualization. The method comprises the steps that a large-scale vehicle data set is used for model training, and a multi-loss function-phased joint training policyis used for training; license plate identification is carried out on each vehicle image, and a license plate identification feature vector is generated according to the license plate recognition condition; a trained model is used to extract the vehicle descriptive features and vehicle attribute features of an image to be analyzed and an image in a query library, and the vehicle descriptive features and the license plate identification vector are combined with the unique re-identification feature vector of each vehicle image; in the stage of similarity measurement, similarity measurement is carried out on the image to be analyzed and the re-identification feature vector of the image in the query library; and a search result which meets requirements is locked and visualized.",computing; calculating; counting
US2004234136_A1,2004-03-22,2004-11-25,2003-03-24,"SIEMENSSIEMENSCOMANICIU, DORINPELLKOFER, MARTINKOEHLER, THORSTENZHU YING","COMANICIU, DORINPELLKOFER, MARTINKOEHLER, THORSTENZHU YING",33102172,vehicle classification,system and method for vehicle detection and tracking,A system and method for detecting and tracking an object is disclosed. A camera captures a video sequence comprised of a plurality of image frames. A processor receives the video sequence and analyzes each image frame to determine if an object is detected. The processor applies one or more classifiers to an object in each image frame and computes a confidence score based on the application of the one or more classifiers to the object. A database stores the one or more classifiers and vehicle training samples. A display displays the video sequence.,"We claim:1. A method for detecting one or more objects belonging to the same object class comprising the steps of: a). receiving a video sequence comprised of a plurality of image frames; b). applying one or more classifiers to detect components of objects in an image frame in the video sequence; c). computing a confidence score based in part on the response from the one or more component detectors; d). repeating steps b). and c). to detect components of objects belonging to the same object class in additional images frames in the video sequence; and e). accumulating confidence scores from the component detectors to determine if an object is detected.2. The method of claim 1 wherein if accumulated confidence scores indicate high confidence of a presence of an object, the method further comprising the step of: identifying the detected components to be an object of a particular type.3. The method of claim 1 wherein the object class is a vehicle.4. The method of claim 1 further comprising the step of: if an object is detected, outputting a detection signal and object position.5. The method of claim 2 further comprising the steps of: testing geometry constraints on a spatial arrangement of detected components in an image; and applying whole-appearance classifiers an image patch that contains the detected components and which is aligned according to the position of the detected components.6. The method of claim 5 wherein the geometry constraints are derived from camera parameters.7. The method of claim 5 wherein the geometry constraints are derived from object size.8. The method of claim 5 wherein the geometry constraints are derived from a location of an object appearance in the image.9. The method of claim 5 wherein the whole appearance classifiers detect entire or partial object appearance, the entire or partial object appearance being aligned according to positioning of at least two components.10. The method of claim 1 wherein the component classifiers include classifiers for detecting components at multiple scales.11. The method of claim 1 wherein component classifiers are defined by discriminant features and decision rules which are learned through boosted training.12. The method of claim 11 wherein the discriminant features include corners.13. The method of claim 11 wherein the discriminant features include horizontal edges.14. The method of claim 11 wherein the discriminant features include vertical edges.15. The method of claim 11 wherein the discriminant features include horizontal stripes.16. The method of claim 11 wherein the discriminant features include vertical stripes.17. The method of claim 11 wherein the discriminant features include diagonal stripes.18. The method of claim 11 further comprising the step of: performing an online adaptation to adapt a classifier structure to an online pattern.19. The method of claim 18 wherein the step of performing an online adaptation further comprises the step of: applying a dynamic switching strategy to direct the detector to take appropriate weak classifiers as discriminants according to auxiliary information about the online pattern.20. The method of claim 2 wherein the one or more classifiers include overlapping component classifiers.21. The method of claim 20 wherein the overlapping component classifiers comprises four corners representing a rear profile of a vehicle.22. The method of claim 20 wherein the overlapping component classifiers comprises four corners representing a frontal profile of a vehicle.23. The method of claim 20 wherein the overlapping component classifiers comprises four corners representing a side profile of a vehicle.24. The method of claim 21 wherein one of the overlapping component classifiers detects the bottom left corner of a vehicle.25. The method of claim 21 wherein one of the overlapping component classifiers detects the bottom right corner of a vehicle.26. The method of claim 21 wherein one of the overlapping component classifiers detects the top left corner of a vehicle.27. The method of claim 21 wherein one of the overlapping component classifiers detects the top right corner of a vehicle.28. The method of claim 21 wherein positioning of the four corners of the rear profile for a vehicle differ for different classes of vehicles.29. The method of claim 28 wherein a class of vehicle includes sedans.30. The method of claim 28 wherein a class of vehicle includes sports utility vehicles.31. The method of claim 28 wherein a class of vehicle includes vans.32. The method of claim 28 wherein a class of vehicle includes tractor trailers.33. The method of claim 28 wherein a class of vehicle includes trucks.34. The method of claim 21 wherein a distance between any two corners of the vehicle is constrained.35. The method of claim 34 wherein the constraint between any two corners of the vehicle is scaled based on a distance between the vehicle and a camera capturing the video sequence and camera parameters.36. The method of claim 35 wherein an image pyramid of multiple resolutions is used to detect objects of size 2*x, 4*x and so on with the classifier for the size x.37. The method of claim 1 wherein the accumulated confidence scores are inferred from confidence scores across multiple frames using a recursive filter.38. The method of claim 37 wherein when the accumulated confidence score is a linear combination of the confidence scores of multiple component classifiers and the whole-appearance classifiers.39. The method of claim 38 wherein when the confidence score for a principal component classifier is sufficiently high, the confidence score of the remaining component classifiers and the whole-appearance classifier are computed.40. The method of claim 2 wherein if an object is detected the method comprising the step of: tracking the object over subsequent image frames.41. The method of claim 40 wherein the step of tracking the object further comprises the step of: restricting an area of search in each subsequent image frame based on the location of the object in a current image frame.42. The method of claim 40 wherein the step of tracking the object further comprises the step of: determining the optimal classifier scale based on a distance between the object and a camera detecting the object and camera parameters.43. The method of claim 1 wherein the confidence scores of component classifiers are computed in a coarse to fine framework.44. The method of claim 1 wherein detection is performed on an image pyramid of multiple resolutions.45. The method of claim 1 wherein an object class includes pedestrians.46. The method of claim 1 wherein an object class includes bicycles.47. The method of claim 1 wherein an object class includes motorcycles.48. The method of claim 1 wherein object class includes different types of traffic signs.49. A system for detection and tracking an object comprising: a camera for capturing a video sequence comprised of a plurality of image frames; a processor for receiving the video sequence and analyzing each image frame to determine if an object is detected, said processor applying one or more classifiers to detect components of objects in each image frame and computing a confidence score based on the response from the one or more component detectors and the result of additional validation; and a database for storing the one or more classifiers and object training samples50. The method of claim 49 wherein the object class is a vehicle.51. The method of claim 49 wherein the object class is a pedestrian.52. The method of claim 49 wherein the object class is a bicycle.53. The method of claim 49 wherein the object class is a motorbike.54. The method of claim 49 wherein the object class includes different types of traffic signs.55. The system of claim 49 wherein the detected components are determined to be an object if the confidence scores are high.56. The system of claim 55 wherein if an object is detected, the processor outputs a warning signal.57. The system of claim 49 further comprising: a display for displaying the video sequence.58. The system of claim 49 wherein the processor further comprises: means for testing geometry constraints on a spatial arrangement of detected components in an image; and means for applying whole-appearance classifiers an image patch that contains the detected components and which is aligned according to the position of the detected components.59. The system of claim 58 wherein the geometry constraints are derived from camera parameters.60. The system of claim 58 wherein the geometry constraints are derived from object size.61. The system of claim 58 wherein the geometry constraints are derived from a location of an object appearance in the image.62. The system of claim 58 wherein the whole appearance classifiers detect entire or partial object appearance, the entire or partial object appearance being aligned according to positioning of at least two components.63. The system of claim 49 wherein the component classifiers include classifiers for detecting components at multiple scales.64. The system of claim 49 wherein component classifiers are defined by discriminant features and decision rules which are learned through boosted training.65. The system of claim 64 wherein the discriminant features include corners.66. The system of claim 64 wherein the discriminant features include horizontal edges.67. The system of claim 64 wherein the discriminant features include vertical edges.68. The system of claim 64 wherein the discriminant features include horizontal stripes.69. The system of claim 64 wherein the discriminant features include vertical stripes.70. The system of claim 64 wherein the discriminant features include diagonal stripes.71. The system of claim 64 further comprising the step of: performing an online adaptation to adapt a classifier structure to an online pattern.72. The system of claim 71 wherein the step of performing an online adaptation further comprises the step of: applying a dynamic switching strategy to direct the detector to take appropriate weak classifiers as discriminants according to auxiliary information about the online pattern.73. The system of claim 49 wherein the one or more classifiers include overlapping component classifiers.74. The system of claim 73 wherein the overlapping component classifiers comprises four corners representing a rear profile of a vehicle.75. The system of claim 74 wherein one of the overlapping component classifiers detects the bottom left corner of a vehicle.76. The system of claim 74 wherein one of the overlapping component classifiers detects the bottom right corner of a vehicle.77. The system of claim 74 wherein one of the overlapping component classifiers detects the top left corner of a vehicle.78. The system of claim 74 wherein one of the overlapping component classifiers detects the top right corner of a vehicle.79. The system of claim 74 wherein positioning of the four corners of the rear profile for a vehicle differ for different classes of vehicles.80. The system of claim 79 wherein a class of vehicle includes sedans.81. The system of claim 79 wherein a class of vehicle includes sports utility vehicles.82. The system of claim 79 wherein a class of vehicle includes vans.83. The system of claim 79 wherein a class of vehicle includes tractor trailers.84. The system of claim 74 wherein a distance between any two corners of the vehicle is constrained.85. The system of claim 84 wherein the constraint between any two corners of the vehicle is scaled based on a distance between the vehicle and a camera capturing the video sequence as well as camera parameters.86. The system of claim 85 wherein an image pyramid of multiple resolutions is used to detect objects of size 2*x, 4*x and so on with the classifier for the size x.87. The system of claim 49 wherein the accumulated confidence scores is inferred from confidence scores across multiple frames using a recursive filter.88. The system of claim 87 wherein when the accumulated confidence score is a linear combination of the confidence scores of multiple component classifiers and the whole-appearance classifiers.89. The system of claim 87 wherein when the confidence score for a principal component classifier is sufficiently high, the confidence score of the remaining component classifiers and the whole-appearance classifier are computed.90. The system of claim 49 wherein the processor comprises: means for tracking a detected object over subsequent image frames.91. The system of claim 90 wherein tracking means further comprises: means for restricting an area of search in each subsequent image frame based on the location of the object in a current image frame.92. The system of claim 90 wherein the tracking means further comprises: means for determining the optimal classifier scale based on a distance between the object and a camera detecting the object and camera parameters.93. The system of claim 49 wherein the confidence scores of component classifiers are computed in a coarse to fine framework94. The system of claim 49 wherein detection and tracking is performed on an image pyramid of multiple resolutions.95. A method for tracking one or more objects depicted in a video sequence comprising: receiving a video sequence comprised of a plurality of image frames; detecting an object to be tracked in one of the image frames; computing one or more appearance trajectories based on the position of the tracked object to estimate the direction in which the tracked object is traveling; and determining if the object is the tracked object based on the appearance trajectory.96. The method of claim 95 wherein said step of determining if the object is the tracked object further comprises the steps of: applying one or more classifiers to the computed appearance trajectory; computing a confidence score for the computed appearance trajectory; and determining the object to be the tracked object if the confidence score is high.97. The method of claim 95 wherein the computation of the appearance trajectory considers physically well-founded motion constraints.98. The method of claim 95 wherein the trajectories are computed by accumulating detection results over multiple frames.99. The method of claim 98 wherein the computed appearance trajectory determines changes in position, scale, location and aspect conditions in consecutive frames.100. The method of claim 95 wherein the tracked object is a vehicle.101. The method of claim 95 wherein the tracked object is a pedestrian.102. The method of claim 95 wherein the tracked object is a bicycle.103. The method of claim 95 wherein the tracked object is a motorbike.104. The method of claim 95 wherein the tracked object includes different types of traffic signs.",US2004234136_A1.txt,"G06K9/00,G06K9/32,G06K9/62,G06T7/00,G06T7/20",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general', 'image data processing or generation, in general']",system and method for vehicle detection and tracking A system and method for detecting and tracking an object is disclosed. A camera captures a video sequence comprised of a plurality of image frames. A processor receives the video sequence and analyzes each image frame to determine if an object is detected. The processor applies one or more classifiers to an object in each image frame and computes a confidence score based on the application of the one or more classifiers to the object. A database stores the one or more classifiers and vehicle training samples. A display displays the video sequence.,computing; calculating; counting
US2018328756_A1,2018-07-13,2018-11-15,2016-11-14,QUALCOMM,"FORUTANPOUR, BIJANKIES, JONATHAN",62107717,road monitoring,systems and methods for trip planning,"A method performed by an electronic device is described. The method includes obtaining one or more trip objectives. The method also includes obtaining one or more evaluation bases. The method further includes identifying an association between at least one site and the one or more trip objectives. The method additionally includes obtaining sensor data from the at least one site. The sensor data includes at least image data. The method also includes performing analysis on the image data to determine dynamic destination information corresponding to the at least one site. The method further includes performing trip planning based on the dynamic destination information, the one or more trip objectives, and the one or more evaluation bases. The method additionally includes providing one or more suggested routes based on the trip planning.","1. A method performed by an electronic device, the method comprising: obtaining one or more trip objectives; providing a first suggested route based on the one or more trip objectives and based on a first trip planning analysis, the first trip planning analysis being based on first dynamic destination information that is based on first image data from a first site, second dynamic destination information that is based on second image data from a second site, and a weighting applied to the first dynamic destination information or the second dynamic destination information; receiving feedback from a user indicating non-acceptance of the first suggested route; and providing a second suggested route based on the one or more trip objectives and based on a second trip planning analysis, the second trip planning analysis being based on the first dynamic destination information, the second dynamic destination information, and an adjusted weighting applied to the first dynamic destination information or the second dynamic destination information.2. The method of claim 1, wherein the first dynamic destination information includes a first aspect and the second dynamic destination information includes a second aspect of a same type as the first aspect, and wherein the adjusted weighting is applied to the first aspect or to the second aspect.3. The method of claim 1, wherein the first dynamic destination information includes a first aspect and the second dynamic destination information includes a second aspect of a different type from the first aspect, and wherein the adjusted weighting is applied to the first aspect or to the second aspect.4. The method of claim 1, wherein the first dynamic destination information includes a first plurality of aspects and the second dynamic destination information includes a second plurality of aspects and wherein the adjusted weighting is applied to at least one of the first plurality of aspects or at least one of the second plurality of aspects.5. The method of claim 1, wherein machine learning is performed to determine the adjusted weighting based on the feedback from the user.6. The method of claim 5, wherein the machine learning is performed by the electronic device.7. The method of claim 5, wherein the machine learning is performed by a server.8. The method of claim 1, wherein trip planning training is performed based on whether the second suggested route is accepted.9. The method of claim 1, wherein the first trip planning analysis and the second trip planning analysis are performed by the electronic device.10. The method of claim 1, wherein the first trip planning analysis and the second trip planning analysis are performed by a server.11. An electronic device, comprising: a processor; a memory in electronic communication with the processor; instructions stored in the memory, the instructions being executable to: obtain one or more trip objectives; provide a first suggested route based on the one or more trip objectives and based on a first trip planning analysis, the first trip planning analysis being based on first dynamic destination information that is based on first image data from a first site, second dynamic destination information that is based on second image data from a second site, and a weighting applied to the first dynamic destination information or the second dynamic destination information; receive feedback from a user indicating non-acceptance of the first suggested route; and provide a second suggested route based on the one or more trip objectives and based on a second trip planning analysis, the second trip planning analysis being based on the first dynamic destination information, the second dynamic destination information, and an adjusted weighting applied to the first dynamic destination information or the second dynamic destination information.12. The electronic device of claim 11, wherein the first dynamic destination information includes a first aspect and the second dynamic destination information includes a second aspect of a same type as the first aspect, and wherein the adjusted weighting is applied to the first aspect or to the second aspect.13. The electronic device of claim 11, wherein the first dynamic destination information includes a first aspect and the second dynamic destination information includes a second aspect of a different type from the first aspect, and wherein the adjusted weighting is applied to the first aspect or to the second aspect.14. The electronic device of claim 11, wherein the first dynamic destination information includes a first plurality of aspects and the second dynamic destination information includes a second plurality of aspects and wherein the adjusted weighting is applied to at least one of the first plurality of aspects or at least one of the second plurality of aspects.15. The electronic device of claim 11, wherein machine learning is performed to determine the adjusted weighting based on the feedback from the user.16. The electronic device of claim 15, wherein the instructions are executable to perform the machine learning.17. The electronic device of claim 15, wherein the machine learning is performed by a server.18. The electronic device of claim 11, wherein trip planning training is performed based on whether the second suggested route is accepted.19. The electronic device of claim 11, wherein the instructions are executable to perform the first trip planning analysis and the second trip planning analysis.20. The electronic device of claim 11, wherein the first trip planning analysis and the second trip planning analysis are performed by a server.21. A non-transitory tangible computer-readable medium storing computer-executable code, comprising: code for causing an electronic device to obtain one or more trip objectives; code for causing the electronic device to provide a first suggested route based on the one or more trip objectives and based on a first trip planning analysis, the first trip planning analysis being based on first dynamic destination information that is based on first image data from a first site, second dynamic destination information that is based on second image data from a second site, and a weighting applied to the first dynamic destination information or the second dynamic destination information; code for causing the electronic device to receive feedback from a user indicating non-acceptance of the first suggested route; and code for causing the electronic device to provide a second suggested route based on the one or more trip objectives and based on a second trip planning analysis, the second trip planning analysis being based on the first dynamic destination information, the second dynamic destination information, and an adjusted weighting applied to the first dynamic destination information or the second dynamic destination information.22. The computer-readable medium of claim 21, wherein the first dynamic destination information includes a first aspect and the second dynamic destination information includes a second aspect of a same type as the first aspect, and wherein the adjusted weighting is applied to the first aspect or to the second aspect.23. The computer-readable medium of claim 21, wherein the first dynamic destination information includes a first aspect and the second dynamic destination information includes a second aspect of a different type from the first aspect, and wherein the adjusted weighting is applied to the first aspect or to the second aspect.24. The computer-readable medium of claim 21, wherein the first dynamic destination information includes a first plurality of aspects and the second dynamic destination information includes a second plurality of aspects and wherein the adjusted weighting is applied to at least one of the first plurality of aspects or at least one of the second plurality of aspects.25. The computer-readable medium of claim 21, wherein machine learning is performed to determine the adjusted weighting based on the feedback from the user.26. An apparatus, comprising: means for obtaining one or more trip objectives; means for providing a first suggested route based on the one or more trip objectives and based on a first trip planning analysis, the first trip planning analysis being based on first dynamic destination information that is based on first image data from a first site, second dynamic destination information that is based on second image data from a second site, and a weighting applied to the first dynamic destination information or the second dynamic destination information; means for receiving feedback from a user indicating non-acceptance of the first suggested route; and means for providing a second suggested route based on the one or more trip objectives and based on a second trip planning analysis, the second trip planning analysis being based on the first dynamic destination information, the second dynamic destination information, and an adjusted weighting applied to the first dynamic destination information or the second dynamic destination information.27. The apparatus of claim 26, wherein the first dynamic destination information includes a first aspect and the second dynamic destination information includes a second aspect of a same type as the first aspect, and wherein the adjusted weighting is applied to the first aspect or to the second aspect.28. The apparatus of claim 26, wherein the first dynamic destination information includes a first aspect and the second dynamic destination information includes a second aspect of a different type from the first aspect, and wherein the adjusted weighting is applied to the first aspect or to the second aspect.29. The apparatus of claim 26, wherein the first dynamic destination information includes a first plurality of aspects and the second dynamic destination information includes a second plurality of aspects and wherein the adjusted weighting is applied to at least one of the first plurality of aspects or at least one of the second plurality of aspects.30. The apparatus of claim 26, wherein machine learning is performed to determine the adjusted weighting based on the feedback from the user.",US2018328756_A1.txt,"G01C21/20,G01C21/34,G01C21/36,G06Q10/04","{'measuring; testing', 'computing; calculating; counting'}","['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for']","systems and methods for trip planning A method performed by an electronic device is described. The method includes obtaining one or more trip objectives. The method also includes obtaining one or more evaluation bases. The method further includes identifying an association between at least one site and the one or more trip objectives. The method additionally includes obtaining sensor data from the at least one site. The sensor data includes at least image data. The method also includes performing analysis on the image data to determine dynamic destination information corresponding to the at least one site. The method further includes performing trip planning based on the dynamic destination information, the one or more trip objectives, and the one or more evaluation bases. The method additionally includes providing one or more suggested routes based on the trip planning.",measuring; testing computing; calculating; counting
US2018189581_A1,2018-03-02,2018-07-05,2010-06-07,AFFECTIVA,"EL KALIOUBY, RANAMAVADATI, SEYEDMOHAMMADTURCOT, PANU JAMESMAHMOUD, ABDELRAHMAN N.",62711758,vehicle classification,vehicle manipulation using convolutional image processing,"Disclosed embodiments provide for vehicle manipulation using convolutional image processing. The convolutional image processing is accomplished using a computer, where the computer can include a multilayered analysis engine. The multilayered analysis engine can include a convolutional neural network (CNN). The computer is initialized for convolutional processing. A plurality of images is obtained using an imaging device within a first vehicle. A multilayered analysis engine is trained using the plurality of images. The multilayered analysis engine includes multiple layers that include convolutional layers hidden layers. The multilayered analysis engine is used for cognitive state analysis. The evaluating provides a cognitive state analysis. Further images are analyzed using the multilayered analysis engine. The further images include facial image data from one or more persons present in a second vehicle. Voice data is collected to augment the cognitive state analysis. Manipulation data is provided to the second vehicle based on the evaluating.","1. A computer-implemented method for vehicle manipulation comprising: initializing a computer for convolutional processing; obtaining, using an imaging device within a first vehicle, a plurality of images of an occupant of the first vehicle; training, on the computer initialized for convolutional processing, a multilayered analysis engine using the plurality of images, wherein the multilayered analysis engine includes multiple layers that include one or more convolutional layers and one or more hidden layers, and wherein the multilayered analysis engine is used for cognitive state analysis; evaluating further images, using the multilayered analysis engine, wherein the further images include facial image data from one or more persons present in a second vehicle; and providing manipulation data to the second vehicle based on the evaluating.2. The method of claim 1 wherein the evaluating provides a cognitive state analysis.3. The method of claim 2 further comprising collecting voice data from the one or more persons present in the second vehicle and augmenting the cognitive state analysis.4. The method of claim 3 further comprising mapping cognitive state data from the cognitive state analysis to location data along a vehicle travel route.5. The method of claim 4 further comprising updating information about the vehicle travel route based on the cognitive state data.6. The method of claim 1 further comprising manipulating the second vehicle based on the manipulation data.7. The method of claim 1 wherein the evaluating includes: analyzing pixels within the further images to identify a facial portion; and identifying a facial expression based on the facial portion.8. The method of claim 1 further comprising generating a cognitive state profile for one or the one or more persons present in the second vehicle.9. The method of claim 1 wherein a last layer within the multiple layers provides output indicative of cognitive state.10. The method of claim 9 further comprising tuning the last layer within the multiple layers for a particular cognitive state.11. The method of claim 1 wherein the multilayered analysis engine further includes a pooling layer.12. The method of claim 1 wherein the training comprises assigning weights to inputs on one or more layers within the multilayered analysis engine.13. The method of claim 12 wherein the assigning weights is accomplished during a feed-forward pass through the multilayered analysis engine.14. The method of claim 13 wherein the weights are updated during a backpropagation process through the multilayered analysis engine.15. The method of claim 1 wherein the occupant is a passenger within the vehicle.16. The method of claim 1 wherein the occupant is a driver within the vehicle.17. The method of claim 1 wherein the second vehicle is an autonomous vehicle.18. The method of claim 1 wherein the second vehicle is a semi-autonomous vehicle.19. The method of claim 1 further comprising performing supervised learning as part of the training by using a set of images, from the plurality of images, that have been labeled for cognitive states.20. The method of claim 1 further comprising performing unsupervised learning as part of the training.21. The method of claim 1 wherein a layer from the multiple layers is fully connected.22. The method of claim 1 further comprising learning image descriptors, as part of the training, for emotional content.23. The method of claim 22 wherein the image descriptors are identified based on a temporal co-occurrence with an external stimulus.24. The method of claim 1 further comprising training an emotion classifier, as part of the training, for emotional content.25. The method of claim 1 wherein the training of the multilayered analysis engine comprises deep learning.26. The method of claim 1 wherein the multilayered analysis engine comprises a convolutional neural network.27. The method of claim 1 further comprising retraining the multilayered analysis engine using a second plurality of images.28. The method of claim 27 wherein the retraining updates weights on a subset of layers within the multilayered analysis engine.29. The method of claim 28 wherein the subset of layers is a single layer within the multilayered analysis engine.30. The method of claim 1 further comprising inferring a cognitive state based on emotional content within a face detected within the facial image data, wherein the cognitive state includes of one or more of drowsiness, fatigue, distraction, impairment, fear, sadness, stress, happiness, anger, frustration, confusion, disappointment, hesitation, cognitive overload, focusing, engagement, attention, boredom, exploration, confidence, trust, delight, disgust, skepticism, doubt, satisfaction, excitement, laughter, calmness, curiosity, humor, depression, envy, sympathy, embarrassment, poignancy, or mirth.31. A computer program product embodied in a non-transitory computer readable medium for vehicle manipulation, the computer program product comprising code which causes one or more processors to perform operations of: initializing a computer for convolutional processing; obtaining, using an imaging device within a first vehicle, a plurality of images of an occupant of the first vehicle; training, on the computer initialized for convolutional processing, a multilayered analysis engine using the plurality of images, wherein the multilayered analysis engine includes multiple layers that include one or more convolutional layers and one or more hidden layers, and wherein the multilayered analysis engine is used for cognitive state analysis; evaluating further images, using the multilayered analysis engine, wherein the further images include facial image data from one or more persons present in a second vehicle; and providing manipulation data to the second vehicle based on the evaluating.32. A computer system for vehicle manipulation comprising: a memory which stores instructions; one or more processors attached to the memory wherein the one or more processors, when executing the instructions which are stored, are configured to: initialize a computer for convolutional processing; obtain, using an imaging device within a first vehicle, a plurality of images of an occupant of the first vehicle; train, on the computer initialized for convolutional processing, a multilayered analysis engine using the plurality of images, wherein the multilayered analysis engine includes multiple layers that include one or more convolutional layers and one or more hidden layers, and wherein the multilayered analysis engine is used for cognitive state analysis; evaluate further images, using the multilayered analysis engine, wherein the further images include facial image data from one or more persons present in a second vehicle; and provide manipulation data to the second vehicle based on the evaluating.",US2018189581_A1.txt,"G01C21/34,G06K9/00,G06K9/62,G06K9/66","{'measuring; testing', 'computing; calculating; counting'}","['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']","vehicle manipulation using convolutional image processing Disclosed embodiments provide for vehicle manipulation using convolutional image processing. The convolutional image processing is accomplished using a computer, where the computer can include a multilayered analysis engine. The multilayered analysis engine can include a convolutional neural network (CNN). The computer is initialized for convolutional processing. A plurality of images is obtained using an imaging device within a first vehicle. A multilayered analysis engine is trained using the plurality of images. The multilayered analysis engine includes multiple layers that include convolutional layers hidden layers. The multilayered analysis engine is used for cognitive state analysis. The evaluating provides a cognitive state analysis. Further images are analyzed using the multilayered analysis engine. The further images include facial image data from one or more persons present in a second vehicle. Voice data is collected to augment the cognitive state analysis. Manipulation data is provided to the second vehicle based on the evaluating.",measuring; testing computing; calculating; counting
CN110705370_A,2019-09-06,2020-01-17,2019-09-06,CHINA PINGAN PROPERTY INSURANCE COMPANY,XIAO SHUANG,69195117,road monitoring,"road condition recognition method, device and equipment based on deep learning and storage medium","The invention discloses a road condition recognition method, device and equipment based on deep learning and a storage medium. The method comprises the following steps: collecting a road condition monitoring video in a starting or driving process of a vehicle in real time through a camera of the vehicle if the vehicle is in a starting or driving state, so as to obtain the image data of the road condition monitoring video; inputting the image data into a preset three-dimensional convolutional neural network model as input data so as to judge whether dangerous road conditions exist in the starting or driving process of the vehicle or not; and if the dangerous road condition exists in the starting or driving process of the vehicle, outputting preset reminding information to remind a driver ofthe vehicle. Based on an intelligent decision-making mode, the technical problem that the probability of traffic accidents is too high due to the fact that dangerous road conditions of vehicles are not effectively recognized and reminded in the prior art is solved.","1. a road condition identification method based on deep learning is characterized by comprising the following steps: acquiring the running distance and the shaking distance of the vehicle at intervals of a preset time period by a preset roadside unit to judge whether the vehicle is in a starting or running state; if the vehicle is in a starting or driving state, acquiring a road condition monitoring video in the starting or driving process of the vehicle in real time through a camera of the vehicle so as to acquire image data of the road condition monitoring video; inputting the image data serving as input data into a preset three-dimensional convolutional neural network model to judge whether dangerous road conditions exist in the vehicle starting or driving process; and if the vehicle is started or dangerous road conditions exist in the driving process, outputting preset reminding information to remind a driver of the vehicle. 2. the deep learning-based traffic condition recognition method as claimed in claim 1, wherein the deep learning-based traffic condition recognition method is applied to a deep learning-based traffic condition recognition device, the deep learning-based traffic condition recognition device comprises a cpu, the step of inputting the image data into a preset three-dimensional convolutional neural network model as input data to judge whether dangerous road conditions exist in the vehicle starting or driving process comprises the following steps: reading the target processing performance of the cpu, and determining a target processing size corresponding to the target processing performance according to a preset incidence relation between the processing performance of the cpu and the single maximum processing size of the image block in the image data; and cutting the image blocks in the image data according to the target processing size to obtain processed image data. 3. the deep learning-based road condition recognition method as claimed in claim 2, wherein the step of inputting the processed image data as input data into a preset three-dimensional convolutional neural network model to determine whether there is a dangerous road condition during the vehicle starting or driving process comprises: inputting the processed image data serving as input data into a preset three-dimensional convolution neural network model so as to perform convolution and pooling alternative processing on the input data for preset times to obtain an initial processing result; and comparing the initial processing result with a preset threshold value to judge whether dangerous road conditions exist in the starting or driving process of the vehicle. 4. the deep learning-based road condition recognition method as claimed in claim 3, wherein the step of inputting the processed image data as input data into a preset three-dimensional convolutional neural network model to perform convolution and pooling alternative processing on the input data for a preset number of times to obtain an initial processing result comprises: acquiring a plurality of identification factors aiming at the dangerous road condition in the preset three-dimensional convolutional neural network model, and selecting identification factors to be processed from the identification factors; for each identification factor to be processed, the following steps are carried out: acquiring a plurality of preset identification image characteristics of the identification factor to be processed and weight matrixes respectively corresponding to the preset identification image characteristics; performing filtering convolution processing on the processed image data according to the preset identification image characteristics and the weight matrix to obtain a convolution processing result; performing pooling treatment on the convolution treatment result to obtain a pooling treatment result; and performing convolution and pooling alternative processing on the pooling processing result again for corresponding times according to the preset times to obtain an initial processing result. 5. the deep learning-based road condition identification method according to claim 4, wherein the deep learning-based road condition identification device comprises a gps sub-device, and the step of obtaining a plurality of identification factors for the dangerous road condition in the preset three-dimensional convolutional neural network model and selecting the identification factor to be processed from the identification factors comprises: acquiring a plurality of identification factors aiming at the dangerous road condition in the preset three-dimensional convolutional neural network model; acquiring the current starting or running road section information of the vehicle according to the gps sub-equipment so as to judge whether the vehicle is in an urban road section; and if the vehicle is in the road section of the urban area, selecting a road sign identification factor, a pedestrian identification factor and an animal identification factor from the multiple identification factors as the identification factors to be processed. 6. the deep learning-based road condition recognition method as claimed in claim 4, wherein the step of performing pooling processing on the convolution processing result to obtain a pooled processing result comprises: dividing the convolution processing result into a plurality of image matrixes with the same size and preset pixel size; obtaining an average pixel value in the image matrix with the preset pixel size, and replacing the image matrix with the average pixel value to obtain a new image matrix; and setting the new image matrix as the pooling processing result. 7. a road condition recognition method based on deep learning as claimed in claim 3, wherein the step of comparing the initial processing result with a preset threshold to determine whether there is a dangerous road condition during the vehicle starting or driving process comprises: comparing the initial processing result with a preset threshold value; if the initial processing result is greater than or equal to the preset classification threshold value, judging that dangerous road conditions exist in the vehicle starting or driving process; and if the initial processing result is smaller than the preset classification threshold value, judging that no dangerous road condition exists in the vehicle starting or driving process. 8. the utility model provides a road conditions recognition device based on degree of depth study which characterized in that, road conditions recognition device based on degree of depth study includes: the system comprises an acquisition module, a display module and a control module, wherein the acquisition module is used for acquiring road condition monitoring videos in the starting or driving process of a vehicle in real time through a camera of the vehicle so as to acquire image data of the monitoring videos; the judging module is used for inputting the image data serving as input data into a preset three-dimensional convolutional neural network model so as to judge whether dangerous road conditions exist in the starting or driving process of the vehicle; and the output module is used for outputting preset reminding information to remind a driver of the vehicle if dangerous road conditions exist in the starting or driving process of the vehicle. 9. the deep learning-based road condition recognition device is characterized by comprising: a memory, a processor, a communication bus and a deep learning-based road condition identification program stored on the memory, the communication bus is used for realizing communication connection between the processor and the memory; the processor is configured to execute the deep learning-based traffic condition identification program to implement the steps of the deep learning-based traffic condition identification method according to any one of claims 1 to 7. 10. a storage medium, wherein the storage medium stores a deep learning-based traffic condition recognition program, and the deep learning-based traffic condition recognition program, when executed by a processor, implements the steps of the deep learning-based traffic condition recognition method according to any one of claims 1 to 7.",CN110705370_A.txt,"G06K9/00,G06N3/04,G06N3/08",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models']","road condition recognition method, device and equipment based on deep learning and storage medium The invention discloses a road condition recognition method, device and equipment based on deep learning and a storage medium. The method comprises the following steps: collecting a road condition monitoring video in a starting or driving process of a vehicle in real time through a camera of the vehicle if the vehicle is in a starting or driving state, so as to obtain the image data of the road condition monitoring video; inputting the image data into a preset three-dimensional convolutional neural network model as input data so as to judge whether dangerous road conditions exist in the starting or driving process of the vehicle or not; and if the dangerous road condition exists in the starting or driving process of the vehicle, outputting preset reminding information to remind a driver ofthe vehicle. Based on an intelligent decision-making mode, the technical problem that the probability of traffic accidents is too high due to the fact that dangerous road conditions of vehicles are not effectively recognized and reminded in the prior art is solved.",computing; calculating; counting
US2018157957_A1,2018-01-18,2018-06-07,2015-07-23,CAMLIN ITALYTOYOTA MOTOR EUROPE,"AMBECK-MADSEN, JONASASCARI, LUCABACCHINI, ALESSANDROMASTROLEO, MARCELLOMUSSI, LUCARATEAU, AYMERICSASSI, FEDERICOUGOLOTTI, ROBERTO",53758203,road monitoring,system and computer-based method for simulating a human-like control behaviour in an environmental context,"A computer-based method for simulating a human-like decision in an environmental context, comprising: capturing environmental data with at least one sensor, realising a computer based method for realising a bi-directional compression of high dimensional data by compressing the data into a lower-dimensional map, if environmental data are captured during a learning phase of a computer-based model, evaluate the map of compressed data by determining the quality of the map by how well it separates data with different properties, the captured data corresponding to known pre-recorded data that have been pre-evaluated, if environmental data are captured after the learning phase, add new point to the compressed data and generate a signal indicating which human-like decision to use to correspond to the state of the operator.","1. A computer based method for realising a bi-directional compression of high dimensional data by compressing the data into a lower dimensional map, the method comprising a reception of data with a first dimension greater than one captured and transmitted by at least one sensor and a compression of the data or attributes derived from the data into compressed data with a second dimension lower than the first dimension, wherein, for two metric spaces X=(x,x) and Y=(y,y) with x a first space which dimension is greater or equal than the dimension of a second space y, x a first metric on the first space x, y a second metric on the second space y, the compression is realized by mapping pairs of points formed between a first set of n points P belonging to the first space x and a second set of n points Q belonging to the second space y, each point of the second set of n points Q being paired with a point of the first set of points P through the following equation:description=""In-line Formulae"" end=""lead""?Q=argmintynd(A1(x(P)),A2(y(t))) (Equation 1)description=""In-line Formulae"" end=""tail""? where x(P) is a first symmetric distance matrix between points in P, x(Q) is a second symmetric distance matrix between points in Q, and d(.,.) is a symmetric distance function on the space of square matrices, while A1 and A2 are any two functions holding the following properties: A:R+[0, 1], A(0)=1, limxA(z)=0, A(z1)>A(z2) if and only if z1<z2.2. The computer based method of claim 1, further comprising an extraction of attributes from at least one data before initiating the bi-directional compression.3. The computer based method according to claim 1, comprising a compression of a new point m belonging to the first space x on the second space y by solving the following equation:description=""In-line Formulae"" end=""lead""?{tilde over (y)}=argmintynd(A1(x(P,m)),A2(y(Q,t))) (Equation 2)description=""In-line Formulae"" end=""tail""? and a decompression of a new point t belonging to the second space y on the first space x by solving the following equation:description=""In-line Formulae"" end=""lead""?{tilde over (x)}=argminmxnd(A1(x(P,m)),A2(y(Q,t))) (Equation 3)description=""In-line Formulae"" end=""tail""? where x(P,m) and y(Q,t) are the distance vectors between the points of BiMap and the new points in the corresponding respective spaces, BiMap being a correspondence between points of the first space x and points of the second space y represented by equation 2 and equation 3, which Q points are calculated from points P by solving equation 1.4. A computer based method for simulating a human-like decision in an environmental context, comprising: capturing environmental data with at least one sensor, realising a computer based method for realising a bi-directional compression of high dimensional data by compressing the data into lower dimensional map according to claim 1 with the environmental data, if environmental data are captured during a learning phase of a computer-based model, evaluating the map of compressed data by determining the quality of the map by how well it separates data with different properties, the captured data corresponding to known pre-recorded data that have been pre-evaluated, if environmental data are captured after the learning phase, adding new points to the compressed data and generating a signal indicating which human-like decision to use to correspond to the state of the operator.5. The computer based method according to claim 4, comprising pre-processing the environmental data with at least one filter before realising the computer based method for realising a bi-directional compression of data, the computer based method for realising a bi-directional compression of data receiving as inputs filtered environmental data.6. The computer based method according to claim 4, wherein the environmental data comprise at least one sample of raw sound recorded inside a car's cockpit with at least one microphone, and the method further comprises inputting the environmental data into auditory models before pre-processing with at least one filter the data outputted by the auditory models deliver a plurality of different versions of the environmental data, the pre-processing being realized over every version of the environmental data.7. A system for simulating a human-like behaviour in an environmental context comprising at least one sensor to capture environmental data, and an architecture with a memory for interacting with dynamic behaviours of a tool and an operator, characterized in that architecture is configured to process the steps of the computer-based method for simulating a human-like decision in an environmental context according to claim 4.8. The system according to claim 7, wherein the memory of the architecture is an artificial memory, the architecture being a first neural network having structures and mechanisms for abstraction, generalisation and learning, the first neural network implementation defining the architecture comprising an artificial hierarchical memory system, comprising a receiving port configured to receive data generated by sensors (on the tool), one or more first nodes configured to learn and recognize frequently occurring input attributes and sequences received from the receiving port, the one or more first nodes forming a second neural network comprising neurons as components and edges connecting two or more of the neurons in a graph, and an output port configured to output data constructed by the architecture and associated with the behaviours, whereby each of the one or more or essentially all of the first nodes is adapted for time series analysis, and comprises components connected in a topological graph or temporal graph.9. The system according to claim 7, further comprising a pre-processing filter to apply a set of different filters to the environmental data captured by the at least one sensor.10. The system according to claim 7, wherein the environmental data comprise at least one sample of raw sound recorded inside a car's cockpit with a noise sensor, and the system further comprises auditory models configured to be applied to the recorded environmental data before the filtering pre-processing to output a plurality of different versions of the environmental data, the filtering pre-processing being realized over every version of the environmental data.11. A computer program product having software to realize the computer based method according to claim 4, which when executed on a processing engine, can contain code segments that provide a Perceptual/Cognitive architecture with artificial memory for interacting with dynamic behaviours of a tool and an operator, wherein the Perceptual/Cognitive architecture is based on a Memory Prediction Framework implementation having structures and mechanisms for abstraction and generalization and optionally learning, the memory Prediction Framework implementation comprising an enhanced artificial hierarchical memory system.12. An automotive vehicle comprising an electronic control unit including a system for simulating a human-like decision in an environmental context according to claim 7.",US2018157957_A1.txt,"G06N3/00,G06N3/04,G06N99/00,G10L25/27","{'computing; calculating; counting', 'musical instruments; acoustics'}","['computing arrangements based on specific computational models', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models', 'speech analysis or synthesis; speech recognition; speech or voice processing; speech or audio coding or decoding']","system and computer-based method for simulating a human-like control behaviour in an environmental context A computer-based method for simulating a human-like decision in an environmental context, comprising: capturing environmental data with at least one sensor, realising a computer based method for realising a bi-directional compression of high dimensional data by compressing the data into a lower-dimensional map, if environmental data are captured during a learning phase of a computer-based model, evaluate the map of compressed data by determining the quality of the map by how well it separates data with different properties, the captured data corresponding to known pre-recorded data that have been pre-evaluated, if environmental data are captured after the learning phase, add new point to the compressed data and generate a signal indicating which human-like decision to use to correspond to the state of the operator.",computing; calculating; counting musical instruments; acoustics
US2017083794_A1,2015-09-18,2017-03-23,2015-09-18,FORD GLOBAL TECHNOLOGIES,"SAEGER, MARTINMICKS, ASHLEY ELIZABETHNALLAPA, VENKATAPATHI RAJUBLUE, DOUGLAS",57288607,road monitoring,"virtual, road-surface-perception test bed","A method for testing the performance of one or more anomaly-detection algorithms. The method may include obtaining sensor data output by a virtual sensor modeling the behavior of an image sensor. The sensor data may correspond to a time when the virtual sensor was sensing a virtual anomaly defined within a virtual road surface. One or more algorithms may be applied to the sensor data to produce at least one perceived dimension of the virtual anomaly. Thereafter, the performance of the one or more algorithms may be quantified by comparing the at least one perceived dimension to at least one actual dimension of the virtual anomaly as defined in the virtual road surface.","1. A method comprising: obtaining, by a computer system, sensor data output by a virtual sensor modeling the behavior of an image sensor while the virtual sensor is sensing a virtual anomaly defined within a virtual road surface; producing, by one or more algorithms applied by the computer system to the sensor data, at least one perceived dimension of the virtual anomaly; and quantifying, by the computer system, performance of the one or more algorithms by comparing the at least one perceived dimension to at least one actual dimension of the virtual anomaly as defined in the virtual road surface.2. The method of claim 1, wherein the image sensor is selected from the group consisting of a camera, a laser scanner, and a radar device.3. The method of claim 2, further comprising obtaining, by the computer system, ground truth data comprising the at least one actual dimension.4. The method of claim 3, further comprising using, by the computer system, the sensor data, the ground truth data, and supervised learning techniques to improve the performance of the one or more algorithms.5. The method of claim 4, wherein the virtual anomaly is selected from the group consisting of a virtual pot hole, a virtual speed bump, a virtual manhole cover, and virtual rough terrain.6. The method of claim 1, wherein the obtaining the sensor data comprises: traversing, by the computer system, the virtual sensor over the virtual road surface in a simulation; manipulating, by the computer system during the traversing, a point of view of the virtual sensor with respect to the virtual road surface; and recording, by the computer system, the sensor data as it is output by the virtual sensor during the traversing.7. The method of claim 6, wherein the manipulating comprises changing an angle of incidence of the virtual sensor with respect to the virtual road surface.8. The method of claim 7, wherein the manipulating further comprises changing a spacing in a normal direction between the virtual road surface and the virtual sensor.9. The method of claim 8, wherein the manipulating further comprises moving the virtual sensor with respect to the virtual road surface as dictated by a vehicle-motion model modeling motion of a vehicle carrying the virtual sensor and driving on the virtual road surface.10. The method of claim 9, wherein the image sensor is selected from the group consisting of a camera, a laser scanner, and a radar device.11. The method of claim 10, further comprising obtaining, by the computer system, ground truth data comprising the at least one actual dimension.12. The method of claim 11, further comprising using, by the computer system, the sensor data, the ground truth data, and supervised learning techniques to improve the performance of the one or more algorithms.13. The method of claim 12, wherein the virtual anomaly is selected from the group consisting of a virtual pot hole, a virtual speed bump, a virtual manhole cover, and virtual rough terrain.14. A method for testing the performance of one or more anomaly-detection algorithms, the method comprising: obtaining, by a computer system, sensor data output by a virtual sensor modeling the behavior of an image sensor while the virtual sensor is sensing a virtual anomaly defined within a virtual road surface; producing, by one or more algorithms applied by the computer system to the sensor data, at least one perceived dimension of the virtual anomaly; obtaining, by a computer system, ground truth data defining exact dimensions of the virtual anomaly as defined within the virtual road surface; quantifying, by the computer system, performance of the one or more algorithms by comparing the at least one perceived dimension to at least one actual dimension of the exact dimensions.15. The method of claim 14, wherein the obtaining the sensor data comprises: executing, by a computer system, a simulation comprising traversing the virtual sensor over the virtual road surface, and moving, during the traversing, the virtual sensor with respect to the virtual road surface as dictated by a vehicle-motion model modeling motion of a vehicle driving on the virtual road surface while carrying the virtual sensor; and recording, by the computer system, the sensor data as it is output by the virtual sensor during the traversing.16. The method of claim 15, wherein the moving comprises: changing an angle of incidence of the virtual sensor with respect to the virtual road surface; and changing a spacing in a normal direction between the virtual road surface and the virtual sensor.17. The method of claim 16, wherein the image sensor is selected from the group consisting of a camera, a laser scanner, and a radar device.18. The method of claim 17, further comprising using, by the computer system, the sensor data, the ground truth data, and supervised learning techniques to improve the performance of the one or more algorithms.19. The method of claim 18, wherein the virtual anomaly is selected from the group consisting of a virtual pot hole, a virtual speed bump, a virtual manhole cover, and virtual rough terrain.20. A computer system comprising: one or more processors; memory operably connected to the one or more processors; and the memory storing a virtual driving environment programmed to include a plurality of virtual anomalies, a first software model programmed to model a sensor, a second software model programmed to model a vehicle, a simulation module programmed to use the virtual driving environment, the first software model, and the second software model to produce an output modeling what would be output by the sensor had the sensor been mounted to the vehicle and the vehicle had driven on an actual driving environment matching the virtual driving environment, and a perception module programmed to apply one or more algorithms to the output to produce perceived dimensions characterizing each virtual anomaly of the plurality of virtual anomalies.21. A method comprising: obtaining, by a computer system, sensor data output by a virtual sensor sensing a virtual anomaly in a virtual driving environment; producing, by an algorithm applied by the computer system to the sensor data, a perceived dimension of the virtual anomaly; and quantifying, by the computer system, performance of the algorithm by comparing the perceived dimension to an actual dimension of the virtual anomaly as defined in the virtual driving environment.",US2017083794_A1.txt,"G06K9/00,G06K9/62,G06N99/00",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models']","virtual, road-surface-perception test bed A method for testing the performance of one or more anomaly-detection algorithms. The method may include obtaining sensor data output by a virtual sensor modeling the behavior of an image sensor. The sensor data may correspond to a time when the virtual sensor was sensing a virtual anomaly defined within a virtual road surface. One or more algorithms may be applied to the sensor data to produce at least one perceived dimension of the virtual anomaly. Thereafter, the performance of the one or more algorithms may be quantified by comparing the at least one perceived dimension to at least one actual dimension of the virtual anomaly as defined in the virtual road surface.",computing; calculating; counting
CN109703606_A,2019-01-16,2019-05-03,2019-01-16,BEIJING JIAOTONG UNIVERSITY,SU SHUAITANG TAOLI KAICHENGLIU WENTAOYIN JIATENG,66261483,road monitoring,a high-speed train intelligent driving control method based on historical operation data,"The invention provides a high-speed train intelligent control method based on historical operation data, and the method comprises the following steps: carrying out the attribute definition of the operation states of a train, and carrying out the clustering analysis of the historical operation data of the high-speed train according to the defined attributes; finding out a data set similar to the inter-station operation process of the current train from the historical operation data according to the operation state and the similarity index of the current train; converting the future inter-station operation process of the current train into a Markov decision process according to the driving task information of the current train; searching a plurality of historical operation states with the highest similarity of the future inter-station operation process of the current train according to the weighted Euclidean distance; using a Monte Carlo method for sampling to obtain a reference runningtrack, using a reinforcement learning method to optimize the reference running track of the train in the future inter-station whole running process, and obtaining a train intelligent control strategyto intelligently control the train. The method can realize automatic driving of the train and improve the operation service quality.","1. A high-speed train intelligent control method based on historical operation data is characterized by comprising the following steps: performing attribute definition on the running state of the train, and performing cluster analysis on historical running data of the high-speed train according to the defined attribute; finding out a data set C similar to the operation process between the current train stations from the historical operation data according to the operation state and the similarity index of the current trainh; Converting a future inter-station operation process of the current train into a Markov decision process according to the driving task information of the current train; from the data set C according to a weighted Euclidean distancehSearching a plurality of historical operating states with the highest similarity to the future inter-station operating process of the current train; sampling by using a Monte Carlo method to obtain a reference running track of the whole running process of the current train between the future stations, and optimizing the reference running track of the whole running process of the train between the future stations by using a reinforcement learning method to obtain an intelligent control strategy of the train; and intelligently controlling the train according to the train intelligent control strategy. 2. The method of claim 1, wherein the defining attributes of the operational status of the train comprises: current route information, vehicle characteristics, and real-time operational data of the train operation. 3. The method of claim 1, wherein the cluster analysis of the historical operating data of the high-speed train according to the defined attributes comprises: and extracting the data in the historical operation data set according to the defined attributes, respectively carrying out cluster analysis on the historical operation data according to a K-means method, and dividing the historical operation data of the train into disjoint data classes according to different attributes. 4. The method according to claim 1, wherein the data set C similar to the operation process between the current train stations is found from the historical operation data according to the operation state and the similarity index of the current trainhThe method comprises the following steps of finding out a data set C similar to the current inter-train station operation process from the historical operation data according to the Euclidean distance of the following formula (1) as a similarity indexh: Wherein s isiAnd sjRespectively are the characteristic vector of the current train operation data and the characteristic vector, s, of the historical train operation datai,zAnd si,zAre respectively siAnd sjThe z-th dimension of (a) of (b),and i  j, disted(si,sj) Smaller values indicate higher similarity between the two. 5. The method of claim 1, wherein the step of converting the future inter-station operation process of the current train into the markov decision process is represented by the following formula (2): wherein the initial state is s0The last state of train operation is sN,(si,ai) Is a state-action pair, from an arbitrary state siInitially, by taking action aiObtain the next state si+1,aiA={a0,a1,...,ai,...,aN-1} 6. The method according to claim 1, wherein the plurality of historical operating states with the highest similarity to the future inter-station operating process of the current train are searched from the data set according to a weighted Euclidean distance, wherein the weighted Euclidean distance is shown as the following formula (3): wherein  isz 0 and  z=1,siFor a certain segment of the future train operation, and siK historical operation pieces with highest similaritySegment is aszDenotes siThe weight of the z-th dimension attribute. 7. The method of claim 6, wherein said searching for a plurality of historical operating states from said data set having a highest similarity to future inter-station operating procedures of the current train based on weighted Euclidean distances comprises searching for an initial state s of the train from a plurality of historical operating states0Starting and searching in sequence until the train operation end state sNAnd obtaining a plurality of inter-station running process tracks. 8. The method according to claim 1, wherein the reference operation track of the whole process of the operation between the future stations of the train is obtained by sampling by using a monte carlo method, and the reference operation track of the whole process of the operation between the future stations of the train is optimized by using a reinforcement learning method, and the method comprises the following steps of optimizing the future operation track according to the following formula (4): wherein, t belongs to {0,1,2,iis shown in state si-1Next, take action ai-1The resulting reward value is expressed as: wherein, c1And c2Is a constant term; t represents the planning operation time division and is a known item; eiRepresenting the tractive effort, T, of the ith operating sectionpRepresenting the actual running time between train stations, EiAnd TpAll can be obtained from historical operating data; the parameter sigma measures the relative importance of the train alignment target to the energy-saving target, and the larger the value sigma is, the more attention is paidAnd (5) the punctuality of the re-train.",CN109703606_A.txt,B61L27/00,{'railways'},"['guiding railway traffic; ensuring the safety of railway traffic (power supply lines for electrically-propelled vehicles b60m; vehicle signalling in general b60q; brakes or auxiliary equipment b61h, b61k; point or crossing construction e01b; insulated rail joints e01b11/54; optical devices in general g02; controlling in general g05; electric communication technique h04)']","a high-speed train intelligent driving control method based on historical operation data The invention provides a high-speed train intelligent control method based on historical operation data, and the method comprises the following steps: carrying out the attribute definition of the operation states of a train, and carrying out the clustering analysis of the historical operation data of the high-speed train according to the defined attributes; finding out a data set similar to the inter-station operation process of the current train from the historical operation data according to the operation state and the similarity index of the current train; converting the future inter-station operation process of the current train into a Markov decision process according to the driving task information of the current train; searching a plurality of historical operation states with the highest similarity of the future inter-station operation process of the current train according to the weighted Euclidean distance; using a Monte Carlo method for sampling to obtain a reference runningtrack, using a reinforcement learning method to optimize the reference running track of the train in the future inter-station whole running process, and obtaining a train intelligent control strategyto intelligently control the train. The method can realize automatic driving of the train and improve the operation service quality.",railways
US2017267251_A1,2016-03-15,2017-09-21,2016-03-15,PALO ALTO RESEARCH CENTER,"ROBERTS MICHAELDENT, KYLE D.BALA, RAJAGUNNING, DAVID RICHARD",58261546,road monitoring,system and method for providing context-specific vehicular driver interactions,"Interacting with the driver based on the driver's context can keep help keep the driver alert. The context can be determined determining driver characteristics including the interests and by monitoring the circumstances surrounding the driver, such as the state of the driver using sensors included in the vehicle, the state of the vehicle, and the information about the driver's current locale. The characteristics and the monitored circumstances define the context of driver. Information of interest to the driver is obtained and is used to generate actions that are recommendable to the driver based on the driver's context. The actions are used to keep the driver alert.","1. A system for performing context-specific actions towards a vehicular driver, comprising: one or more servers connected over an Internetwork to a vehicle, the servers configured to execute code, comprising: a context module configured to determine a context of a driver of the vehicle, comprising: a driver state module configured to determine a state of the driver; and a vehicle state module configured to determine a state of the vehicle; a characteristic module configured to determine one or more characteristics of the driver; a recommendation module configured to recommend one or more actions to be performed to the driver based on the context; and a performance module configured to perform one or more of the recommended actions.2. A system according to claim 1, further comprising: a location module configured to determine a location of the vehicle; a data module configured to obtain data regarding the determined location.3. A system according to claim 2, further comprising: a graph module configured to represent the driver state, the vehicle state, and the location data in a semantic graph; a vector module configured to represent the characteristics in a vector; and a merging module configured to merge the semantic graph and the vector into a different vector representing the context.4. A system according to claim 1, further comprising: a pose module configured to perform a course pose estimation on the driver using a camera included in the vehicle; a feature module configured to detect one or more facial landmark features on the driver's face using at least some of results of course pose estimation; a gaze module configured to perform a fine gaze estimation on the driver using the detected facial landmark features comprising determining one or more directions of the driver's gaze; and a distraction module configured to use the fine gaze estimation and state of the vehicle to measure a level of distraction of the driver.5. A system according to claim 4, further comprising: eye metric module configured to estimate one or more eye metrics of the driver, the metrics comprising at least one of a blink rate and percentage of eye closure, using the camera; a biometric data module configured to obtain biometric data using a sensor wearable by the driver; and a drowsiness module configured to use the biometric data, the eye metrics, and the head motion to measure a level of the driver's drowsiness.6. A system according to claim 4, further comprising: a video module configured to obtain training videos of the driver looking at a plurality of known directions; an identification module configured to identify in each of the training videos the driver's landmark facial features; a comparison module configured to compare the training videos landmark facial features to the detected facial features, wherein the fine gaze estimation is performed based on the comparison; and a smoothing module configured to perform temporal smoothing of results of the gaze estimation.7. A system according to claim 1, further comprising: an extraction module configured to extract from the Internet a plurality of data items associated with the driver; a comparison module configured to compare the data items to a hierarchy of topics and identifying one or more of the topics associated with each of the data items; a classification module configured to classify the data items using the topics associated with each of the data items; and a creation module creating a profile of the driver, the profile comprising the driver's current and historical interests, using the classifications of the data items.8. A system according to claim 7, further comprising: a list module configured to maintain a list of possible actions; an extraction module configured to extract from the Internet current information associated with the driver; a generation module configured to generate one more recommendable actions based on the current information, the driver's context, the driver's profile and the possible actions; a comparison module configured to compare the recommendable actions to the driver's profile and to recommend one or more of the actions based on the comparison.9. A system according to claim 1, further comprising: wherein the recommended actions comprise one or more of conversing with the driver.10. A system according to claim 9, wherein the conversation is performed using natural language.11. A method for performing context-specific actions towards a vehicular driver, comprising the steps of: determining a context of a driver of a vehicle, comprising: determining a state of the driver; and determining a state of the vehicle; determining one or more characteristics of the driver; recommending one or more actions to be performed to the driver based on the context; and performing one or more of the recommended actions, wherein the steps are performed by at least one suitably programmed computer.12. A method according to claim 11, wherein determining the context further comprises: determining a location of the vehicle; obtaining data regarding the determined location.13. A method according to claim 12, further comprising: representing the driver state, the vehicle state, and the location data in a semantic graph; representing the characteristics in a vector; and merging the semantic graph and the vector into a different vector representing the context.14. A method according to claim 11, further comprising: performing a course pose estimation on the driver using a camera included in the vehicle; detecting one or more facial landmark features on the driver's face using at least some of results of course pose estimation; performing fine gaze estimation on the driver using the detected facial landmark features comprising determining one or more directions of the driver's gaze; and using the fine gaze estimation and state of the vehicle to measure a level of distraction of the driver.15. A method according to claim 14, further comprising: estimating one or more eye metrics of the driver, the metrics comprising at least one of a blink rate and percentage of eye closure, using the camera; obtaining biometric data using a sensor wearable by the driver; and using the biometric data, the eye metrics, and the head motion to measure a level of the driver's drowsiness.16. A method according to claim 14, further comprising: obtaining training videos of the driver looking at a plurality of known directions; identifying in each of the training videos the driver's landmark facial features; comparing the training videos landmark facial features to the detected facial features, wherein the fine gaze estimation is performed based on the comparison; and performing temporal smoothing of results of the gaze estimation.17. A method according to claim 11, further comprising: extracting from the Internet a plurality of data items associated with the driver; comparing the data items to a hierarchy of topics and identifying one or more of the topics associated with each of the data items; classifying the data items using the topics associated with each of the data items; and creating a profile of the driver, the profile comprising the driver's current and historical interests, using the classifications of the data items.18. A method according to claim 17, further comprising: maintaining a list of possible actions; extracting from the Internet current information associated with the driver; generating one more recommendable actions based on the current information, the driver's context, the driver's profile and the possible actions; comparing the recommendable actions to the driver's profile and recommending one or more of the actions based on the comparison.19. A method according to claim 11, further comprising: wherein the recommended actions comprise one or more of conversing with the driver.20. A method according to claim 19, wherein the conversation is performed using natural language.",US2017267251_A1.txt,B60W40/08,{'vehicles in general'},['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit'],"system and method for providing context-specific vehicular driver interactions Interacting with the driver based on the driver's context can keep help keep the driver alert. The context can be determined determining driver characteristics including the interests and by monitoring the circumstances surrounding the driver, such as the state of the driver using sensors included in the vehicle, the state of the vehicle, and the information about the driver's current locale. The characteristics and the monitored circumstances define the context of driver. Information of interest to the driver is obtained and is used to generate actions that are recommendable to the driver based on the driver's context. The actions are used to keep the driver alert.",vehicles in general
US2014270350_A1,2013-03-14,2014-09-18,2013-03-14,XEROX CORPORATION,"RODRIGUEZ-SERRANO, JOSE ANTONIOLARLUS-LARRONDO, DIANE",51527210,gps,data driven localization using task-dependent representations,"A computer implemented method for localization of an object, such as a license plate, in an input image includes generating a task-dependent representation of the input image based on relevance scores for the object to be localized. The relevance scores are output by a classifier for a plurality of locations in the input image, such as patches. The classifier is trained on patches extracted from training images and their respective relevance labels. One or more similar images are identified from a set of images, based on a comparison of the task-dependent representation of the input image and task-dependent representations of images in the set of images. A location of the object in the input image is identified based on object location annotations for the similar images.","1. A method for object localization in an image comprising: for an input image, generating a task-dependent representation of the input image based on relevance scores for an object to be localized, the relevance scores being output by a classifier for a plurality of locations in the input image; identifying at least one similar image from a set of images, based on the task-dependent representation of the input image and task-dependent representations of images in the set of images; and identifying a location of the object in the input image based on an object location annotation for at least one of the at least one similar images identified in the set of images, wherein at least one of the generating of the task-dependent representation, identifying of the at least one similar image, and the identifying a location of the object in the input image is performed with a computer processor.2. The method of claim 1, wherein the generating of the task-dependent representation comprises: for each of a plurality of patches of the image, generating a patch-based representation based on low level features extracted from the patch; with the classifier, outputting a relevance score for each patch, based on the respective patch-based representation; and generating a probability map for the input image based on the patch based representations.3. The method of claim 2, wherein the task-dependent representation comprises a vectorial representation of the probability map.4. The method of claim 2, wherein at least some of the plurality of patches are overlapping.5. The method of claim 2, wherein the patch-based representations are each derived from low level features of the respective patch selected from color and gradient features.6. The method of claim 2, wherein the patch-based representations are output by a generative model built from low level features.7. The method of claim 6, wherein the patch-based representations comprise Fisher vectors.8. The method of claim 2, wherein the generating of the task-dependent representation comprises generating a global representation of the image with a generative model based on representations of sub-regions of the image where the contribution of each the sub-regions to the global representation is weighted by the value of the probability map in the sub-region location.9. The method of claim 1, wherein the identifying of the at least one similar image from the set of images comprises identifying a plurality of similar images and the identifying of the location of the object in the input image is based on object location annotations for the plurality of similar images.10. The method of claim 1, wherein the identifying of the at least one similar image from the set of images comprises computing a linear kernel between the task-dependent representation of the input image and each of the task-dependent representations of a plurality of the images in the set of images.11. The method of claim 1, wherein the identifying of the at least one similar image from the set of images comprises computing a similarity between the task-dependent representation of the input image and each of the task-dependent representations of a plurality of the images in the set of images wherein in computing the similarity, the task-dependent representations are embedded with a metric that has been learned on a training set of annotated images and their task-dependent representations.12. The method of claim 1, wherein the set of images comprises a first set of images and a second set of images and the identifying a location of the object in the input image comprises: identifying an approximate location of the object in the input image based on the object location annotation for at least one of the at least one similar images identified in the first set of images; based on the approximate location, identifying a cropped region of the input image; identifying at least one similar image from the second set of images, based on a task-dependent representation of the cropped region of the input image and task-dependent representations of the images in the second set of images; and identifying a location of the object in the input image based on an object location annotation for at least one of the at least one similar images identified in the second set of images.13. The method of claim 1, wherein the object to be located comprises a license plate.14. The method of claim 1, further comprising extracting information from the image in the identified location.15. The method of claim 1 further comprising training the classifier on patch-based representations of patches extracted from a set of training images, the patches of the training images being labeled with a label representing the overlap between the patch and a location of an object of in the training image which is in the object class.16. The method of claim 1, wherein the object to be localized comprises a plurality of objects, each object being in a different object class, and the generating of the task-dependent representation of the input image comprises generating a first task-dependent representation of the input image based on relevance scores for the first class of object at locations in the input image output by a first classifier for the first class of object and generating a second task-dependent representation of the input image based on relevance scores for the second class of object at locations in the input image output by a second classifier for the second class of object.17. The method of claim 1, further comprising outputting at least one of the location of the object in the input image and information extracted from the image in the object location.18. A computer program product comprising a non-transitory recording medium storing instructions, which when executed by a computer, perform the method of claim 1.19. A system comprising memory which stores instructions for performing the method of claim 1 and a processor in communication with the memory for executing the instructions.20. A system for object localization in images comprising: a patch representation generator which generates patch-based representations of a plurality of patches of an input image; a classifier component which classifies each of the patches with a trained classifier model based on the respective patch-based representation; a signature generator which generates a task-dependent representation of the input image based on the classifications of the patches; a retrieval component configured for retrieving at least one similar image from a set of images, based on a comparison measure between the task-dependent representation of the input image and task-dependent representations of images in the set of images; a segmentation component which segments the input image based on a location of an object in the at least one similar image and identifying a location of an object in the input image based on the segmentation; and a processor which implements the patch representation generator, classifier component, signature generator, and segmentation component.21. The system of claim 20, further comprising an information extraction component for extracting information from the identified location.22. The system of claim 20, wherein the classifier model is trained on patches extracted from images of vehicles in which a license plate is localized and the object to be localized in the input image is a license plate.23. The system of claim 20, wherein the classifier component classifies patches of images in the set of images with the trained classifier model based on respective patch-based representations of the patches of the images in the set of images and the signature generator generates a task-dependent representation of each of the images in the set of images based on the classifications of the patches of the respective image.24. A method for object localization in an image comprising: with a processor: for each of a set of test images, generating a patch-based representation of a plurality of patches of the test image with a generative model; classifying each of the test image patches with a trained classifier model based on the respective patch-based representation; generating a task-dependent representation of each of the test images based on the classifications of the patches of the test image; generating patch-based representations of a plurality of patches of an input image with the generative model; classifying each of the patches of the input image with the trained classifier model based on the respective patch-based representation; generating a task-dependent representation of the input image based on the classifications of the patches of the input image; retrieving at least one similar test image from the set of test images, based on a comparison measure between the task-dependent representation of the input image and the task-dependent representations of the test images in the set of test images; and segmenting the input image based on a location of an object in the at least one similar test image and identifying a location of an object in the input image based on the segmentation.",US2014270350_A1.txt,"G06K9/32,G06K9/62,G06T7/00,G06T7/20",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general', 'image data processing or generation, in general']","data driven localization using task-dependent representations A computer implemented method for localization of an object, such as a license plate, in an input image includes generating a task-dependent representation of the input image based on relevance scores for the object to be localized. The relevance scores are output by a classifier for a plurality of locations in the input image, such as patches. The classifier is trained on patches extracted from training images and their respective relevance labels. One or more similar images are identified from a set of images, based on a comparison of the task-dependent representation of the input image and task-dependent representations of images in the set of images. A location of the object in the input image is identified based on object location annotations for the similar images.",computing; calculating; counting
US2020307561_A1,2019-03-25,2020-10-01,2019-03-25,GM GLOBAL TECHNOLOGY OPERATIONS,"ZENG, SHUQINGBAJWA, MANPREET S.BUSH, LAWRENCE A.Sprague, Rickie A.",72606845,road monitoring,system and method for radar cross traffic tracking and maneuver risk estimation,A risk maneuver assessment system and method to generate a perception of an environment of a vehicle and a behavior decision making model for the vehicle; a sensor system configured to provide the sensor input in the environment for filtering target objects; one or more modules configured to map and track target objects to make a candidate detection from multiple candidate detections of a true candidate detection as the tracked target object; apply a Markov Random Field (MRF) algorithm for recognizing a current situation of the vehicle and predict a risk of executing a planned vehicle maneuver at the true detection of the dynamically tracked target; apply mapping functions to sensed data of the environment for configuring a machine learning model of decision making behavior of the vehicle; and apply adaptive threshold to cells of an occupancy grid for representing an area of tracking of objects within the vehicle environment.,"1. A risk maneuver assessment system for planning maneuvers with uncertainties of a vehicle, comprising: a first controller with a processor programmed to generate a perception of an environment of the vehicle and a behavior decision making model for the vehicle including performing a calculation upon a sensor input to provide, as an output an action risk mapping and at least one target object tracking for different areas within the environment of the vehicle; a sensor system configured to provide the sensor input to the processor for providing an area in the environment of vehicle for filtering target objects; one or more modules configured to, by a processor, map and track target objects to make a candidate detection from multiple candidate detections of a true candidate detection as the tracked target object; one or more first modules configured to, by the processor, apply a Markov Random Field (MRF) algorithm for recognizing a current situation of the vehicle in the environment and predict a risk of executing a planned vehicle maneuver at the true detection of the dynamically tracked target; one or more second modules configured to, by the processor, apply mapping functions to sensed data of the environment for configuring a machine learning model of decision making behavior of the vehicle; one or more third modules configured to, by the processor, apply adaptive threshold to cells of an occupancy grid configured for representing an area of tracking of objects within the vehicle environment; and a second controller with a processor configured to generate control commands in accordance with modeling of the decision making behavior and the perception of the environment of the vehicle for planned vehicle maneuvers.2. The system of claim 1, further comprising: the one or more first modules programmed to generate a Markov Random Field (MRF) to recognize the current situation.3. The system of claim 1, further comprising: the one or more of the second and/or third modules further configured to select as the true detection at least one of the candidate detections that is within a radius for the target, and a candidate detection that is closest to a first known mapped pathway.4. The system of claim 3, further comprising: the one or more of the second and/or third modules further configured to select as the true detection, the candidate that indicates a position and velocity that are consistent with a target traveling on a second known travel mapped pathway, and a selection as a false detection, the candidate that indicates a position that is outside the second known pathway or the velocity is not consistent with the target traveling.5. The system of claim 4, further comprising: a fourth module configured to compute, by a gating operation, a distance metric from the last position of a tracked target object to a predicted position less than a threshold distance related to one or more of the candidate detections.6. The system of claim 5, further comprising: one or more fifth modules configured to, by the processor, apply the Markov Random Field (MRF) algorithm representing the tracked target object in one or more cells of an occupancy grid by: calculating an object measurement density for each tracked target object represented in the one or more cells of the occupancy grid; spreading the density over a window comprising the set of cells of the occupancy grid represented by the tracked target object; and spreading velocities over the window comprising a same set of cells of the occupancy grid represented by the tracked target object.7. The system of claim 6, further comprising: one or more sixth modules configured to, by the processor, apply mapping functions to sensed data of the environment for configuring a machine learning (ML) model of decision making behavior of the vehicle by an action risk assessment model trained using semi-supervised machine learning techniques by on-line and off-line training for mapping function to candidate actions to determine with risk factors a learned drivable path.8. The system of claim 7, further comprising: a seventh module configured to, by a processor, perform in the off-line training of the ML model comprising: collecting labels, co-collecting occupancy velocity grids, extracting features from the occupancy grids, and applying at least support vector machine (SVM) techniques for recognizing class patterns of the candidate actions to determine with risk factors the learned drivable path.9. The system of claim 8, further comprising an eighth module configured to, by the processor, apply adaptive threshold to cells of an occupancy grid configured for representing area of tracking of objects within the vehicle environment comprising: a ninth module configured to, by the processor, compute by an adaptive threshold occupancy density, the likelihood that a candidate action is available for the target tracked object based on the computed density distribution, and select the candidate action that has the highest probability of being available; and a tenth module configured to, by the processor, compute a clustering for velocity clusters for a set of candidate actions to select the target tracked object that indicates a position that is consistent with a learned drivable path.10. The system of claim 8, wherein the ML model is trained using reinforcement learning techniques using a data set of past collected labels and sensor data of drivable paths and wherein the eight module is configured to select the candidate action that will likely contribute to one of the drivable paths wherein the sensor data at least comprises one of: radar, acoustic, lidar or image sensor data.11. A vehicle, comprising: a sensor detection sensing device including one or more of a set comprising: a radar, acoustic, lidar and image sensing device; a risk maneuver assessment system for assessing one or more uncertainty factors in planned maneuvers; and a plurality of modules configured to, by a processor, generate a perception of an environment of the vehicle and a output target output for tracking different areas within the environment; the plurality of modules comprising: one or more modules configured to, by a processor, map and track target objects to make a candidate detection from multiple candidate detections of a true candidate detection as the tracked target object; one or more modules configured to, by the processor, apply a Markov Random Field (MRF) algorithm for recognizing a current situation of the vehicle in the environment and for predicting a risk of executing a planned vehicle maneuver at the true detection of the dynamically tracked target; one or more modules configured to, by the processor, apply mapping functions to sensed data of the environment for configuring a machine learning model of decision making behavior of the vehicle; one or more modules configured to, by the processor, apply adaptive threshold to cells of an occupancy grid configured for representing areas of tracking of objects within the environment; and a controller with a processor configured to generate control commands in accordance with modeling of the decision making behavior and the perception of the environment of the vehicle for planned vehicle maneuvers.12. The system of claim 11, wherein the one or more modules are programmed to generate a Markov Random Field (MRF) to recognize the current situation.13. The system of claim 11, further comprising: the one or more modules configured to select as the true detection comprising: a first module configured to select, as the true detection, the candidate detection that is within a radius for the target; and a second module configured to select, as the true detection, the candidate detection that is closest to a first known mapped pathway.14. The system of claim 13, further comprising: the one or more modules further configured to select as the true detection comprising: a third module configured to select the true detection, the candidate that indicates a position and velocity that are consistent with a target traveling on a second known travel mapped pathway; and the third module configured to select a false detection, the candidate that indicates a position that is outside the second known pathway or the velocity is not consistent with the target traveling.15. The system of claim 14, further comprising: the one or more modules further configured to select as the true detection comprising: a fourth module configured to compute, by a gating operation, a distance metric from the last position of a tracked target object to a predicted position less than a threshold distance related to one or more of the candidate detections.16. The system of claim 15, wherein the one or more modules configured by the processor for applying the Markov Random Field (MRF) algorithm representing the tracked target object in one or more cells of an occupancy grid further comprising: a fifth module is configured to calculate an object measurement density for each tracked target object represented in the one or more cells of the occupancy grid; a sixth module is configured to spread the density over a window comprising the set of cells of the occupancy grid represented by the tracked target object; and a seventh module is configured to spread velocities over the window comprising a same set of cells of the occupancy grid represented by the tracked target object.17. The system of claim 16, wherein the one or more modules configured by the processor for applying mapping functions to sensed data of the environment for configuring a machine learning (ML) model of decision making behavior of the vehicle further comprising: an eighth module comprising an action risk assessment model trained using semi-supervised machine learning techniques, by on-line and off-line training, for mapping functions to candidate actions to determine with risk factors a learned drivable path wherein the eight module in the off-line training of the ML model comprises: collecting labels, co-collecting occupancy velocity grids, extracting features from the occupancy grids, and applying at least support vector machine (SVM) techniques for recognizing class patterns of the candidate actions to determine with risk factors the learned drivable path.18. The system of claim 17, wherein the one or more modules for applying adaptive threshold to cells of an occupancy grid configured for representing area of tracking of objects within the vehicle environment comprising: a ninth module configured to compute by an adaptive threshold occupancy density, the likelihood that a candidate action is available for the target tracked object based on the computed density distribution, and select the candidate action that has the highest probability of being available; and a tenth module configured to compute a clustering for velocity clusters for a set of candidate actions to select the target tracked object that indicates a position that is consistent with a learned drivable path.19. The system of claim 18, wherein the ML model is trained using reinforcement learning techniques using a data set of past collected labels and radar data of drivable paths and wherein the eight module is configured to select the candidate action that will likely contribute to one of the drivable paths.20. A planning system of a vehicle, the system comprising: a sensor system configured to provide the sensor input to the processor for providing an area in the environment of vehicle for filtering target objects; and a non-transitory computer readable medium comprising: a first module configured to, by a processor, select, as the true detection, the candidate detection that is within a radius for the target; a second module configured to, by a processor, select, as the true detection, the candidate detection that is closest to a first known mapped pathway; a third module configured to, by a processor, select the true detection, the candidate that indicates a position and velocity that are consistent with a target traveling on a second known travel mapped pathway, and the third module configured to select a false detection, the candidate that indicates a position that is outside the second known pathway or the velocity is not consistent with the target traveling; a fourth module configured to, by a processor, compute, by a gating operation, a distance metric from the last position of a tracked target object to a predicted position less than a threshold distance related to one or more of the candidate detections; a fifth module is configured to, by a processor, calculate an object measurement density for each tracked target object represented in the one or more cells of the occupancy grid; a sixth module is configured to, by a processor, spread the density over a window comprising the set of cells of the occupancy grid represented by the tracked target object; a seventh module is configured to, by a processor, spread velocities over the window comprising a same set of cells of the occupancy grid represented by the tracked target object; an eighth module comprising an action risk assessment model trained using semi-supervised machine learning techniques by on-line and off-line training for mapping function to candidate actions to determine with risk factors a learned drivable path wherein the eighth module in the off-line training of the ML model is configured to, by a processor, collect labels, co-collect occupancy velocity grids, extract features from the occupancy grids, and apply at least support vector machine (SVM) techniques for recognizing class patterns of the candidate actions to determine with risk factors the learned drivable path; a ninth module configured to, by a processor, compute by an adaptive threshold occupancy density, the likelihood that a candidate action is available for the target tracked object based on the computed density distribution, and select the candidate action that has the highest probability of being available; and a tenth module configured to, by a processor, compute a clustering for velocity clusters for a set of candidate actions to select the target tracked object that indicates a position that is consistent with a learned drivable path wherein the ML model is trained using reinforcement learning techniques using a data set of past collected labels and radar data of drivable paths and wherein the eight module is configured to select the candidate action that will likely contribute to one of the drivable paths.",US2020307561_A1.txt,"B60W30/09,B60W30/095,G01S13/93,G05D1/02,G06N20/00","{'measuring; testing', 'controlling; regulating', 'computing; calculating; counting', 'vehicles in general'}","['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'systems for controlling or regulating non-electric variables (for continuous casting of metals b22d11/16; valves per se f16k; sensing non-electric variables, see the relevant subclasses of g01; for regulating electric or magnetic variables g05f)', 'computing arrangements based on specific computational models']",system and method for radar cross traffic tracking and maneuver risk estimation A risk maneuver assessment system and method to generate a perception of an environment of a vehicle and a behavior decision making model for the vehicle; a sensor system configured to provide the sensor input in the environment for filtering target objects; one or more modules configured to map and track target objects to make a candidate detection from multiple candidate detections of a true candidate detection as the tracked target object; apply a Markov Random Field (MRF) algorithm for recognizing a current situation of the vehicle and predict a risk of executing a planned vehicle maneuver at the true detection of the dynamically tracked target; apply mapping functions to sensed data of the environment for configuring a machine learning model of decision making behavior of the vehicle; and apply adaptive threshold to cells of an occupancy grid for representing an area of tracking of objects within the vehicle environment.,measuring; testing controlling; regulating computing; calculating; counting vehicles in general
CN110728702_A,2019-08-30,2020-01-24,2019-08-30,SHENZHEN UNIVERSITYSHENZHEN CITY PINGSHAN DISTRICT BIG DATA RESOURCE MANAGEMENT CENTER,LIU XINGLI YANSHANWANG HAIPENGLIU YUZHOU WEIQIWEI JIALI,69218769,road monitoring,high-speed cross-camera single-target tracking method and system based on deep learning,"The invention discloses a high-speed cross-camera single-target tracking method based on deep learning, relates to the technical field of computer vision, and solves the problems that in the prior art, multi-camera multi-target tracking needs to extract features of all track pieces and carry out global association, and only part of data cannot be processed. The high-speed cross-camera single-target tracking method comprises the following steps: establishing a pedestrian detection model; performing multi-target tracking in a single camera, and performing multi-target tracking based on a Kalmanfiltering algorithm; screening the track piece representative pictures; extracting pedestrian appearance features by adopting a pre-trained pedestrian re-identification model; designing a search constraint condition and association in a single camera domain; and realizing single-target tracking through the designed cross-camera trajectory direction constraint and the cross-camera trajectory association step based on the trajectory direction constraint. Experimental results and analysis show that the tracking method provided by the invention has good real-time performance and accuracy.","1. the high-speed cross-camera single-target tracking method based on deep learning is characterized by comprising the following steps: the method comprises the following steps: step one, detecting a pedestrian target by adopting a pedestrian detection model, and outputting the position size and the detection score of a detection frame of each pedestrian under each camera; step two, carrying out multi-target tracking based on a kalman filtering algorithm by using the intersection and parallel ratio of the detection frames obtained in the step one as a basis to obtain a motion track set; step three, screening by adopting a track sheet representative picture according to the motion track set obtained in the step two to obtain a track picture with high pedestrian detection score in the track characteristics; step four, extracting the appearance characteristics of the pedestrians from the track picture obtained in the step three by adopting a pre-trained pedestrian re-identification model; setting a search constraint condition and track association in a single camera domain, taking a track picture with the minimum cosine distance in a search range as a matching track, and associating by adopting the matching track to obtain an associated track set; step six, designing a direction constraint condition associated with the cross-camera track local bipartite graph for the associated track set obtained in the step five, and finally matching and establishing a space-time constraint condition; seventhly, according to the space-time constraint condition in the sixth step, based on the cross-camera track association of track direction constraint, searching for the matching association which enables the sum of all matching distances to be minimum by adopting a hungary method, and if the target is hit, the association is successful; otherwise, the association fails, and the tracking of the target is terminated. 2. the high-speed cross-camera single-target tracking method based on deep learning of claim 1, wherein: in the first step, a fasterrcnn model is adopted as a pedestrian detection algorithm, and the following steps are kept: 2, retraining the detection frame on the kitti data set, finally obtaining a better pedestrian detection result on the nlpr _ mct, and saving the detection result and the detection score of each frame as algorithm input. 3. the high-speed cross-camera single-target tracking method based on deep learning of claim 1, wherein: in the third step, the picture screening is represented by the track sheet, and the specific process is as follows: screening track pictures with the heights determined by pedestrian detection in the track characteristics; the screening rule is as follows: i=argsort(s) i'={l|liandsl<} wherein s is the pedestrian detection score of each frame of the pedestrian in the track, and sldetecting scores for pedestrians in the ith frame of the track, wherein i is an index set ranked according to the scores in a descending order; alpha is alphafor the detection score threshold screening, i' is the picture index after the primary screening; and selecting three track pictures with the longest track timestamp interval to maximize the position difference of the pedestrians, and finally obtaining the track pictures with higher pedestrian detection scores and various positions in the representative track. 4. the high-speed cross-camera single-target tracking method based on deep learning of claim 1, wherein: in the fifth step, the following function is adopted to designate the search range; f=x+v in the formula, an x vector is a motion state of a pedestrian boundary frame, v is a change rate of each state, and delta is a frame difference when the current time and the track time disappear; x is { x, y, w, h }, x, y is the target center position when the trajectory disappears, and w and h are width and height; wherein f  { x ', y', w ', h' }, x ', y' are predicted position centers, and w 'and h' are the widths and heights of the search regions; setting a track picture with a plurality of pedestrians at different positions in a track as a matching track, wherein the track picture with the minimum cosine distance in a search range is expressed by a formula as follows: dist(p,q)=minp,qcos(p,q) in the formula, p and q are respectively the picture features of the extracted target track and candidate track, p, q are respectively the picture feature sets of the extracted target track and candidate track, and association is performed according to the obtained matching track to obtain the associated track set. 5. the high-speed cross-camera single-target tracking method based on deep learning of claim 1, wherein: the concrete process of finally matching and establishing the space-time constraint condition in the step six is as follows: setting a topological matrix m as direction limitation of cross-camera matching; the topological matrix m is a 4 x n-order unit boolean matrix, and n is the number of cameras; by using numbers1-4 represent four directions of a scene plane; m4n+i,4m+j1 represents that the i-direction track of the nth camera and the j-direction track of the mth camera are in a correlation relationship, if the result is 0, the correlation is not performed, and the track direction is determined by the minimum included angle between the vector from the starting point to the end point of the track and the reference coordinate system; and finally, establishing space-time constraint conditions by matching as follows: m(si,sj)=1 in the formula etaseconstraining parameters for time interval ranges of trajectory extinction and recurrence;track loss and reproduction time; siand sji and j direction trajectories in the topology matrix m, respectively. 6. the high-speed cross-camera single-target tracking method based on deep learning of claim 1, wherein: step seven, cross-camera track association based on track direction constraint comprises the following specific processes: setting pedestrians in the same row with the target to be constrained by the space-time constraint condition, searching an auxiliary track set in a disappearing scene when the pedestrians are matched across the cameras, searching a candidate track set according to the formula of the matched track in the step, and taking the matching of the auxiliary track set and the candidate track set as bipartite graph matching; searching for a matching which minimizes the sum of all matching distances by adopting the hungarian method; the matching objective function is: wherein c is an assignment matrix with only one element of 1 in each row and column, dist (r, o) is the characteristic distance between the ith auxiliary track and the ith track of the candidate track pool; mm and nn are the number of auxiliary trajectories and candidate trajectories, respectively. 7. the high-speed cross-camera single-target tracking method based on deep learning of claim 6, wherein: the set of auxiliary tracks is defined as: in two body position ranges of the target track disappearing position, u tracks closest to the disappearing time, wherein the u tracks and the target track form an auxiliary track pool; defining candidate trajectories as a set of trajectories subject to the spatiotemporal constraint; the specific process is as follows: 1) sequencing pedestrian tracks according to a time appearance sequence; 2) when the current track is finished, searching an auxiliary track of the current track, and extracting the characteristics of the current track and the auxiliary track; 3) searching candidate tracks and extracting track characteristics; 4) matching the auxiliary track with the candidate track; performing multi-track association by adopting a hungarian method; 5) if the result assignment matrix hits the target, the association is successful; otherwise, the association fails, and the tracking of the target is terminated. 8. the high-speed cross-camera single-target tracking method based on deep learning of claim 1, wherein: and between the sixth step and the seventh step, a step of verifying feature validity of the extracted features is further included, a mode of constructing a graph model and converting the track association into the minimum cost network flow is adopted, the graph structure model is g (v, e, w), wherein a vertex v is a feature set of the track, e is an edge of a node, namely the association between two tracks is constrained by a space-time constraint condition, and w is a link between two tracksrepresenting the edge weight of the first node and the beta node. adding a starting node s and a terminating node t, wherein the flow from the starting node to the terminating node is equal to the target track number ws,wtrespectively represent the start nodess to node epsilon and node epsilon to the terminating node t. the weight represents the cost of each edge, i.e., the distance of the trace feature. given the number of trajectories k, the objective function is: ={e1,e2,e} the optimal association is to find an optimal set of edges e that can traverse all node streams at a minimum cost, each stream  representing a complete trace. 9. the tracking system of the high-speed cross-camera single-target tracking method based on deep learning according to any one of claims 1 to 8, characterized in that: the system comprises a pedestrian detection module, a single-phase machine internal multi-target tracking module, a pedestrian re-identification model feature extraction module and a track association module; the pedestrian detection module is used for detecting a pedestrian target and outputting a pedestrian detection frame and a detection score; the single-phase machine internal multi-target tracking module is used for carrying out multi-target tracking by adopting a kalman filtering algorithm according to the intersection-parallel ratio of the detection frames obtained by the pedestrian detection module to obtain a track picture with high detection score; the pedestrian re-recognition model feature extraction module is used for extracting appearance features of a track picture obtained by the multi-target tracking module in the single-phase machine; and the track association module is used for carrying out track association on the features extracted by the pedestrian re-identification model feature extraction module.",CN110728702_A.txt,"G06K9/00,G06K9/62,G06N3/04,G06T7/292",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'image data processing or generation, in general']","high-speed cross-camera single-target tracking method and system based on deep learning The invention discloses a high-speed cross-camera single-target tracking method based on deep learning, relates to the technical field of computer vision, and solves the problems that in the prior art, multi-camera multi-target tracking needs to extract features of all track pieces and carry out global association, and only part of data cannot be processed. The high-speed cross-camera single-target tracking method comprises the following steps: establishing a pedestrian detection model; performing multi-target tracking in a single camera, and performing multi-target tracking based on a Kalmanfiltering algorithm; screening the track piece representative pictures; extracting pedestrian appearance features by adopting a pre-trained pedestrian re-identification model; designing a search constraint condition and association in a single camera domain; and realizing single-target tracking through the designed cross-camera trajectory direction constraint and the cross-camera trajectory association step based on the trajectory direction constraint. Experimental results and analysis show that the tracking method provided by the invention has good real-time performance and accuracy.",computing; calculating; counting
US10600199_B2,2017-06-27,2020-03-24,2017-06-27,TOYOTA RESEARCH INSTITUTE,"JAMES MICHAEL R.KAMATA, NOBUHIDESAKAI, KATSUHIRO",64693409,vehicle classification,extending object detection and identification capability for an object sensor device,"A device and method for extending an object identification range in a vehicle environment are disclosed. In this regard, the embodiments may operate to solve to a remaining one of a set of pre-defined shape models to an object by successive iterations of point cloud data and associated closing distance to the object. Successive iterations of point cloud data may include receiving, via an object sensor device, the point cloud data, and detecting at least a portion of an object from the point cloud data, which portion is compared with each of a set of pre-defined shape models. The set is updated to include ones comparing favorably with the point cloud data. When the successive iterations solve to a singular remaining one of the set, an iterative match to the object may be based on the singular remaining one of the set of pre-defined shape models to identify the object.","1. A method for extending an object identification range in a vehicle environment, the method comprising: solving to a remaining one of a set of pre-defined shape models for an object by successive iterations of point cloud data as a distance from a vehicle to the object along a travel lane decreases due to a velocity of the vehicle exceeding a velocity of the object by: receiving, via an object sensor device, the point cloud data; detecting at least a portion of the object from the point cloud data; comparing each of a set of pre-defined shape models with the at least the portion of the object; and updating the set of pre-defined shape models to include ones comparing favorably with the point cloud data; and when the successive iterations solve to a singular remaining one of the set of pre-defined shape models, identifying an iterative match to the object based on the singular remaining one of the set of pre-defined shape models.2. The method of claim 1, further comprising: tracking the object based on the singular remaining one of the set of pre-defined shape models.3. The method of claim 1, wherein the point cloud data is generated at a range gate distance characteristic of the object sensor device, the range gate distance characteristic producing a point cloud density relatable to the at least the portion of the object.4. The method of claim 3, wherein the object sensor device comprises at least one of: a LIDAR object sensor device; a radar object sensor device; and a milliwave object sensor device.5. The method of claim 1, wherein the point cloud data is generated at a range gate distance characteristic of the object sensor device.6. The method of claim 1, wherein the set of pre-defined shape models comprises: a vehicular model profile relatable to each of a plurality of point cloud data samples of a pre-defined three-dimensional vehicle based on a corresponding plurality of closing distances relative to a range gate distance characteristic of the object sensor device.7. The method of claim 6, wherein the object sensor device has a known origin location with respect to the point cloud data of the object.8. The method of claim 6, wherein the set of pre-defined shape models comprises at least one of: a passenger vehicle model; a sport utility vehicle model; a sport vehicle model; and a cargo vehicle model.9. A method for object identification in a vehicle environment, the method comprising: receiving, via an object sensor device, first point cloud data relating to at least a spatial portion of the vehicle environment, the first point cloud data being relatable to at least a portion of a vehicular object and a set of pre-defined shape models associated with the at least a spatial portion of the environment; comparing each of the set of pre-defined shape models with the first point cloud data; forming a first subset of pre-defined shape models based on remaining ones of the set of pre-defined shape models comparing favorably with the first point cloud; and determining most-likely match of the set of pre-defined shape models by iteratively, as a distance between a vehicle and the vehicular object along a travel lane decreases due to a velocity of the vehicle exceeding a velocity of the vehicular object: receiving subsequent point cloud data relating to at least a further portion of the vehicular object; comparing each of a remaining ones of the first subset of pre-defined shape models with the subsequent point cloud data; and updating the remaining ones of the first subset of pre-defined shape models; and when the first subset of pre-defined shape models corresponds to a remaining one of the first subset of pre-defined shape models, identifying a most-likely match of the vehicular object with the remaining one of the first subset of pre-defined shape models.10. The method of claim 9, wherein the first point cloud data is generated at a range gate distance characteristic of the object sensor device, the range gate distance characteristic producing a point cloud density relatable to the at least the portion of the vehicular object.11. The method of claim 10, wherein the object sensor device comprises at least one of: a LIDAR object sensor device; a radar object sensor device; and a milliwave object sensor device.12. The method of claim 9, wherein the first point cloud data is generated at a range gate distance characteristic of the object sensor device.13. The method of claim 9, wherein the set of pre-defined shape models comprises: a vehicular model profile relatable to each of a plurality of point cloud data samples of a known three-dimensional vehicle based on a corresponding plurality of sample distances up to a range gate distance characteristic of the object sensor device.14. The method of claim 13, wherein the object sensor device has a known origin location with respect to the each of the plurality of point cloud data samples of the known three-dimensional vehicle.15. The method of claim 13, wherein the set of pre-defined shape models comprise at least one of: a passenger vehicle model; a sport utility vehicle model; a sport vehicle model; and a cargo vehicle model.16. A vehicle control unit for vehicular object identification in a vehicle environment, the vehicle control unit comprising: a processor; memory communicably coupled to the processor and to an object sensor device, and the memory storing: an object detection module including instructions that, when executed by the processor, cause the processor to: receive, via an object sensor device, point cloud data; detect at least a portion of a vehicular object from the point cloud data; and produce detected vehicular object data from the at least the portion of the vehicular object from the point cloud data; and an object decision module including instructions that, when executed by the processor, cause the processor to: compare each of a set of pre-defined shape models with the detected vehicular object data; and update the set of pre-defined shape models to ones comparing favorably with the detected vehicular object data; and wherein, when successive iterations of the object detection module and the object decision module on subsequent point cloud data, as a distance from a vehicle to the vehicular object along a travel lane decreases due to a velocity of the vehicle exceeding a velocity of the vehicular object, solve to a remaining one of the set of pre-defined shape models, the object decision module produces vehicular object classification data based on the remaining one of the set of pre-defined shape models.17. The vehicle control unit of claim 16, wherein the point cloud data is generated at a range gate distance characteristic of the object sensor device, the range gate distance characteristic producing a point cloud density relatable to the at least the portion of the object.18. The vehicle control unit of claim 16, wherein the point cloud data is generated at a range gate distance characteristic of the object sensor device.19. The vehicle control unit of claim 18, wherein the object sensor device has a predetermined origin location with respect to the each of the plurality of point cloud data samples of each of the set of pre-defined shape models.20. The vehicle control unit of claim 16, wherein the set of pre-defined shape models comprises at least one of: a passenger vehicle model; a sport utility vehicle model; a sport vehicle model; and a cargo vehicle model.",US10600199_B2.txt,"G06K9/00,G06T7/73",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general']","extending object detection and identification capability for an object sensor device A device and method for extending an object identification range in a vehicle environment are disclosed. In this regard, the embodiments may operate to solve to a remaining one of a set of pre-defined shape models to an object by successive iterations of point cloud data and associated closing distance to the object. Successive iterations of point cloud data may include receiving, via an object sensor device, the point cloud data, and detecting at least a portion of an object from the point cloud data, which portion is compared with each of a set of pre-defined shape models. The set is updated to include ones comparing favorably with the point cloud data. When the successive iterations solve to a singular remaining one of the set, an iterative match to the object may be based on the singular remaining one of the set of pre-defined shape models to identify the object.",computing; calculating; counting
US2021011489_A1,2020-09-25,2021-01-14,2012-10-02,"safeXai, Inc.","MEHTA, RISHPATTON, DAMIEN",67905735,road monitoring,event-based vehicle operation and event remediation,Embodiments of a method and/or system for facilitating event-based vehicle operation can include moving a vehicle along a route that defines coordinates of travel in a three-dimensional space; detecting an event in an area along the route based on an external signal; determining that the event is of interest to an entity based on one or more parameters associated with the vehicle; re-routing the vehicle toward the event along a shortened route that defines adjusted coordinates of travel in the three-dimensional space; and assisting in remediating the event (or an impact thereof) utilizing a vehicle component.,"We claim:1. A method comprising: operating a vehicle locomotion component moving a vehicle along a route that defines coordinates of travel in a three-dimensional space; detecting an event in an area along the route based on an external signal; determining that the event is of interest to an entity based on one or more parameters associated with the vehicle; re-routing the vehicle toward the event along a shortened route that defines adjusted coordinates of travel in the three-dimensional space, including dynamically modifying operation of the vehicle locomotion component moving the vehicle toward the event along the shortened route; and assisting in remediating an event impact utilizing a vehicle component.2. The method of claim 1, wherein assisting in remediating an event impact comprises utilizing a component mechanically coupled to the aerial vehicle.3. The method of claim 1, wherein detecting an event comprises detecting a hotspot; and wherein re-routing the vehicle toward the event comprises re-routing the vehicle toward the hotspot.4. The method of claim 1, wherein detecting an event comprises detecting an increased chance of fire in the area.5. The method of claim 1, wherein detecting an event comprises detecting a child abduction; and wherein re-routing the vehicle toward the event comprises re-routing the vehicle toward the child abduction.6. The method of claim 1, wherein assisting in remediating an event impact event comprises one of: capturing an image or deploying a net.7. The method of claim 1, wherein assisting in remediating an event impact comprises activating a fire extinguishing component.8. The method of claim 1, further comprising: collecting a plurality of social networking system posts, including the external signal; associating a subset of social networking system posts, from among the plurality of social networking system posts and including the external signal, to the area; and determining a keyword frequency corresponding to the subset of social networking system posts; and wherein detecting an event comprises detecting the event in response to the keyword frequency exceeding a historic keyword frequency associated with the area.9. The method of claim 1, further comprising: collecting sensor data while the vehicle is moving; and detecting a set of events based on the sensor data; and wherein detecting the event comprises: determining an event category probability corresponding to each event in the set of events; categorizing each event in the set of events with an event category based on the corresponding event category probability exceeding a category probability threshold; determining event parameters describing each event in the set of events; and identifying the event from the set of events based on the event parameters and an event category categorizing the event.10. The method of claim 1, wherein operating a vehicle locomotion component comprises operating a vehicle locomotion component of the vehicle selected from among: a submersible or a space-based craft; and wherein re-routing the vehicle comprises re-routing the vehicle selected from among: the submersible or the space-based craft.11. A system comprising: a processor; system memory coupled to the processor and storing instructions configured to cause the processor to: operate a vehicle locomotion component moving a vehicle along a route that defines coordinates of travel in a three-dimensional space; detect an event in an area along the route based on an external signal; determine that the event is of interest to an entity based on one or more parameters associated with the vehicle; re-route the vehicle toward the event along a shortened route that defines adjusted coordinates of travel in the three-dimensional space, including dynamically modifying operation of the vehicle locomotion component moving the vehicle toward the event along the shortened route; and assist in remediating an event impact utilizing a vehicle component.12. The computer system of claim 11, wherein instructions configured to assist in remediating an event impact comprise instructions configured to utilize a component mechanically coupled to the aerial vehicle.13. The computer system of claim 11, wherein instructions configured to detect an event comprise instructions configured to detect a hotspot; and wherein instructions configured to re-route the vehicle toward the event comprises instructions configured to re-route the vehicle toward the hotspot.14. The computer system of claim 11, wherein instructions configured to detect an event associated with the area comprise instructions configured to detect an increased chance of fire in the area.15. The computer system of claim 11, wherein instructions configured to detect an event comprises instructions configured to detect a child abduction; and wherein instructions configured to re-route the vehicle toward the event comprises instructions configured to re-route the vehicle toward the child abduction.16. The computer system of claim 11, wherein instructions configured to assist in remediating an event impact comprises one of: capturing an image or deploying a net17. The computer system of claim 11, wherein instructions configured to assist in remediating an event impact comprise instructions configured to activate a fire extinguishing component.18. The computer system of claim 1, further comprising instructions configured to: collect a plurality of social networking system posts, including the external signal; associate a subset of social networking system posts, from among the plurality of social networking system posts and including the external signal, to the area; and determine a keyword frequency corresponding to the subset of social networking system posts; and wherein instructions configured to detect an event comprise instructions configured to detect the event in response to the keyword frequency exceeding a historic keyword frequency associated with the area.19. The computer system of claim 1, further comprising instructions configured to: collect sensor data while the vehicle is moving; and detect a set of events based on the sensor data; and wherein instructions configured to detect an event comprise instructions configured to: determine an event category probability corresponding to each event in the set of events; categorize each event in the set of events with an event category based on the corresponding event category probability exceeding a category probability threshold; determine event parameters describing each event in the set of events; and identify the event from the set of events based on the event parameters and an event category categorizing the event.20. The computer system of claim 1, wherein instructions configured to operate a vehicle locomotion component comprise instructions configured to operate a vehicle locomotion component of the vehicle, the vehicle selected from among: a submersible or a space-based craft; and wherein instructions configured to re-route the vehicle comprises instructions configured to re-route the vehicle selected from among: the submersible or the space-based craft.",US2021011489_A1.txt,G05D1/10,{'controlling; regulating'},"['systems for controlling or regulating non-electric variables (for continuous casting of metals b22d11/16; valves per se f16k; sensing non-electric variables, see the relevant subclasses of g01; for regulating electric or magnetic variables g05f)']",event-based vehicle operation and event remediation Embodiments of a method and/or system for facilitating event-based vehicle operation can include moving a vehicle along a route that defines coordinates of travel in a three-dimensional space; detecting an event in an area along the route based on an external signal; determining that the event is of interest to an entity based on one or more parameters associated with the vehicle; re-routing the vehicle toward the event along a shortened route that defines adjusted coordinates of travel in the three-dimensional space; and assisting in remediating the event (or an impact thereof) utilizing a vehicle component.,controlling; regulating
US10573037_B2,2012-12-20,2020-02-25,2012-12-20,SRI INTERNATIONAL,"SAMARASEKERA, SUPUNKUMAR, RAKESHZHU ZHIWEIACHARYA, GIRISHAYAN, NECIP FAZILVILLAMIL, RYANWOLVERTON, MICHAEL JOHN",50974138,road monitoring,method and apparatus for mentoring via an augmented reality assistant,"A method and apparatus for training and guiding users comprising generating a scene understanding based on video and audio input of a scene of a user performing a task in the scene, correlating the scene understanding with a knowledge base to produce a task understanding, comprising one or more goals, of a current activity of the user, reasoning, based on the task understanding and a user's current state, a next step for advancing the user towards completing one of the one or more goals of the task understanding and overlaying the scene with an augmented reality view comprising one or more visual and audio representation of the next step to the user.","1. A computer-implemented method for utilizing augmented reality to assist a user in performing a real-world task comprising: generating a scene understanding based on an automated analysis of a video input and an audio input, the video input comprising a view of the user of a real-world scene during performance of a task, the audio input comprising speech of the user during performance of the task, and the automated analysis further comprising identifying an object in the real-world scene, extracting one or more visual cues to situate the user in relation to the identified object, wherein the user is situated by tracking a head orientation of the user; correlating the scene understanding with a knowledge database comprising at least data relating to models of respective procedures of different tasks to create a task understanding of the task in the scene understanding, wherein the task understanding comprises a set of goals relating to performance of the task in the scene understanding; processing the task understanding along with the models of respective procedures of the different tasks from the knowledge database to determine a next step of the task; generating a plurality of visual representations responsive to an ongoing interaction of the computer-implemented method with the user relating to the next step to achieve a goal; presenting the plurality of visual representations on a see-through display as an augmented overlay to the user's view of the real-world scene wherein the plurality of visual representations are rendered based on predicted head pose based on the tracked head orientation; guiding a user to perform the next step of the task during operation of the task via visual or audio output; analyzing actions of the user during the performance of the next task in response to the augmented overlay using the task understanding along with the models of respective procedures of the different tasks; and if the user has not completed all tasks, modifying or creating new visual representations to be generated and presented as an augmented overlay of a second next step of the task understanding.2. The method of claim 1 further comprising: training a user to perform the task understanding as a training exercise.3. The method of claim 1 wherein the visual representations of a next step of the task understanding are aligned with the real-world scene based on a pose of the user.4. The method of claim 1 further comprising: analyzing the audio input to generate a language understanding based on natural language input in the audio input; and identifying one or more speakers in the audio input where a topic of conversation in the real-world scene is bounded to a specific domain.5. The method of claim 4 further comprising: distinguishing acoustic realizations of sounds in the audio input using a statistical model to discriminate between a set of words.6. The method of claim 5 further comprising: determining a goal of the user in a given utterance in the audio input; and extracting a set of arguments associated with the goal.7. The method of claim 1 further comprising: producing the task understanding based on prior scene understanding information stored in the knowledge database and based on a semantic frame representing a user goal up to a current time.8. The method of claim 1 further comprising: prioritizing among each of the goals in the set of goals for the task understanding; and suggesting a next step to the user towards completion of the real-world task, based on said prioritizing.9. The method of claim 1, wherein at least one of the goals in the task understanding further comprises a set of sub-goals.10. An apparatus for utilizing augmented reality in assisting users in completing a complex physical task, the apparatus comprising: at least one processor; at least one input device; and at least one storage device storing processor-executable instructions which, when executed by the at least one processor, perform a method comprising: generating a scene understanding based on an automated analysis of a video input and an audio input, the video input comprising a view of the user of a real-world scene during performance of a task, the audio input comprising speech of the user during performance of the task, and the automated analysis further comprising identifying an object in the real-world scene, extracting one or more visual cues to situate the user in relation to the identified object, wherein the user is situated by tracking a head orientation of the user; correlating the scene understanding with a knowledge database comprising at least data relating to models of respective procedures of different tasks to create a task understanding of the task in the scene understanding, wherein the task understanding comprises a set of goals relating to performance of the task in the scene understanding; processing the task understanding along with the models of respective procedures of different tasks from the knowledge database to determine a next step of the task; generating a plurality of visual representations responsive to an ongoing interaction of the apparatus with the user relating to the next step to achieve a goal, wherein the plurality of visual representations are rendered based on predicted head pose based on the tracked head orientation; presenting the plurality of visual representations on a see-through display as an augmented overlay to the user's view of the real-world scene; analyzing actions of the user during the performance of the next task in response to the augmented overlay using the task understanding along with the models of respective procedures of the different tasks; and if the user has not completed all tasks, modifying or creating new visual representations to be generated and presented as an augmented overlay of a second next step of the task understanding.11. The apparatus of claim 10 wherein generating a scene understanding comprises observing visual cues and scene characteristics to generate the scene understanding.12. The apparatus of claim 10 wherein the method further aligns the visual representations of a next step in the set of goals with the real-world scene based on a pose of the user.13. The apparatus of claim 10, wherein the method further comprises: analyzing the audio input to generate a language understanding based on natural language input in the audio input; and identifying one or more speakers in the audio input where a topic of conversation is bounded to a specific domain.14. The apparatus of claim 13, wherein the method further comprises distinguishing acoustic realizations of sounds in the audio input using a statistical model to discriminate between a set of words.15. The apparatus of claim 14, wherein the method further comprises: determining a goal of the user in a given utterance in the audio input; and extracting a set of arguments associated with the goal.16. The apparatus of claim 10 wherein the method further comprises: producing the task understanding based on prior scene understanding information stored in the knowledge database and a semantic frame representing a user goal up to a current time.17. A computer-implemented method for utilizing augmented reality to assist a user in performing a real-world task, the method comprising: generating a scene understanding based on an automated analysis of a video input and an audio input, the video input comprising a view of the user of a real-world scene during performance of the task, the audio input comprising speech of the user during performance of the task, and the automated analysis further comprising identifying an object in the real-world scene, extracting one or more visual cues to situate the user in relation to the identified object, wherein the user is situated by tracking a head orientation of the user; correlating the scene understanding with a knowledge database comprising at least data relating to models of respective procedures of different tasks to create a task understanding of the task in the scene understanding, wherein the task understanding comprises a set of goals relating to performance of the task in the scene understanding; generating a plurality of visual representations responsive to an ongoing interaction of the computer-implemented method with the user relating to the next step to achieve a goal, the ongoing interaction comprising at least audible interactions with the user based on natural language utterances interpreted from the speech received from the user, each of the visual representations relating one or more of the natural language utterances to one or more objects recognized in the real-world scene; computing a head pose of the user and a set of visual occlusions in the scene as viewed from a viewpoint of the user; presenting the plurality of visual representations on a see-through display as an augmented overlay to the user's view of the one or more objects in the real-world scene, wherein the plurality of visual representations are rendered based on predicted head pose based on the tracked head orientation; analyzing actions of the user during the performance of the next task in response to the augmented overlay using the task understanding along with the models of respective procedures of the different tasks; and if the user has not completed all tasks, modifying or creating new visual representations to be generated and presented as an augmented overlay of a second next step of the task understanding.",US10573037_B2.txt,"G06F3/01,G06T11/00,G06T11/60,G09G5/00","{'education; cryptography; display; advertising; seals', 'computing; calculating; counting'}","['electric digital data processing (computer systems based on specific computational models g06n)', 'image data processing or generation, in general', 'image data processing or generation, in general', 'arrangements or circuits for control of indicating devices using static means to present variable information (lighting in general f21; arrangements for displaying electric variables or waveforms g01r3/00; devices or arrangements for the control of light beams g02f1/00; indicating of time by visual means g04b19/00, g04c17/00, g04g9/00; arrangements for transferring data between computers and peripheral equipment g06f3/00; visible signalling arrangements or devices g08b5/00; traffic control systems g08g; display, advertising, signs g09f, e.g. static indicating arrangements comprising an association of a number of separate sources or light control cells g09f9/00; static indicating arrangements comprising integral associations of a number of light sources h01j, h01k, h01l, h05b33/12; circuits in pulse counters for indicating the result h03k21/18; coding, decoding or code conversion, in general h03m; reproducing a picture or pattern using electric signals representing parts thereof and produced by scanning an original h04n)']","method and apparatus for mentoring via an augmented reality assistant A method and apparatus for training and guiding users comprising generating a scene understanding based on video and audio input of a scene of a user performing a task in the scene, correlating the scene understanding with a knowledge base to produce a task understanding, comprising one or more goals, of a current activity of the user, reasoning, based on the task understanding and a user's current state, a next step for advancing the user towards completing one of the one or more goals of the task understanding and overlaying the scene with an augmented reality view comprising one or more visual and audio representation of the next step to the user.",education; cryptography; display; advertising; seals computing; calculating; counting
US2018268532_A1,2017-03-16,2018-09-20,2017-03-16,GM GLOBAL TECHNOLOGY OPERATIONS,"LITKOUHI, BAKHTIAR B.WANG JINSONG",63372578,road monitoring,methods and systems for vehicle tire analysis using vehicle mounted cameras,"Methods and systems are provided for analyzing tires of a vehicle utilizing camera images from one or more cameras mounted on the vehicle (or infrastructure). In one example, the method includes obtaining camera images of one or more tires of a vehicle, utilizing one or more cameras that are mounted on the vehicle, during operation of the vehicle; and processing the camera images, via a processor, in order to generate an analysis of one or more of the tires based on the images that were obtained via the one or more cameras that are mounted on the vehicle.","1. A method comprising: obtaining camera images of one or more tires of a vehicle, utilizing one or more cameras, during operation of the vehicle; processing the camera images, via a processor, in order to generate an analysis of one or more of the tires based on the images that were obtained via the one or more cameras that are mounted on the vehicle; determining, using data provided by one or more sensors, a speed of the vehicle; and determining, using data provided by the one or more sensors, a turn angle of a turn of the vehicle; wherein the obtaining of the camera images comprises obtaining the camera images when the turn angle is greater than a first predetermined threshold and the speed is less than a second predetermined threshold.2. The method of claim 1, wherein the obtaining of the camera images comprises obtaining camera images of the one or more tires from one or more side cameras that are mounted on one or more sides of the vehicle, when the vehicle is making a turn.3. The method of claim 2, wherein the obtaining of the camera images comprises: obtaining first images of a tread of a respective tire from a first respective camera that is on a first side of the vehicle, during the turn; and obtaining second images of a sidewall of the respective tire from a second respective camera that is on a second side of the vehicle, opposite the first side, during the turn.4. (canceled)5. The method of claim 1, wherein the processing of the camera images comprises: determining a first tread pattern of a respective tire based on the camera images; and comparing the first tread pattern with one or more known second tread patterns from a historical database of a different tire, or the vehicles tires when rather new, having a known amount of wear.6. The method of claim 5, wherein the determining of the first tread pattern comprises determining the first tread pattern using a histogram of oriented gradient (HoG) values.7. The method of claim 5, wherein the determining of the first tread pattern comprises determining the first tread pattern using a machine learning method or a deep learning neural network model.8. The method of claim 5, further comprising: updating the historical database using the first tread pattern.9. The method of claim 1, wherein the processing of the camera images comprises: determining a first sidewall pattern of a respective tire based on the camera images; and comparing the first sidewall pattern with one or more known sidewall patterns of a different tire having a known amount of wear.10. The method of claim 1, further comprising: determining whether a warning is appropriate based on the analysis of the tire; and providing the warning, via instructions provided by the processor, when it is determined that the warning is appropriate.11. The method of claim 1, further comprising: obtaining additional sensor data pertaining to a wheel of the vehicle; wherein the processing of processing of the camera images further comprises processing the camera images using the additional sensor data pertaining to the wheel of the vehicle, via the processor, in order to facilitate the analysis of the one or more of the tires based on the images that were obtained via the one or more cameras that are mounted on the vehicle.12. A method comprising: obtaining camera images of tracks made by one or more tires of a vehicle, utilizing one or more cameras that are mounted on the vehicle, during operation of the vehicle; and processing the camera images of the tracks, via a processor, in order to generate an analysis of one or more of the tires based on the images that were obtained via the one or more cameras that are mounted on the vehicle; wherein the processing of the camera images of the tracks comprises, via a processor: determining a first tread pattern of a respective tire based on the tracks made by the one or more tires, using the camera images of the tracks; and comparing the first tread pattern with one or more known second tread patterns of a different tire having a known amount of wear.13. The method of claim 12, wherein the obtaining of the camera images comprises obtaining camera images of the tracks from a rear camera when the vehicle is driving forward.14. The method of claim 12, wherein the obtaining of the camera images comprises obtaining camera images of the tracks from a front camera when the vehicle is driving in reverse.15. The method of claim 12, further comprising: determining, using data provided by one or more sensors or map data, a condition of a road or type of the road on which the vehicle is travelling; wherein the camera images are obtained and processed when the condition represents not a dry road, but when the condition represents a wet, snowy, sandy, or muddy road.16. (canceled)17. The method of claim 12, wherein the determining of the first tread pattern comprises determining the first tread pattern using a histogram of oriented gradient (HoG) values.18. The method of claim 12, wherein the determining of the first tread pattern comprises determining the first tread pattern using a neural network model machine learning method or a deep learning neural network model.19. The method of claim 12, further comprising: obtaining additional sensor data pertaining to a wheel of the vehicle; wherein the processing of processing of the camera images further comprises processing the camera images using the additional sensor data pertaining to the wheel of the vehicle, via the processor, in order to facilitate the analysis of the one or more of the tires based on the images that were obtained via the one or more cameras that are mounted on the vehicle.20. A vehicle comprising: one or more tires; one or more cameras onboard the vehicle, the one or cameras mounted on the vehicle as part of the vehicle, or low on an infrastructure, as part of the vehicle, the one or more cameras configured to generate camera images of tracks made by one or more of the tires; and a processor onboard the vehicle and configured to process the camera images in order to generate an analysis of one or more of the tires based on the images that were obtained via the one or more cameras that are mounted on the vehicle, wherein the processor is configured to generate the analysis by: determining a first tread pattern of a respective tire based on the tracks made by the one or more tires, using the camera images of the tracks; and comparing the first tread pattern with one or more known second tread patterns of a different tire having a known amount of wear.",US2018268532_A1.txt,"B60C11/24,G01M17/02,G06T7/00","{'measuring; testing', 'computing; calculating; counting', 'vehicles in general'}","['vehicle tyres; tyre inflation; tyre changing; connecting valves to inflatable elastic bodies in general; devices or arrangements related to tyres', 'testing static or dynamic balance of machines or structures; testing of structures or apparatus, not otherwise provided for', 'image data processing or generation, in general']","methods and systems for vehicle tire analysis using vehicle mounted cameras Methods and systems are provided for analyzing tires of a vehicle utilizing camera images from one or more cameras mounted on the vehicle (or infrastructure). In one example, the method includes obtaining camera images of one or more tires of a vehicle, utilizing one or more cameras that are mounted on the vehicle, during operation of the vehicle; and processing the camera images, via a processor, in order to generate an analysis of one or more of the tires based on the images that were obtained via the one or more cameras that are mounted on the vehicle.",measuring; testing computing; calculating; counting vehicles in general
US2020191943_A1,2020-02-22,2020-06-18,2015-07-17,ORIGIN WIRELESS,"AU, OSCAR CHI-LIMLIU, K. J. RAYWANG, BEIBEIWU, CHENSHUZHANG, FENG",71070862,gps,"method, apparatus, and system for wireless object tracking","Methods, apparatus and systems for wireless object tracking are described. In one example, a described wireless tracking system comprises: a transmitter, a receiver, and a processor. The transmitter is configured for transmitting a first wireless signal using a plurality of transmit antennas towards at least one object in a venue through a wireless multipath channel of the venue. The receiver is configured for: receiving a second wireless signal using a plurality of receive antennas through the wireless multipath channel between the transmitter and the receiver. The second wireless signal differs from the first wireless signal due to the wireless multipath channel and a modulation of the first wireless signal by the at least one object. The processor is configured for obtaining a set of channel information (CI) of the wireless multipath channel based on the second wireless signal received by the receiver, and tracking the at least one object simultaneously based on the set of CI. Each CI in the set is associated with a respective one of the plurality of transmit antennas and a respective one of the plurality of receive antennas.","We claim:1. A wireless tracking system, comprising: a transmitter configured for transmitting a first wireless signal using a plurality of transmit antennas towards at least one object in a venue through a wireless multipath channel of the venue; a receiver configured for receiving a second wireless signal using a plurality of receive antennas through the wireless multipath channel between the transmitter and the receiver, wherein the second wireless signal differs from the first wireless signal due to the wireless multipath channel and a modulation of the first wireless signal by the at least one object; and a processor configured for: obtaining a set of channel information (CI) of the wireless multipath channel based on the second wireless signal received by the receiver, wherein each CI in the set is associated with a respective one of the plurality of transmit antennas and a respective one of the plurality of receive antennas, and tracking the at least one object simultaneously based on the set of CI.2. The wireless tracking system of claim 1, wherein the processor is further configured for: computing a beamforming based on the set of CI; and tracking the at least one object based on the beamforming, wherein the beamforming is computed based on at least one of: an analog beamforming, digital beamforming, non-adaptive beamforming, adaptive beamforming, parametric beamforming, non-parametric beamforming, compressive sensing based beamforming, minimum variance distortionless response (MVDR) beamformer, a solution to an inverse problem, direction-of-arrival estimation, tomographic reconstruction, a maximum likelihood method, a maximum entropy method, a covariance method, peak detection of discrete Fourier transform, a super-resolution method, a parameter-free super-resolution method, constrained minimization of a distortion in a target direction, constrained minimization of a power of interference and noise from directions other than the target direction, an eigen-decomposition, singular value decomposition (SVD), another decomposition, a signal subspace determination, a noise subspace determination, a projection, an autoregressive (AR) model, an autoregressive-moving-average (ARMA) model, steering vector associated with the direction, an analysis of a matrix comprising a signal matrix, a correlation matrix, a covariance matrix, an autocorrelation matrix, an auto-covariance matrix, an inverse of a matrix, and another matrix, a multiple signal classification (MUSIC), MUSIC-like method, time-reversal MUSIC (TR-MUSIC), Capon beamformer, Butler matrix, Pisarenko harmonic decomposition, iterative sparse asymptotic minimum variance (SAMV), a spectral estimation, an Akaike Information Criterion (AIC), Bayesian information Criterion (BIC), Generalized Information Criterion (GIC), a criterion variant, a model order selection algorithm, and another beamforming method.3. The wireless tracking system of claim 1, wherein the processor is further configured for: computing a spatial spectrum associated with a set of propagation delays for a set of directions based on the set of CI and a beamforming; and tracking the at least one object based on the spatial spectrum, wherein each direction of the set of directions is associated with at least one of: an angle, an azimuth, an elevation angle, a coordinate, a steering vector, a link between a transmit antenna of the transmitter and a receive antenna of the receiver, a path associated with an object in the direction, and another direction descriptor, each propagation delay of the set of propagation delays is associated with at least one of: a time lag, a time delay, a time index, a propagation time, a time-of-flight, distance, a range, and another time descriptor.4. The wireless tracking system of claim 1, wherein the processor is further configured for: computing a set of directional quantities based on the set of CI and a beamforming, wherein each directional quantity of the set of directional quantities is associated with a propagation delay and a direction with respect to the receiver, wherein each of the set of directional quantities is computed for a direction based on at least one of: analog beamforming, digital beamforming, non-adaptive beamforming, adaptive beamforming, parametric beamforming, non-parametric beamforming, compressive sensing based beamforming, direction-of-arrival estimation, tomographic reconstruction, a solution to an inverse problem, a maximum likelihood method, a maximum entropy method, a covariance method, an eigen-analysis of a signal matrix or an autocorrelation matrix or a covariance matrix, eigen-decomposition, singular value decomposition (SVD), another decomposition, a projection, peak detection of discrete Fourier transform, a super-resolution method, a parameter-free superresolution method, an autoregressive (AR) model, an autoregressive-moving-average (ARMA) model, a correlation matrix, an inverse of a matrix, steering vector associated with the direction, minimum variance distortionless response (MVDR) beamformer, minimum power of interference and noise from other directions, distortionless response towards the looking direction, Capon beamformer, Butler matrix, multiple signal classification (MUSIC), MUSIC-like method, time-reversal MUSIC (TR-MUSIC), Pisarenko harmonic decomposition, iterative sparse asymptotic minimum variance (SAMV), spectral estimation, an Akaike Information Criterion (AIC), Bayesian information Criterion (BIC), Generalized Information Criterion (GIC), a criterion variant, a model order selection algorithm, a signal subspace determination, a noise subspace determination, a projection, and another digital beamforming method; and tracking the at least one object based on the set of directional quantities, wherein the set of directional quantities comprise at least one of: a directional weight, a spatial spectrum, a distribution of an information of reflected signals in the venue, a distribution of energy of reflected signals in the venue, a heat map, and another directional quantity.5. The wireless tracking system of claim 1, wherein the processor is further configured for: computing a set of point-of-interests (PoIs) based on at least one of: the set of CI, and a set of directional quantities computed based on the set of CI and a beamforming, wherein each PoI of the set of PoIs is associated with a presence of the at least one object, and is a point in the venue associated with a propagation delay and a direction; and tracking the at least one object based on the set of PoIs which forms a visual representation of a movement of the at least one object.6. The wireless tracking system of claim 5, wherein the processor is further configured for: computing a range-of-interest (RoI) based on the set of CI and the set of directional quantities, wherein the RoI comprises a set of selected propagation delays each of which is associated with a respective range, each of the set of selected propagation delays is associated with the presence of the at least one object, and the set of PoIs is computed based on the RoI.7. The wireless tracking system of claim 6, wherein the processor is further configured for: computing a variability measure of a set of directional quantities associated with a propagation delay, wherein each directional quantity of the set of directional quantities is computed based on the set of CI and a beamforming, wherein the variability measure comprises at least one of: a standard deviation, a variance, a measure of variation, a measure of spread, a measure of dispersion, a measure of deviation, a measure of divergence, a range, an interquartile range, a total variation, an absolute deviation, and a total deviation; selecting the propagation delay and associating the propagation delay with the presence of the at least one object when the variability measure is greater than a threshold; and adding the selected propagation delay into the RoI.8. The wireless tracking system of claim 7, wherein the processor is further configured for: computing a set of variability measures associated with a set of propagation delays, wherein each of the set of variability measures is associated with a respective one of the set of propagation delays; and determining the threshold based on a function of the set of variability measures, wherein the function comprises at least one of: a mean, median, mode, percentile, weighted average, geometric mean, harmonic mean, trimmed mean, variance, scaling, offset, linear function, nonlinear function, monotonic non-decreasing function, monotonic non-increasing function, and a hybrid function.9. The wireless tracking system of claim 6, wherein: the RoI comprises at least two clusters of consecutive propagation delay values; and the processor is further configured for: computing a distance between two clusters of the RoI; and expanding the RoI by adding a missing propagation delay value between the two clusters to the RoI when the distance is less than a threshold.10. The wireless tracking system of claim 6, wherein: the RoI comprises at least one disjoint cluster of consecutive propagation delay values; and the processor is further configured for: examining directional quantities associated with propagation delays in the RoI in a particular direction; identifying a cell of points in the particular direction, wherein propagation delay values associated with the cell of points are a subset of a disjoint cluster of the RoI, and each point in the cell has an associated directional quantity greater than a threshold; selecting, in the cell of points, a characteristic point associated with a propagation delay with at least one of the following characteristics of the cell: a minimum, maximum, mid-point, mean, centroid with respect to (w.r.t.) the directional quantities, a mean w.r.t. the directional quantities, a weighted mean w.r.t. the directional quantities, a median w.r.t. the directional quantities, a mode w.r.t. the directional quantities, a percentile w.r.t. the directional quantities, a minimum w.r.t. the directional quantities, a maximum w.r.t. the directional quantities, a local minimum w.r.t. the directional quantities, a local maximum w.r.t. the directional quantities, a minimum slope w.r.t. the directional quantities, a maximum slope w.r.t. the directional quantities, a local minimum slope w.r.t. the directional quantities, a local maximum slope w.r.t. the directional quantities, and another characteristics; and adding the selected characteristic point as a PoI to the set of PoIs.11. The wireless tracking system of claim 6, wherein: the RoI comprises at least one disjoint cluster of consecutive propagation delay values; and the processor is further configured for: examining directional quantities associated with propagation delays in the RoI; identifying a set of points in the venue, wherein propagation delay values associated with the set of points are a subset of a particular disjoint cluster of the RoI, and each point in the set has an associated directional quantity satisfying a first criterion; and for each direction associated with the set of points: selecting at least one characteristic point based on at least one of: a subset of the set of points in the direction and the associated directional quantities, and adding the at least one selected characteristic point as a PoI to the set of PoIs.12. The wireless tracking system of claim 11, wherein the processor is further configured for: refining the points in the set of PoIs, wherein the refining comprises at least one of: smoothing, filtering, super-resolution, interpolation, 1-dimensional (1-D) interpolation, 2-D interpolation, 3-D interpolation, transformation, transformation from polar coordinate to rectangular coordinate, and another processing.13. The wireless tracking system of claim 11, wherein the processor is further configured for: cleaning the set of PoIs based on the associated directional quantities by at least one of: resampling the set of PoIs, weighted resampling based on the associated directional quantities, removing a PoI from the set of PoIs, adding a PoI to the set of PoIs, eliminating an insignificant PoI from the set of PoI, eliminating a PoI with an insignificant associated directional quantity, eliminating a PoI with an associated directional quantity less than a threshold, eliminating a PoI with an associated directional quantity less than a dynamic threshold, eliminating a PoI with an associated directional quantity less than an adaptive threshold, wherein the adaptive threshold is monotonic non-decreasing with respect to propagation delay, eliminating a PoI with an associated directional quantity less than an adaptive threshold, wherein the adaptive threshold is a piecewise linear function of the range associated with the propagation delay, filtering, linear filtering, nonlinear filtering, and another operation on the set of PoIs.14. The wireless tracking system of claim 11, wherein the processor is further configured for: processing of the set of PoIs and the associated directional quantities, and tracking the at least one object based on the processing, wherein the processing comprises at least one of: clustering, identification of at least one cluster, associating the at least one cluster with the at least one object, k-mean clustering, resampling, weighted resampling, morphological operation, thresholding, silhouette analysis, computing a silhouette value associated with each PoI, computing a silhouette value based on at least one of: an average distance from a PoI to another PoI in the same cluster of PoI, a minimum distance from PoI to PoI in a different cluster, and a maximization of the two distances, maximizing a silhouette value, computing a similarity score between a PoI and a cluster, maximizing the similarity score, computing a characterization score regarding how similar a PoI is to its own cluster of PoI compared to a separation distance to its neighboring clusters of PoI, maximizing the characterization score, associating the set of PoIs with a time stamp, processing the set of PoIs jointly with another set of PoIs associated with another time stamp, computing a global centroid of the set of PoIs, iteratively expanding to all neighboring points that satisfy a density constraint, iteratively expanding to a set of neighboring points each with a density larger than a threshold, iteratively expanding to a set of neighboring points each with a density larger than a minimum number of points within a neighboring circle of a particular radius, projecting a set of points extended from the centroid to a plane, computing an area of the projected points from the set of points, computing the area in the plane, computing a medoid of each cluster, computing an intra-cluster medoid distance, computing a distance of every point in a cluster to its medoid, computing a distance between medoids of two clusters, and another processing.15. The wireless tracking system of claim 5, wherein the processor is further configured for: determining a number of clusters in the set of PoIs by applying a clustering algorithm that optimizes a cost function, wherein the clustering algorithm comprises at least one of: connectivity-based clustering, hierarchical clustering, centroid-based clustering, k-means algorithm, vector quantization, distribution-based clustering, multivariate normal distribution based clustering, Gaussian-mixture based clustering, statistical distribution fitting, maximum likelihood, expectation-maximization algorithm, density-based clustering, DBSCAN, OPTICS, dense connected regions identification, subspace-based clustering, biclustering, co-clustering, two-mode-clustering, group-based clustering, model-less clustering, graph-based clustering, clique-based clustering, quasi-clique based clustering, HCS clustering, signed graph based clustering, balance theory based clustering, bifurcated graph based clustering, clustererability axiom based clustering, neural network (NN)-based clustering, self-organization map based clustering, unsupervised NN based clustering, principal component analysis, independent component analysis, hard clustering, soft clustering, fuzzy clustering, strict partition clustering, strict partition clustering with outliers, overlapping clustering, alternative clustering, multi-view clustering, and another clustering; wherein optimizing the cost function comprises at least one of: maximization, constrained maximization, global maximization, local maximization, maximization restricted to medoid, minimization, constrained minimization, global minimization, local minimization, minimization restricted to medoid, and another optimization; wherein the cost function is based on: a distance, an average distance, an intra-cluster distance, an inter-cluster distance, an average distance from a PoI in a cluster to other PoI in the same cluster, a minimum average distance from PoI in a cluster to PoI in another cluster, medoid distance, intra-cluster medoid distance, inter-cluster medoid distance, Euclidean distance, absolute distance, L-1 norm, L-2 norm, L-k norm, a silhouette value, an average silhouette value, an intra-cluster silhouette value, an inter-cluster silhouette value, a silhouette value based on a distance, a silhouette value based on an intra-cluster distance and an inter-cluster distance, a silhouette value based on an average distance from a PoI in a cluster to other PoI in the same cluster and a minimum average distance from PoI in a cluster to PoI in another cluster, a similarity score, an average similarity score, an intra-cluster similarity score, an inter-cluster similarity score, a similarity score based on a distance, a similarity score based on an intra-cluster distance and an inter-cluster distance, a similarity score based on an average distance from a PoI in a cluster to other PoI in the same cluster and a minimum average distance from PoI in a cluster to PoI in another cluster, an adaptive silhouette value based on a number of temporally neighboring silhouette values, an adaptive silhouette value based on a number of past silhouette values, an adaptive silhouette value based on application of a weighting function, wherein the weighting function is a function of clusterNum which is the number of clusters, an adaptive silhouette value based on application of a weighting function, wherein the weighting function has a maximum when clusterNum is equal to a majority value of a set of past clusterNum, and another cost function.16. The wireless tracking system of claim 15, wherein the processor is further configured for: validating the clusters in the set of PoIs based on a medoid distance; splitting a cluster when an intra-cluster medoid distance of the cluster is greater than a threshold; and combining two clusters when an inter-cluster medoid distance of the two clusters is less than another threshold.17. The wireless tracking system of claim 15, wherein the processor is further configured for: associating the number of clusters with the at least one object; and computing at least one of: a quantity of the at least one object, a location of an object of the at least one objects based on a geometric medoid of all the points belonging to an associated cluster, and a height of the object based on a maximum of heights associated with all the points belonging to the associated cluster.18. The wireless tracking system of claim 15, wherein the processor is further configured for: constructing at least one trajectory of the at least one object based on the simultaneous tracking of the at least one object, wherein each of the at least one trajectory comprises locations of an object over time; tracking the at least one object continuously; associating a cluster of PoIs with at least one of: an existing trajectory and a new trajectory, based on at least one of: a shortest distance between a location of the cluster and a last known location of an existing trajectory, a shortest distance between the location of the cluster and last few known locations of an existing trajectory, a shortest distance between the location of the cluster and at least one location of an existing trajectory, a shortest distance between a geometric medoid of the cluster to a last known location of an existing trajectory, a shortest distance between a geometric medoid of the cluster to last few known locations of an existing trajectory, a shortest distance between a geometric medoid of the cluster to at least one location of an existing trajectory, a shortest distance between more than one locations of the cluster and a last known location of an existing trajectory, a shortest distance between the more than one locations of the cluster and last few known locations of an existing trajectory, a shortest distance between the more than one locations of the cluster and at least one location of an existing trajectory, a shortest weighted distance between a location of the cluster and a last known location of an existing trajectory that has not disappeared for a duration more than a threshold, a shortest weighted distance between the location of the cluster and last few known locations of an existing trajectory that has not disappeared for a duration more than a threshold, a shortest weighted distance between the location of the cluster and at least one location of an existing trajectory that has not disappeared for a duration more than a threshold, a shortest weighted distance between a geometric medoid of the cluster and a last known location of an existing trajectory that has not disappeared for a duration more than a threshold, a shortest weighted distance between the geometric medoid of the cluster and last few known locations of an existing trajectory that has not disappeared for a duration more than a threshold, a shortest weighted distance between the geometric medoid of the cluster and at least one location of an existing trajectory that has not disappeared for a duration more than a threshold, a shortest weighted distance between a location of the cluster and a last known location of an existing trajectory that has disappeared for a duration more than a threshold, a shortest weighted distance between the location of the cluster and last few known locations of an existing trajectory that has disappeared for a duration more than a threshold, a shortest weighted distance between the location of the cluster and at least one location of an existing trajectory that has disappeared for a duration more than a threshold, a shortest weighted distance between a location of the cluster and a last known location of an existing trajectory that has been terminated, a shortest weighted distance between the location of the cluster and last few known locations of an existing trajectory that has been terminated, a shortest weighted distance between the location of the cluster and at least one location of an existing trajectory that has been terminated, a maximum similarity score between a location of the cluster and a last known location of an existing trajectory, a maximum similarity cost between the location of the cluster and last few known locations of an existing trajectory, a maximum similarity cost between the location of the cluster and at least one location of an existing trajectory, a maximum similarity score between a geometric medoid of the cluster and a last known location of an existing trajectory, a maximum similarity cost between the geometric medoid of the cluster and last few known locations of an existing trajectory, a maximum similarity cost between the geometric medoid of the cluster and at least one location of an existing trajectory, a maximum similarity score between more than one locations of the cluster and a last known location of an existing trajectory, a maximum similarity cost between the more than one locations of the cluster and last few known locations of an existing trajectory, a maximum similarity cost between the more than one locations of the cluster and at least one location of an existing trajectory, a maximum similarity score between a directional quantity associated with a location of the cluster and a directional quantity associated with a last known location of an existing trajectory, a maximum similarity cost between the directional quantity associated with the location of the cluster and directional quantities associated with last few known locations of an existing trajectory, a maximum similarity cost between the directional quantity associated with the location of the cluster and directional quantities associated with at least one location of an existing trajectory, a maximum similarity score between a directional quantity associated with a geometric medoid of the cluster and a directional quantity associated with a last known location of an existing trajectory, a maximum similarity cost between the directional quantity associated with the geometric medoid of the cluster and directional quantities associated with last few known locations of an existing trajectory, a maximum similarity cost between the directional quantity associated with the geometric medoid of the cluster and directional quantities associated with at least one location of an existing trajectory, a maximum similarity score between directional quantities associated with more than one locations of the cluster and a directional quantity associated with a last known location of an existing trajectory, a maximum similarity cost between the directional quantities associated with the more than one locations of the cluster and directional quantities associated with last few known locations of an existing trajectory, a maximum similarity cost between the directional quantities associated with the more than one locations of the cluster and directional quantities associated with at least one location of an existing trajectory, and another cost associated with the cluster and the existing trajectory; and updating a quantity of the at least one object.19. The wireless tracking system of claim 5, wherein the processor is further configured for: computing a direction of interest (DoI) based on the set of CI and the set of directional quantities, wherein the DoI comprises a set of directions that are associated with the presence of the at least one object, and the set of PoIs is computed based on the DoI; and refining the DoI by combining and/or dividing clusters of directions.20. The wireless tracking system of claim 1, wherein the processor is further configured for: constructing at least one trajectory of the at least one object based on the simultaneous tracking of the at least one object, wherein each of the at least one trajectory comprises locations of one object over time; adding a current location of an object to at least one of: an existing trajectory associated with the object, and a new trajectory; splitting a first trajectory associated with a first object into: the first trajectory associated with the first object, and at least one new trajectory each associated with a new object; terminating a second trajectory associated with a second object when the second object disappears for a duration longer than a threshold; updating a quantity of the at least one object; and tracking the at least one object continuously.21. The wireless tracking system of claim 1, wherein: tracking the at least one object comprises computing at least one of: a real-time tracking, an offline tracking, a presentation of the tracking, a joint tracking based on another input, a joint tracking based on the receiver and another receiver, a presence of an object, an absence of an object, an appearance of an object, a disappearance of an object, a re-appearance of an object, a blocking of an object by another object, a locationing of an object, a positioning of an object, a tracking of position of an object, a location of an object, a change of the location, a time-varying location as the object moves, a speed, an acceleration, an angular speed, an angular acceleration, a direction, a change of direction, a location on a map, a path from the location of the object to another location, a presentation on a map, a spatial information of an object, a spatial description of an object, an azimuth, an elevation, a distance, an angle, a polar coordinate, a rectangular coordination, a geometric coordinate, a coordinate of a projection onto a spatial subspace, a length, an area, a volume, a shape, a silhouette, a movement of an object, a motion of an object, an activity of an object, a gesture of an object, a gait of an object, an expression of an object, a posture of an object, a handwriting of an object, a behavior of an object, a trend of the object, a history of the movement, a summary of the movement, a recognition of a motion, a meaning of a motion, a presence, an absence, a beginning, an ending, a duration, a change, a pause, a discontinuity, an analytics associated with a periodic motion of an object, a frequency, a period, a mode, a breathing rate, a heart rate, a sleep stage, a well being, an analytics associated with a transient motion of an object, a fall down, a danger, a dangerous motion, a health condition, an approaching, a receding, a daily activity, a repeated activity, a seasonal activity, a detection of a targeted movement of an object, a detection of: a fall, a detection of a dangerous movement, a gesture, a daily activity, a recurring movement, a non-recurring movement, and another movement, a detection of a targeted state of an object, a detection of a mode of an object, a silhouette of an object, a shape of an object, a posture of an object, a reconstruction of an object, a 3-dimensional reconstruction of an object, an identification of the number of objects, a count of the at least one object, an identification of an object, an identification of an aspect of an object, an identification of a material of an object, a recognition of an object, and another analytics.22. The wireless tracking system of claim 1, wherein: the plurality of transmit antennas are arranged in a first lattice with at least one first characteristic spacing; the plurality of receive antennas are arranged in a second lattice with at least one second characteristic spacing; the transmitter and the receiver are placed in at least one of the following manners: placed at two different locations in the venue, collocated and placed at similar locations in the venue, coupled together as a same device comprising an integrated circuit (IC) that transmits and receives wireless signals, and coupled together as a same device comprising the IC and the processor.23. The wireless tracking system of claim 1, wherein: there are multiple pairs of transmitters and receivers in the venue; a respective receiver of each pair receives a respective wireless signal asynchronously transmitted from a respective transmitter of the pair and obtains asynchronously a respective set of CI; at least one pair of transmitter and receiver are collocated; at least one pair of transmitter and receiver are placed at two different locations in the venue; and the at least one object is tracked simultaneously based on the sets of CI.24. An apparatus for wireless tracking in a venue where a transmitter and a receiver are located, comprising: at least one of the transmitter and the receiver, wherein: the transmitter is configured for transmitting a first wireless signal using a plurality of transmit antennas towards at least one object in the venue through a wireless multipath channel of the venue, the receiver is configured for receiving a second wireless signal using a plurality of receive antennas through the wireless multipath channel between the transmitter and the receiver, and the second wireless signal differs from the first wireless signal due to the wireless multipath channel and a modulation of the first wireless signal by the at least one object; and a processor configured for: obtaining a set of channel information (CI) of the wireless multipath channel based on the second wireless signal, wherein each CI in the set is associated with a respective one of the plurality of transmit antennas and a respective one of the plurality of receive antennas, and tracking the at least one object simultaneously based on the set of CI.25. The apparatus of claim 24, wherein the processor is further configured for: computing a spatial spectrum associated with a set of propagation delays for a set of directions based on the set of CI and a beamforming; and tracking the at least one object based on the spatial spectrum.26. The apparatu",US2020191943_A1.txt,"G01S13/00,G01S13/72,G01S7/41",{'measuring; testing'},"['radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves']","method, apparatus, and system for wireless object tracking Methods, apparatus and systems for wireless object tracking are described. In one example, a described wireless tracking system comprises: a transmitter, a receiver, and a processor. The transmitter is configured for transmitting a first wireless signal using a plurality of transmit antennas towards at least one object in a venue through a wireless multipath channel of the venue. The receiver is configured for: receiving a second wireless signal using a plurality of receive antennas through the wireless multipath channel between the transmitter and the receiver. The second wireless signal differs from the first wireless signal due to the wireless multipath channel and a modulation of the first wireless signal by the at least one object. The processor is configured for obtaining a set of channel information (CI) of the wireless multipath channel based on the second wireless signal received by the receiver, and tracking the at least one object simultaneously based on the set of CI. Each CI in the set is associated with a respective one of the plurality of transmit antennas and a respective one of the plurality of receive antennas.",measuring; testing
US9760806_B1,2016-05-11,2017-09-12,2016-05-11,TCL RESEARCH AMERICA,"WANG, HAOHONGREN, XIAOBONING, GUANGHANBO, WENQIANG",59752846,road monitoring,method and system for vision-centric deep-learning-based road situation analysis,"In accordance with various embodiments of the disclosed subject matter, a method and a system for vision-centric deep-learning-based road situation analysis are provided. The method can include: receiving real-time traffic environment visual input from a camera; determining, using a ROLO engine, at least one initial region of interest from the real-time traffic environment visual input by using a CNN training method; verifying the at least one initial region of interest to determine if a detected object in the at least one initial region of interest is a candidate object to be tracked; using LSTMs to track the detected object based on the real-time traffic environment visual input, and predicting a future status of the detected object by using the CNN training method; and determining if a warning signal is to be presented to a driver of a vehicle based on the predicted future status of the detected object.","1. A method for vision-centric deep-learning-based road situation analysis, comprising: receiving real-time traffic environment visual input from at least one camera; determining, using a recurrent you only look once (ROLO) engine, at least one initial region of interest from the real-time traffic environment visual input by using a convolutional neural networks (CNN) training method; verifying, using the recurrent you only look once (ROLO) engine, the at least one initial region of interest to determine if a detected object in the at least one initial region of interest is a candidate object to be tracked by using the CNN training method; in response to determining the detected object is a candidate object, tracking, using a plurality of long short-term memory units (LSTMs), the detected object based on the real-time traffic environment visual input, and predicting a future status of the detected object by using the CNN training method; and determining if a warning signal is to be presented to a driver of a vehicle based on the predicted future status of the detected object.2. The method of claim 1, wherein tracking the detected object further comprises: tracking the detected object based at least in partial on real-time signals of the detected object from a lidar sensor and an infrared sensor.3. The method of claim 1, wherein the future status of the detected object is determined by calculating a distance between the detected object and the vehicle, a speed of the detected object, and a moving direction of the detected object.4. The method of claim 1, wherein the candidate object to be tracked comprises: a road line, another vehicle near the vehicle, a pedestrian, an obstacle in front of the vehicle, and a traffic sign.5. The method of claim 1, wherein the CNN training method comprises: a pre-training phase of convolutional layers for feature learning; a you only look once (YOLO) training phase for object detection; and a LSTM training phase for object tracking.6. The method of claim 5, wherein the pre-training phase of convolutional layers comprises generating a feature cube to represent visual features of a plurality of detected objects.7. The method of claim 6, wherein the YOLO training phase for object detection comprises translating the feature cube to a tensor representation.8. The method of claim 5, before the LSTM training phase, further comprising: encoding the feature cube into feature vectors.9. The method of claim 5, wherein the LSTM training phase for object tracking is performed in together with a Kalman filter.10. The method of claim 1, wherein the CNN training method comprises using a convolutional neural networks having a plurality of convolutional layers followed by two fully connected layers.11. A system for vision-centric deep-learning-based road situation analysis, comprising: at least one camera for receiving real-time traffic environment visual input; a recurrent you only look once (ROLO) engine configured for: determining at least one initial region of interest from the real-time traffic environment visual input by using a convolutional neural networks (CNN) training method, and verifying the at least one initial region of interest to determine if a detected object in the at least one initial region of interest is a candidate object to be tracked by using the CNN training method; a plurality of long short-term memory units (LSTMs) configured for: in response to determining the detected object is a candidate object, tracking the detected object based on the real-time traffic environment visual input, and predicting a future status of the detected object by using the CNN training method; and a decision making agent for determining if a warning signal to be presented to a driver of a vehicle based on the predicted future status of the detected object.12. The system of claim 11, further comprising: a sensor fusion configured for processing real-time signals of the detected object from a lidar sensor and an infrared sensor.13. The system of claim 11, wherein the plurality of long short-term memory units (LSTMs) are further configured for calculating a distance between the detected object and the vehicle, a speed of the detected object, and a moving direction of the detected object.14. The system of claim 11, further comprising: a road line recognition module for determining if the detected object is a road line; a pedestrian detection module for determining if the detected object is a pedestrian; an obstacle detection module for determine if the detected object is an obstacle in front of the vehicle; and a traffic sign recognition module for determine if the detected object is a traffic sign.15. The system of claim 11, wherein the recurrent you only look once (ROLO) engine comprises a convolutional neural networks (CNN) for generating a feature cube to represent visual features of a plurality of detected objects.16. The system of claim 15, wherein the convolutional neural networks (CNN) is further configured for translating the feature cube to a tensor representation.17. The system of claim 15, wherein the convolutional neural networks (CNN) is further configured for encoding the feature cube into feature vectors before a LSTM training phase.18. The system of claim 17, wherein the plurality of long short-term memory units (LSTMs) are further configured for performing the LSTM training phase for object tracking in together with a Kalman filter.19. The system of claim 15, wherein the convolutional neural networks (CNN) has a plurality of convolutional layers followed by two fully connected layers.20. The system of claim 11, further comprising a human-computer interface to present the warning signal to the driver of the vehicle.",US9760806_B1.txt,"B60W50/14,G06K9/00,G06K9/66,G06T7/00,G06T7/20","{'computing; calculating; counting', 'vehicles in general'}","['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general', 'image data processing or generation, in general']","method and system for vision-centric deep-learning-based road situation analysis In accordance with various embodiments of the disclosed subject matter, a method and a system for vision-centric deep-learning-based road situation analysis are provided. The method can include: receiving real-time traffic environment visual input from a camera; determining, using a ROLO engine, at least one initial region of interest from the real-time traffic environment visual input by using a CNN training method; verifying the at least one initial region of interest to determine if a detected object in the at least one initial region of interest is a candidate object to be tracked; using LSTMs to track the detected object based on the real-time traffic environment visual input, and predicting a future status of the detected object by using the CNN training method; and determining if a warning signal is to be presented to a driver of a vehicle based on the predicted future status of the detected object.",computing; calculating; counting vehicles in general
US2016325680_A1,2016-05-04,2016-11-10,2015-05-04,KAMAMA,"CURTIS, ROBERTFISHER, JOSEPHKOCALAR, ERTURKSHOEMAKER, DAVIDDU BOIS, RYANGARDNER, BRYSONFINO, JORGEMINCEY, TYLERSANDER, BRIANVORA, SAKET",57217920,road monitoring,system and method of vehicle sensor management,"A method for vehicle sensor management including: acquiring sensor measurements at a sensor module; transmitting the sensor measurements from the sensor module; processing the sensor measurements; and transmitting the processed sensor measurements to a client associated with the vehicle, wherein the processed sensor measurements are rendered by the client on the user device.","We claim:1. A method of operating a vehicle sensor management system, the system including: an imaging system configured to removably mount to a vehicle exterior, a processing system configured to removably connect to a data bus of the vehicle, and a client configured to run on a user device, the method comprising: acquiring a raw video stream at the imaging system; processing the raw video stream into a user stream and an analysis stream at the imaging system; transmitting the analysis stream and the user stream from the imaging system to the processing system; transmitting the user stream from the processing system to the client; determining an ambient environment parameter for the vehicle, based on the analysis stream, at the processing system; generating a notification based on the ambient environment parameter at the processing system; transmitting the notification from the processing system to the client; generating a composite video stream by overlaying graphics associated with the notification over the user stream; and displaying the composite video stream at the client.2. The method of claim 1, further comprising: powering the imaging system using power from a first secondary battery, wherein the imaging system comprises the first secondary battery; powering the processing system with power from the vehicle; and powering the user device with power from a second secondary battery, wherein the user device comprises the second secondary battery.3. The method of claim 1, wherein the analysis stream, user stream, and notification are transmitted between the imaging system, processing system, and client through a high bandwidth wireless communication network created by the processing system.4. The method of claim 3, further comprising transmitting control instructions between the client, processing system, and the imaging system using a low bandwidth wireless communication protocol.5. The method of claim 1, further comprising: operating the imaging system in a low-power mode; generating, at the processing system, a streaming control signal in response to occurrence of a streaming event; transmitting the streaming control signal from the processing system to the imaging system; operating the imaging system in a high-power mode in response to receipt of the streaming control signal, prior to acquiring the raw video stream; generating, at the processing system, a termination control signal in response to occurrence a termination event; transmitting the termination control signal from the processing system to the imaging system; and operating the imaging system in the low-power mode in response to receipt of the termination control signal.6. The method of claim 5, wherein the initialization event comprises wireless connection of the user device to the processing system, wherein the termination event comprises receiving data indicative of vehicle parking gear engagement at the processing system from the data bus.7. The method of claim 1, wherein identifying the ambient environment parameter comprises identifying an object from the analysis stream, wherein the notification is determined based on a position of the object within a video frame.8. The method of claim 1, wherein processing the raw video stream further comprises generating a cropped video stream, the cropped video stream comprising a retained section of each video frame of the video stream, the retained section defined by a set of cropping dimensions and a first set of orientation pixel coordinates, wherein the user stream comprises the cropped video stream.9. The method of claim 8, further comprising: receiving a user input at an input region defined by the client, the user input indicative of moving a camera field of view; generating new cropping instructions based on the user input at the client, the new cropping instructions comprising a second set of orientation pixel coordinates different from the first set of orientation pixel coordinates; and transmitting the new cropping instructions to the imaging system.10. The method of claim 9, further comprising, at the client: generating and displaying a second user stream based on the cropped video stream and the analysis stream until occurrence of a transition event; and in response to occurrence of the transition event, displaying a second cropped video stream, generated by the imaging system and received from the processing system.11. The method of claim 1, further comprising: determining a software update at a remote server system; transmitting a data packet, based on the software update, to the client from the remote server system; in response to client connection with the processing system, transmitting the data packet to the processing system; and updating the processing system based on the data packet.12. The method of claim 11, wherein the data packet comprises the software update.13. The method of claim 1, wherein the notification is generated from a first subset of video frames of the analysis stream, wherein the graphics are composited with video frames of the user stream generated from a second subset of video frames of the analysis stream, the second subset of video frames different from the first subset of video frames.14. A vehicular guidance method using a guidance system including: an imaging system configured to removably mount to a vehicle exterior, the method comprising: at the imaging system, concurrently recording a first and second raw video stream; at the imaging system, processing the first raw video stream into a user stream; transmitting the user stream to a client running on a user device; determining an ambient environment parameter based on the first and second raw video streams; generating a notification based on the ambient environment parameter; transmitting the notification to the client; at the client, generating a composite video stream by overlaying graphics associated with the notification over the user stream; and presenting the composite video stream with the client on a display region of the user device.15. The method of claim 14, wherein the notification is generated based on a first subset of video frames of the first raw video stream, wherein the graphics are composited with video frames of the user stream generated based on a second subset of video frames of the first raw video stream, the second subset of video frames different from the first subset of video frames.16. The method of claim 14, wherein the user stream is transmitted over a high-bandwidth wireless communication network, wherein the client and imaging system are connected to the high-bandwidth wireless communication network.17. The method of claim 14, further comprising processing the first and second raw video streams into a first and second analysis stream, respectively; and transmitting the first and second analysis stream to a processing system, wherein the processing system determines the ambient environment parameter based on the first and second raw video streams, generates the notification, and transmits the notification to the client.18. The method of claim 17, wherein the processing system is housed in a separate housing from the imaging system and user device, the processing system configured to removably mount to a vehicle interior.19. The method of claim 14, further comprising: transmitting a first analysis stream, generated from the first raw video stream, to the user device; at the user device: generating a second composite video stream by aligning the user stream with the first analysis stream; receiving a user input at an input region on the user device, the input region overlaying the display region, the user input comprising an input direction; in response to receipt of the user input, determining an adjusted user video stream from the composite stream, the adjusted user video stream comprising a section of the second composite stream, shifted relative to the user video stream, along a direction opposing the input direction; in response to receipt of the user input, sending user stream instructions, determined based on the user input, to the imaging system; receiving and storing the user stream instructions at the imaging system; concurrently recording a third and fourth raw video stream at the imaging system, the third and fourth raw video stream recorded after first and second raw video stream recordation; processing the third raw video stream into a second user stream according to the user stream instructions at the imaging system; and transmitting the second user stream to the client, wherein the client displays the second user stream at the display region.20. The method of claim 14, further comprising: recording imaging system operation parameters at the imaging system; transmitting imaging system operation parameters to the client; and transmitting the imaging system operation parameters to a remote server system from the client.",US2016325680_A1.txt,"B60R1/00,H04L29/08,H04N5/232,H04N5/247,H04N5/262,H04N5/265,H04N5/77,H04N7/18","{'electric communication technique', 'vehicles in general'}","['vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'transmission of digital information, e.g. telegraphic communication ({coding or ciphering apparatus for cryptographic or other purposes involving the need for secrecy g09c;} arrangements common to telegraphic and telephonic communication h04m)', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television']","system and method of vehicle sensor management A method for vehicle sensor management including: acquiring sensor measurements at a sensor module; transmitting the sensor measurements from the sensor module; processing the sensor measurements; and transmitting the processed sensor measurements to a client associated with the vehicle, wherein the processed sensor measurements are rendered by the client on the user device.",electric communication technique vehicles in general
US2002008637_A1,2001-05-31,2002-01-24,1999-09-15,LEMELSON; JEROME H.PEDERSEN; ROBERT D.PEDERSEN; STEVEN R.,"LEMELSON, JEROME H.PEDERSEN, ROBERT D.PEDERSEN, STEVEN R.",23570633,road monitoring,intelligent traffic control and warning system and method,A system and method for controlling traffic and traffic lights and selectively distributing warning messages to motorists includes a controller to determine appropriate action based on traffic congestion parameters. Fuzzy logic is used to determine optimum traffic light phase split based on the traffic information from the traffic information units. Global Positioning System technology is used by the system and method in order to track moving vehicles and signs and be able to communicate with them.</PTEXT>,"1. A method of using at least one central controller, at least one intelligent traffic light controller and at least one other intelligent controller for controlling traffic and traffic lights and selectively distributing warning messages to motorists comprising the acts of: (a) obtaining traffic information from various traffic information units, (b) transmitting the traffic information to at least one central controller, (c) using the central controller to determine traffic congestion parameters and warning information, (d) further using the derived congestion and warning information as input variables to a fuzzy logic controller to derive traffic light phase split control signals, (e) transmitting traffic light phase split control information to one or more intelligent traffic light controllers, (f) setting the traffic light phase splits at at least one traffic light and transmitting a confirmation message back to the central controller, (g) further broadcasting traffic warning information signals from at least one central controller, said traffic warning information signals defining the nature of at least one traffic situation to be avoided, geographic coordinates of the traffic situation and a level of avoidance indication for the identified traffic situations, (h) receiving said broadcast warning information signals at at least one other intelligent traffic controller, (i) determining the geographic coordinates of at least one other receiving intelligent traffic controller, (j) comparing the coordinates of the receiving intelligent traffic controller with the coordinates of the traffic situation to be avoided and computing the distance between that intelligent controller and the situation, (k) using the received level of avoidance indication and the derived distance as fuzzy variable inputs to a second fuzzy logic controller located in the receiving intelligent controller to derive a danger warning message for the traffic situation to be avoided relative to the location of the receiving intelligent controller, and (l) intelligibly indicating the danger warning message to motorists.2. The method of claim 1 wherein at least one of the other intelligent traffic controllers of act (h) is a controller for a fixed location traffic warning sign with known geographic coordinates.3. The method of claim 1 wherein at least one of the other intelligent traffic controllers of act (h) is a controller for a portable traffic warning sign and where the geographic coordinates of that portable sign are determined using GPS satellite location signals.4. The method of claim 1 wherein the fuzzy logic calculation of act (d) is made at a central controller.5. The method of claim 1 wherein the fuzzy logic calculation of act (d) is made at a traffic light intelligent controller.6. The method of claim 1 wherein at least one of the other intelligent traffic controllers of act (h) is located in a motor vehicle, the GPS coordinates of that motor vehicle are calculated in the vehicle, and the fuzzy logic calculation determining the degree of danger is made in the vehicle.7. The method of claim 1 wherein at least one of the other intelligent traffic controllers of act (h) is located at a traffic warning sign and at least one other of those intelligent traffic controllers is located in a motor vehicle.8. The method of claim 1 wherein at least one of the traffic light intelligent controllers includes a television camera used to monitor traffic at an intersection and transmit video information signals to at least one central controller.9. The method of claim 1 wherein the at least one of the traffic situations to be avoided indicated in act (g) is an intersection with unusual traffic light phase splits as calculated using the fuzzy logic calculation of act (d).10. A method of using an intelligent traffic light controller for controlling traffic at an intersection having traffic lights comprising the acts of: (a) obtaining traffic information from various traffic information units, (b) transmitting said traffic information to said intelligent traffic light controller, (c) using said intelligent traffic light controller to determine traffic congestion parameters, (d) further using the derived congestion information as input variables to a fuzzy logic controller to derive traffic light phase split control signals, (e) setting the traffic light phase splits at one traffic light and transmitting a confirmation message back to the intelligent traffic light controller.11. A method of using at least one central controller and at least one intelligent controller for controlling traffic and traffic lights and selectively distributing warning messages to motorists comprising the acts of: (a) obtaining traffic information from various traffic information units, (b) transmitting the traffic information to at least one central controller, (c) using the central controller to determine congestion parameters and warning information, (d) transmitting the congestion parameters and the warning information from the at least one central controller to the intelligent controller, and (e) using the intelligent controllers to determine appropriate action based on the congestion parameters and the warning information.12. The method according to claim 11 wherein the traffic information units are traffic lights with intelligent controllers wherein each of the traffic lights with the intelligent controllers further comprises: (a) a computer controller including a processor and memory, (b) a receiver coupled to the computer controller wherein the receiver receives and analyzes communication signals from at least one central controller, and (c) a transmitter coupled to the computer controller wherein the transmitter generates and transmits signals to at least some of the other traffic information units.13. The method according to claim 11 wherein the traffic information units are traffic lights with intelligent warning signs wherein each of the traffic lights with the intelligent warning signs further comprises: (a) a receiver that receives and analyzes communication signals from at least some of the other traffic information units, and (b) a warning sign that displays the warning messages to the motorists.14. The method according to claim 11 wherein the traffic information units are intelligent roadside warning signs wherein each of the intelligent roadside warning signs further comprises: (a) a receiver that receives and analyzes communication signals from at least some of the other traffic information units, and (b) a warning sign that displays the warning messages to the motorists.15. The method according to claim 11 wherein the traffic information units are traffic lights with cameras wherein the traffic lights with cameras further comprises: (a) a camera that monitors an intersection or road, (b) a receiver that receives and analyzes communication signals from at least some of the other traffic information units, and (c) a transmitter that at least receives signals from the camera and generates and transmits signals to at least some of the other traffic information units.16. The method according to claim 11 wherein the traffic information units are roadside traffic and weather sensors wherein each of the roadside traffic and weather sensors further comprises a transmitter that generates and transmits signals to the at least one central controller.17. The method according to claim 11 wherein the traffic information units are vehicle warning units wherein each of the vehicle warning units further comprises: (a) a receiver that receives and analyzes communication signals from at least one central controller, (b) a satellite receiver that receives and analyzes communications signals from a satellite positioning system and determines current geographic location of each of the warning units, (c) a transmitter that generates and transmits data to at least one central controller, and (d) an alarm indicator that indicates a relevant traffic situation or emergency.18. The method according to claim 17 wherein the vehicle warning units are a plurality of vehicle warning units wherein each of the vehicle warning units further comprises: (a) a receiving circuit to receive data from the at least one central controller, (b) a transmitter to transmit data to the at least one central controller, and (c) a global positioning system receiver to determine exact location of each of the vehicle warning units.19. The method according to claim 17 wherein each of the vehicle warning units further comprises a communication system that communicates with a fuzzy logic processor which determines and calculates if received warning messages are relevant to the each of the vehicle warning units and communicates vehicle warnings based on the received warning messages and the current geographic location of each of the vehicle warning units.20. The method according to claim 19 wherein the fuzzy processor uses results of the fuzzy logic calculations at the central controller for determining traffic light phase splits and further uses the traffic light phase splits as input variables into the calculation of the vehicle warnings thereby creating a series of dependent fuzzy logic calculations.21. The method according to claim 17 further comprises the acts of: (a) providing the vehicle warning units with audio and speech recognition capabilities, (b) determining if recognized audio or speech is indicative of an emergency or dangerous situation, and (c) transmitting warning messages to the central controller when the audio or speech indicative of an emergency or dangerous situation are detected.22. The method according to claim 17 wherein the intelligent central controller comprises a plurality of central controllers and wherein each of the vehicle warning units is capable of determining from which one of the plurality of central controllers it is to receive data transmission based on the current geographic location of the each of the vehicle warning units.23. The method according to claim 11 wherein at least one controller is one of a plurality of central controllers, wherein each of the central controllers further comprises: (a) a database computer having a database storage unit, (b) a processor and memory configured to monitor existing traffic conditions and emergency situations and distribute warning messages, (c) a receiver that receives and analyzes communication signals from the traffic information units, and (d) a transmitter that generates and transmits signals to the traffic information units.24. The method according to claim 11 further comprises the acts of: (a) providing a plurality of traffic light controllers, and (b) configuring the traffic light controllers to receive data from the central controller, to transmit data to the central controller, to transmit data from at least some of the traffic information units, and to receive data from the at least some of the traffic information units.25. The method according to claim 24 wherein the act of providing a plurality of traffic light controllers further comprises the acts of providing traffic light controllers with fuzzy logic processors wherein the fuzzy logic processors calculate correct traffic light phase split and determine if received warning messages are relevant to each of the traffic information units.26. The method according to claim 11 further comprising the acts of: (a) providing a plurality of traffic sensors, and (b) configuring the traffic sensors to transmit information to at least one central controller.27. The method according to claim 11 further comprising the act of providing a plurality of roadside warning signs wherein each of the roadside warning signs includes a receiving circuit to receive data from the at least one central controller and at least some of the traffic information units and also includes global positioning system receivers to determine exact locations of the roadside warning signs.28. The method according to claim 27 wherein the act of providing a plurality of roadside warning signs further comprises the act of providing a plurality of fixed-location roadside warning signs.29. The method according to claim 27 wherein the act of providing a plurality of roadside warning signs further comprises the act of providing a plurality of movable roadside warning signs.30. The method according to claim 29 wherein the act of providing a plurality of movable warning signs further comprises the act of providing each of the movable roadside warning signs with a fuzzy logic processor that determines if received warning messages are relevant to the each of the movable roadside warning signs and calculates warnings based on location of the each of movable roadside warning signs, the received warning messages, and abnormal phase splits of traffic lights.31. The method according to claim 30 further comprises the acts of having the fuzzy processors of the road-side warning signs use results of the fuzzy logic calculation for determining traffic light phase splits and having the fuzzy processors further use the traffic light phase splits as input variables into calculation of warning messages thereby creating a series of dependent fuzzy logic calculations.32. The method according to claim 11 further comprising the act of providing global positioning system location receivers and processors for the traffic information units located in emergency vehicles wherein the receivers and the processors precisely locate the emergency vehicles and report location, movement and destination to the at least one central controller for use in generating traffic management controls.33. The method according to claim 11 further comprising the act of using fuzzy logic to determine optimum traffic light phase split based on the traffic information from the traffic information units.34. The method according to claim 33 wherein the act of using fuzzy logic further comprises the act of determining the optimum traffic light phase split at each of the intelligent controllers.35. The method according to claim 33 wherein the act of using fuzzy logic further comprises the act of determining the optimum traffic phase split at the at least one central controller.36. The method according to claim 11 further comprising the act of using fuzzy logic controllers to execute fuzzy logic inference rules from a fuzzy rule base in determining the congestion parameters and the warning information and the appropriate action.37. The method according to claim 36 further comprising the acts of: (a) defining input variables and output variables as members of fuzzy sets having degrees of membership determined by membership functions, (b) using the fuzzy rule base to define a fuzzy inference system wherein the fuzzy rule base is based on expert knowledge for system control based on observed values of control variables, (c) using the input variables to define the membership functions used by the fuzzy rule base, (d) using a reasoning mechanism to execute the fuzzy rule base and the fuzzy inference system, and (e) using the membership functions to convert the input variables to output variables that define the control variables.38. The method according to claim 37 wherein one of the membership functions is a fuzzy membership for traffic flow.39. The method according to claim 38 wherein one of the fuzzy sets for the fuzzy membership is a low traffic flow.40. The method according to claim 38 wherein one of the fuzzy sets for the fuzzy membership is a medium traffic flow.41. The method according to claim 38 wherein one of the fuzzy sets for the fuzzy membership is a high traffic flow.42. The method according to claim 37 wherein one of the membership functions is a fuzzy membership for a traffic light phase split.43. The method according to claim 42 wherein one of the fuzzy sets for the fuzzy membership is a short traffic light phase split.44. The method according to claim 42 wherein one of the fuzzy sets for the fuzzy membership is a normal traffic light phase split.45. The method according to claim 42 wherein one of the fuzzy sets for the fuzzy membership is a long traffic light phase split.46. The method according to claim 37 wherein one of the membership functions is a fuzzy membership for traffic flow and wherein one of the membership functions is a fuzzy membership for a traffic light phase split and further comprising the act of using the fuzzy rule base to determine the traffic light phase splits based on the traffic flow from various directions of an intersection and on outside factors at the intersection.47. The method according to claim 46 further comprising the acts of: (a) communicating fuzzy logic calculations to the at least one central controller controlling the intersection, (b) implementing the respective traffic light phase split for the intersection, (c) detecting abnormal traffic light phase split for the intersection, and (d) transmitting warning signals to the respective traffic information units if an abnormal traffic light phase split is detected.48. The method according to claim 47 wherein the act of transmitting warning signals further comprises the acts of: (a) comparing geographic locations of the traffic information units that are in vehicles to geographic locations of intersections, (b) generating warning signals in the vehicles in proximity of the intersection.49. The method according to claim 11 the act of using the intelligent controllers further comprises the act of operating at least one of the intelligent controllers for controlling an intersection.50. The method according to claim 49 wherein the operating act further comprises the acts of: (a) sensing and updating data from traffic sensors at the intersection, (b) sensing and updating data from auxiliary sources, (c) selecting a fuzzy logic rule set, (d) using the at least one central controller to derive a correct traffic light phase split based on the fuzzy logic rule set selected, (e) generating and displaying respective warning messages at the intersection, (f) transmitting appropriate traffic light control and warning information to the at least one central controller, and (g) updating data at the at least one central controller.51. The method according to claim 50 wherein the determining act further comprises the acts of: (a) entering a time delay and repeating the method acts if the operation is to continue, and (b) terminating the operation if the operation is to not continue.52. The method according to claim 37 wherein one of the input variables is a level of avoidance variable.53. The method according to claim 37 wherein one of the input variables is a length of warning radius variable.54. The method according to claim 37 wherein one of the input variables is a distance to dangerous situation variable.55. The method according to claim 37 wherein one of the output variables is an output danger index.56. The method according to claim 37 wherein one of the output variables is a radius of concern parameter.57. The method according to claim 11 wherein the act of using the intelligent controllers further comprises the act of using the intelligent controllers to determine appropriate traffic control action.58. The method according to claim 11 wherein the act of using the intelligent controllers further comprises the act of using the intelligent controllers to determine appropriate traffic information distribution.59. The method according to claim 58 wherein the traffic information is traffic warning messages.60. The method according to claim 11 further comprising the act of integrating traffic light control operations and traffic information distribution operations in determining the appropriate action.61. The method according to claim 11 further comprising the act of using fuzzy logic to derive the warning information based on avoidance level of dangerous situation and distance to dangerous situation and detection of abnormal phase splits of traffic lights.62. The method according to claim 61 further comprising the act of using communication systems located in vehicles that communicate with fuzzy logic controllers which make fuzzy logic calculations for the vehicles based on the avoidance level of the dangerous situation and global positioning system coordinates of the dangerous situation received in the message from the respective at least one central controller and global positioning system coordinates of the vehicles derived by local global positioning system receivers and location processors in the vehicles.63. The method according to claim 61 further comprising the acts of: (a) locating at least one warning sign at a fixed location of known global positioning system coordinates, (b) determining the warning information to be displayed using fuzzy logic at the at least one central controller, and (c) transmitting the warning information to the at least one warning sign at the fixed location.64. The method according to claim 61 further comprising the acts of: (a) providing at least one portable warning sign having a global positioning system receiver and processor to determine the global positioning system coordinates of the at least one portable warning sign and further having a control processor that uses fuzzy logic, (b) using the control processor to determine global positioning system coordinates of the at least one portable warning sign, and (c) receiving a danger avoidance level of a dangerous situation to compute an appropriate warning message to be displayed on the at least one portable warning sign depending on a distance of the at least one portable warning sign to the dangerous situation.65. A system for controlling traffic and traffic lights and selectively distributing warning messages to motorists comprising: (a) various traffic information units with intelligent controllers that obtain traffic information, (b) at least one central controller that receives the traffic information transmitted from the various traffic information units and that also determines congestion parameters and warning information, (c) wherein the congestion parameters and the warning information are transmitted from the at least one central controller to the intelligent controllers, and (e) wherein the intelligent controllers determine appropriate action based on the congestion parameters and the warming information.66. The system according to claim 65 wherein the traffic information units are traffic lights with intelligent controllers wherein each of the traffic lights with the intelligent controllers further comprises: (a) a computer controller including a processor and memory, (b) a receiver coupled to the computer controller wherein the receiver receives and analyzes communication signals from the at least one central controller, and (c) a transmitter coupled to the computer controller wherein the transmitter generates and transmits signals to at least some of the other traffic information units.67. The system according to claim 65 wherein the traffic information units are traffic lights with intelligent warning signs wherein each of the traffic lights with the intelligent warning signs further comprises: (a) a receiver that receives and analyzes communication signals from at least some of the other traffic information units, and (b) a warning sign that displays the warning messages to the motorists.68. The system according to claim 65 wherein the traffic information units are intelligent roadside warning signs wherein each of the intelligent roadside warning signs further comprises: (a) a receiver that receives and analyzes communication signals from at least some of the other traffic information units, and (b) a warning sign that displays the warning messages to the motorists.69. The system according to claim 65 wherein the traffic information units are traffic lights with cameras wherein each of the traffic lights with cameras further comprises: (a) a camera that monitors an intersection or road, (b) a receiver that receives and analyzes communication signals from at least some of the other traffic information units, and (c) a transmitter that at least receives signals from the camera and generates and transmits signals to at least some of the other traffic information units.70. The system according to claim 65 wherein the traffic information units are roadside traffic and weather sensors wherein each of the roadside traffic and weather sensors further comprises a transmitter that generates and transmits signals to the at least one central controller.71. The system according to claim 65 wherein the traffic information units are vehicle warning units wherein each of the vehicle warning units further comprises: (a) a receiver that receives and analyzes communication signals from the at least one central controller, (b) a satellite receiver that receives and analyzes communications signals from a satellite positioning system and determines current geographic location of each of the warning units, (c) a transmitter that generates and transmits data to the at least one central controller, and (d) an alarm indicator that indicates a relevant traffic situation or emergency.72. The system according to claim 71 wherein the vehicle warning units are a plurality of vehicle warning units wherein each of the vehicle warning units further comprises: (a) a receiving circuit to receive data from the at least one central controller, (b) a transmitter to transmit data to the at least one central controller, and (c) a global positioning system receiver to determine exact location of each of the vehicle warning units.73. The system according to claim 71 wherein each of the vehicle warning units further comprises a communication system that communicates with a fuzzy logic processor that determines and calculates if received warning messages are relevant to the each of the vehicle warning units and communicates vehicle warnings based on the received warning messages and the current geographic location of the each of the vehicle warning units.74. The system according to claim 73 wherein the fuzzy processor uses results of the fuzzy logic calculations at the central controller for determining traffic light phase splits and further uses the traffic light phase splits as input variables into the calculation of the vehicle warnings thereby creating a series of dependent fuzzy logic calculations.75. The system according to claim 71 wherein the vehicle warning units further comprises an audio and speech recognition system determining if recognized audio or speech signals are indicative of an emergency or dangerous situation and for transmitting warning messages to the at least one central controller when the audio or speech signals indicative of an emergency or dangerous situation are detected.76. The system according to claim 71 wherein the at least one central controller is a plurality of central controllers and wherein each of the vehicle warning units is capable of determining from which one of the plurality of central controllers it is to receive data transmission based on the current geographic location of the each of the vehicle warning units.77. The system according to claim 65 wherein the at least one controller is one of a plurality of central controllers, wherein each of the central controllers further comprises: (a) a database computer having a database storage unit, (b) a processor and memory configured to monitor existing traffic conditions and emergency situations and distribute warning messages, (c) a receiver that receives and analyzes communication signals from the traffic information units, and (d) a transmitter that generates and transmits signals to the traffic information units.78. The system according to claim 65 further comprises a plurality of traffic light controllers that are configured to receive data from the at least one central controller, to transmit data to the at least one central controller, to transmit data from at least some of the traffic information units, and to receive data from the at least some of the traffic information units.79. The system according to claim 78 wherein the plurality of traffic light controllers further comprises traffic light controllers with fuzzy logic processors wherein the fuzzy logic processors calculate correct traffic light phase split and determine if received warning messages are relevant to each of the traffic information units.80. The system according to claim 65 further comprises a plurality of traffic sensors configured to transmit information to the at least one central controller.81. The system according to claim 65 further comprises a plurality of roadside warning signs wherein each of the roadside warning signs includes a receiving circuit to receive data from the at least one central controller and at least some of the traffic information units and also includes global positioning system receivers to determine exact locations of the roadside warning signs.82. The system according to claim 81 wherein the plurality of roadside warning signs further comprises a plurality of fixed-location roadside warning signs.83. The system according to claim 81 wherein the plurality of roadside warning signs further comprises a plurality of movable roadside warning signs.84. The system according to claim 83 wherein each of the plurality of movable warning signs further comprises a fuzzy logic processor that determines if received warning messages are relevant to the each of the movable roadside warning signs and calculates warnings based on location of the each of movable roadside warning signs, the received warning messages, and abnormal phase splits of traffic lights.85. The system according to claim 84 wherein the fuzzy processors of the roadside warning signs use results of the fuzzy logic calculation for determining traffic light phase splits and having the fuzzy processors further use the traffic light phase splits as input variables into calculation of warning messages thereby creating a series of dependent fuzzy logic calculations.86. The system according to claim 65 further comprises global positioning system location receivers and processors for the traffic information units located in emergency vehicles wherein the receivers and the processors precisely locate the emergency vehicles and report location, movement and destination to the at least one central controller for use in generating traffic management controls.87. The system according to claim 65 further comprises fuzzy logic controllers that communicate with the at least one central controller and the intelligent controllers to execute fuzzy logic inference rules from a fuzzy rule base in determining the congestion parameters and the warning information and the appropriate action.88. The system according to claim 65 wherein the intelligent controllers determine appropriate traffic control action.89. The system according to claim 65 wherein the intelligent controllers determine appropriate traffic information distribution.90. The system according to claim 89 wherein the traffic information is traffic warning messages.91. The system according to claim 65 further comprises fuzzy logic controllers that use fuzzy logic to derive the warning information based on avoidance level of dangerous situation and distance to dangerous situation and detection of abnormal phase splits of traffic lights.92. The system according to claim 91 further comprises communication systems located in vehicles wherein the communication systems communicate with the fuzzy logic controllers that make fuzzy logic calculations for the vehicles based on the avoidance level of the dangerous situation and global positioning system coordinates of the dangerous situation received in the message from the respective at least one central controller and global positioning system coordinates of the vehicles derived by local global positioning system receivers and location processors in the vehicles.93. The system according to claim 91 further comprises at least one warning sign located at a fixed location of known global positioning system coordinates wherein the at least one central controller uses fuzzy logic to determine the warning information to be displayed and transmits the warning information to the at least one warning sign at the fixed location.94. The system according to claim 91 further comprises at least one portable warning sign having a global positioning system receiver and processor to determine the global positioning system coordinates of the at least one portable warning sign and further having a control processor that uses fuzzy logic",US2002008637_A1.txt,G08G1/07,{'signalling'},"['traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})']",intelligent traffic control and warning system and method A system and method for controlling traffic and traffic lights and selectively distributing warning messages to motorists includes a controller to determine appropriate action based on traffic congestion parameters. Fuzzy logic is used to determine optimum traffic light phase split based on the traffic information from the traffic information units. Global Positioning System technology is used by the system and method in order to track moving vehicles and signs and be able to communicate with them.</PTEXT>,signalling
US10852725_B2,2017-06-29,2020-12-01,2016-06-29,FARADAY & FUTURE,"SONG, XIUFENGNIKULIN, SERGESAMPER, JUAN PABLO",62147560,road monitoring,activate/deactivate functionality in response to environmental conditions,"Methods, systems, computer-readable media, and apparatuses for executing an event pertaining to a vehicle and a user of the vehicle are presented. In some embodiments, a includes (a) detecting that a motion state of the vehicle is in a first state, (b) detecting a condition that warrants a change in the motion state of the vehicle to a second state, and (c) detecting the user of the vehicle's engagement in an activity that potentially impairs the user from performing an action to change the motion state of the vehicle to the second state. The method also includes, detecting the condition, and (c) detecting the user of the vehicle's engagement in the activity, executing an event to facilitate the change in the motion state of the vehicle to the second state.","1. A method for executing an event pertaining to a vehicle and a user of the vehicle, comprising: the vehicle (a) detecting that a motion state of the vehicle is in a first state; (b) detecting a condition that warrants a change in the motion state of the vehicle to a second state, the condition comprising an expected change in a status of a traffic light; (c) detecting an engagement of the user of the vehicle in an activity that impairs the user from performing an action to change the motion state of the vehicle to the second state; in response to (a) detecting that the motion state of the vehicle is in the first state, (b) detecting the condition, and (c) detecting the user of the vehicle's engagement in the activity, executing an event to facilitate the change in the motion state of the vehicle to the second state; (d) detecting an obstacle in front of the vehicle; and in response to (d), preventing an accelerator input from the user and communicating by having taillights displaying a pattern to another vehicle behind the vehicle, the pattern indicating the detection of the obstacle.2. The method of claim 1, wherein the event comprises at least one of: presenting a notification to the user of the vehicle; or facilitating entry of the vehicle into an autonomous driving mode.3. The method of claim 2, wherein the notification comprises at least one of an auditory notification, visual notification, or haptic notification.4. The method of claim 1, wherein detecting the condition that warrants the change in the motion state of the vehicle to a second state further comprises at least one of: detecting a change in a status of a traffic light; or detecting a change in a traffic condition.5. The method of claim 1, wherein the first state comprises a stop state and the second state comprises a moving state.6. The method of claim 1, further comprising, in response to (a) detecting that the motion state of the vehicle is in the first state, (b) detecting the condition, and (c) detecting the user of the vehicle's engagement in the activity, preventing interaction with a device within the vehicle.7. The method of claim 6, wherein the device comprises at least one of a mobile device or infotainment system.8. The method of claim 1, wherein (b) detecting the condition is based at least in part on at least one of: one or more images obtained from a camera; or one more signals obtained by a radar or Light Detection And Ranging (LIDAR) apparatus.9. The method of claim 1, wherein (b) detecting the condition is based at least in part on at least one of: a received wireless communication; or a recorded audio signal.10. An apparatus for executing an event pertaining to a vehicle and a user of the vehicle, comprising: (a) means for detecting that a motion state of the vehicle is in a first state; (b) means for detecting a condition that warrants a change in the motion state of the vehicle to a second state, the condition comprising an expected change in a status of a traffic light; (c) means for detecting an engagement of the user of the vehicle in an activity that impairs the user from performing an action to change the motion state of the vehicle to the second state; means, in response to (a) detecting that the motion state of the vehicle is in the first state, (b) detecting the condition, and (c) detecting the user of the vehicle's engagement in the activity, for executing an event to facilitate the change in the motion state of the vehicle to the second state; (d) means for detecting an obstacle in front of the vehicle; and means, in response to (d), for preventing an accelerator input from the user and communicating by having taillights displaying a pattern to another vehicle behind the vehicle, the pattern indicating the detection of the obstacle.11. The apparatus of claim 10, further comprising means for in response to (a) detecting that the motion state of the vehicle is in the first state, (b) detecting the condition, and (c) detecting the user of the vehicle's engagement in the activity, preventing interaction with a device within the vehicle.12. A system for executing an event pertaining to a vehicle and a user of the vehicle, comprising: a processor; and a non-transitory computer readable medium coupled to the processor, the computer readable medium comprising code, executable by the processor, for implementing a method comprising: (a) detecting that a motion state of the vehicle is in a first state; (b) detecting a condition that warrants a change in the motion state of the vehicle to a second state, the condition comprising an expected change in a status of a traffic light; (c) detecting an engagement of the user of the vehicle in an activity that impairs the user from performing an action to change the motion state of the vehicle to the second state; in response to (a) detecting that the motion state of the vehicle is in the first state, (b) detecting the condition, and (c) detecting the user of the vehicle's engagement in the activity, executing an event to facilitate the change in the motion state of the vehicle to the second state; (d) detecting an obstacle in front of the vehicle; and in response to (d), preventing an accelerator input from the user and communicating by having taillights displaying a pattern to another vehicle behind the vehicle, the pattern indicating the detection of the obstacle.13. The system of claim 12, wherein the event comprises at least one of: presenting a notification to the user of the vehicle; or facilitating entry of the vehicle into an autonomous driving mode.14. The system of claim 13, wherein the notification comprises at least one of an auditory notification, visual notification, or haptic notification.15. The system of claim 12, wherein detecting the condition that warrants the change in the motion state of the vehicle to a second state further comprises at least one of: detecting a change in a status of a traffic light; or detecting a change in a traffic condition.16. The system of claim 12, wherein the first state comprises a stop state and the second state comprises a moving state.17. The system of claim 12, wherein the method further comprises, in response to (a) detecting that the motion state of the vehicle is in the first state, (b) detecting the condition, and (c) detecting the user of the vehicle's engagement in the activity, preventing interaction with a device within the vehicle.18. The system of claim 17, wherein the device comprises at least one of a mobile device or infotainment system.19. The system of claim 12, wherein (b) detecting the condition is based at least in part on at least one of: one or more images obtained from a camera; or one more signals obtained by a radar or Light Detection And Ranging (LIDAR) apparatus.20. The system of claim 12, wherein (b) detecting the condition is based at least in part on at least one of: a received wireless communication; or a recorded audio signal.",US10852725_B2.txt,"B60W30/18,B60W40/08,B60W50/12,B60W50/14,G05D1/00,G08B7/06","{'controlling; regulating', 'signalling', 'vehicles in general'}","['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'systems for controlling or regulating non-electric variables (for continuous casting of metals b22d11/16; valves per se f16k; sensing non-electric variables, see the relevant subclasses of g01; for regulating electric or magnetic variables g05f)', 'signalling or calling systems; order telegraphs; alarm systems']","activate/deactivate functionality in response to environmental conditions Methods, systems, computer-readable media, and apparatuses for executing an event pertaining to a vehicle and a user of the vehicle are presented. In some embodiments, a includes (a) detecting that a motion state of the vehicle is in a first state, (b) detecting a condition that warrants a change in the motion state of the vehicle to a second state, and (c) detecting the user of the vehicle's engagement in an activity that potentially impairs the user from performing an action to change the motion state of the vehicle to the second state. The method also includes, detecting the condition, and (c) detecting the user of the vehicle's engagement in the activity, executing an event to facilitate the change in the motion state of the vehicle to the second state.",controlling; regulating signalling vehicles in general
US10319007_B1,2018-03-08,2019-06-11,2018-03-08,CAPITAL ONE SERVICES,"TANG, QIAOCHUDAGLEY, GEOFFREYVASISHT, SUNIL SUBRAHMANYAMPRICE, MICAHWYLIE, STEPHEN MICHAELHOOVER, JASON RICHARD",66767393,gps,database image matching using machine learning with output estimation,"A method for processing an image including a vehicle using machine learning can include determining a location of the client device. The method can further include receiving a first image of a vehicle from an image sensor of the client device and matching, using machine learning, the first image to one or more images of vehicles in a vehicle database to identify the vehicle. The vehicle database can list vehicles located at the determined location of the client device and images of the vehicles. The method can include retrieving vehicle information from the vehicle database, based on the identified vehicle, and obtaining comparison information based at least in part on the vehicle information. The method can include estimating a quote for the vehicle based on the comparison information and transmitting the estimated quote for display on the client device.","1. A system for processing an image including a vehicle using machine learning, comprising: a processor in communication with a client device; and a storage medium storing instructions that, when executed, cause the processor to perform operations comprising: determining a location of the client device; mapping the location of the client device to a vehicle dealership; receiving a first image of a vehicle from an image sensor of the client device; matching, using machine learning, the first image to one or more images of vehicles in a vehicle database of the vehicle dealership to identify the vehicle, the vehicle database listing vehicles located at the vehicle dealership, the vehicle database including images of the listed vehicles, wherein matching includes: inputting the image to an input layer of a first convolutional neural network; extracting features indicating characteristics of the vehicle from an output layer of the first convolutional neural network; inputting the extracted features to an input layer of a second convolutional neural network; determining attributes of the vehicle from an output layer of the second convolutional neural network; and determining the identified vehicle as a vehicle in the matched one or more images in the database having associated attributes matching the determined attributes; retrieving vehicle information from the vehicle database, based on the identified vehicle; obtaining comparison information from a comparison resource of a service provider based at least in part on the vehicle information; estimating a quote for the identified vehicle based on the comparison information; and transmitting the estimated quote for display on the client device.2. The system of claim 1, wherein the operations further comprise: determining that the first image is below an acceptability threshold; and transmitting information for display on the client device, the information including an error message or a guiding line for obtaining a second image of the vehicle.3. The system of claim 1, wherein obtaining the comparison information further comprises retrieving pricing information based at least in part on the vehicle information.4. The system of claim 3, wherein the pricing information is retrieved based at least in part on personal information of a user of the client device.5. The system of claim 3, wherein estimating the quote for the vehicle comprises calculating statistics of the pricing information.6. The system of claim 5, wherein estimating the quote for the vehicle comprises receiving selections or modifications of financing terms and recalculating statistics of the pricing information based on the received selections or modifications of the financing terms.7. The system of claim 1, wherein the processor and the comparison resource are associated with distinct service providers.8. A computer-implemented method for processing an image including a vehicle using machine learning, the method comprising: determining a location of a client device; mapping the location of the client device to a vehicle dealership; receiving a first image of a vehicle from an image sensor of the client device; matching, by a system using machine learning, the first image to one or more images of vehicles in a vehicle database of the vehicle dealership to identify the vehicle, the vehicle database listing vehicles located at the vehicle dealership, the vehicle database including images of the listed vehicles, wherein matching includes: inputting the image to an input layer of a first convolutional neural network; extracting features indicating characteristics of the vehicle from an output layer of the first convolutional neural network; inputting the extracted features to an input layer of a second convolutional neural network; determining attributes of the vehicle from an output layer of the second convolutional neural network; and determining the identified vehicle as a vehicle in the matched one or more images in the database having associated attributes matching the determined attributes; retrieving vehicle information from the vehicle database, based on the identified vehicle; obtaining comparison information from a comparison resource of a service provider based at least in part on the vehicle information; estimating a quote for the identified vehicle based on the comparison information; and transmitting the estimated quote for display on the client device.9. The method of claim 8, further comprising: determining that the first image is below an acceptability threshold; and transmitting information for display on the client device, the information including an error message or a guiding line for obtaining a second image of the vehicle.10. The method of claim 8, wherein obtaining the comparison information further comprises retrieving pricing information.11. The method of claim 10, wherein the pricing information is retrieved based at least in part on personal information of a user of the client device.12. The method of claim 10, wherein estimating the quote for the vehicle comprises calculating statistics of the pricing information.13. The method of claim 12, wherein estimating the quote for the vehicle comprises receiving selections or modifications of financing terms and recalculating statistics of the pricing information based on the received selections or modifications of the financing terms.14. The method of claim 8, wherein the comparison resource stores pricing information, vehicle-related information, and personal information, and wherein the quote for the vehicle is estimated based on pricing information, vehicle-related information, and personal information retrieved from the comparison resource.15. The method of claim 8, wherein the system and the comparison resource are associated with distinct service providers.16. A non-transitory computer-readable medium storing instructions that, when executed by a processor, cause the processor to operate a computer system for processing an image including a vehicle using machine learning, comprising: determining a location of a client device; mapping the location of the client device to a vehicle dealership; receiving a first image of a vehicle from an image sensor of the client device; matching, using machine learning, the first image to one or more images of vehicles in a vehicle database of the vehicle dealership to identify the vehicle, the vehicle database listing vehicles located at the vehicle dealership, the vehicle database including images of the listed vehicles, wherein matching includes: inputting the image to an input layer of a first convolutional neural network; extracting features indicating characteristics of the vehicle from an output layer of the first convolutional neural network; inputting the extracted features to an input layer of a second convolutional neural network; determining attributes of the vehicle from an output layer of the second convolutional neural network; and determining the identified vehicle as a vehicle in the matched one or more images in the database having associated attributes matching the determined attributes retrieving vehicle information from the vehicle database, based on the identified vehicle; obtaining comparison information from a comparison resource of a service provider based at least in part on the vehicle information; estimating a quote for the identified vehicle based on the comparison information; and transmitting the estimated quote for display on the client device.17. The non-transitory computer-readable medium of claim 16, wherein the computer system and the comparison resource are associated with distinct service providers.",US10319007_B1.txt,"G06F16/58,G06N20/00,G06Q30/06",{'computing; calculating; counting'},"['electric digital data processing (computer systems based on specific computational models g06n)', 'computing arrangements based on specific computational models', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for']","database image matching using machine learning with output estimation A method for processing an image including a vehicle using machine learning can include determining a location of the client device. The method can further include receiving a first image of a vehicle from an image sensor of the client device and matching, using machine learning, the first image to one or more images of vehicles in a vehicle database to identify the vehicle. The vehicle database can list vehicles located at the determined location of the client device and images of the vehicles. The method can include retrieving vehicle information from the vehicle database, based on the identified vehicle, and obtaining comparison information based at least in part on the vehicle information. The method can include estimating a quote for the vehicle based on the comparison information and transmitting the estimated quote for display on the client device.",computing; calculating; counting
US6970576_B1,2000-08-23,2005-11-29,1999-08-04,MBDA UK,"TILSLEY, GWILYM J",10858477,survillance,surveillance system with autonomic control,"A surveillance system which allows an operator to determine the type of objects or events which trigger image capture and analysis. Analysis of captured images is provided by access to various type of databases thereby providing for positive identification and monitoring of objects or events, and further provided for the system to issue control commands or prompt operations if required.","1. A surveillance system comprising: (a) a first camera, positioned to have a field of view of a surveillance area, for providing images of said surveillance area, (b) a second camera for providing a higher quality image of at least a portion of the surveillance area, (c) an image processing means for making a qualitative assessment of the images provided by said first camera and for causing, when the image quality is inadequate, operation of said second camera to provide a higher quality image of said portion of the surveillance area requiring further analysis, (d) a data-base comprising at least one data-base source from the group comprising human operators, rule based systems, knowledge based systems, artificial intelligence systems, data-bases and algorithms, and (e) an image analysis means for analyzing the content of said higher quality images from said second camera with reference to said data-base, assessing whether any further analysis of the content of said higher quality images from said second camera is required, and controlling said second camera to provide further image information of any portion of said surveillance area requiring further analysis.2. A surveillance system, as in claim 1, wherein said image analysis means further includes means for: analyzing the content of said images from said first camera with reference to said data-base, assessing whether any further analysis of the content of said images from said first camera is required, and controlling at least one of said cameras to provide further image information of any part of said surveillance area requiring further analysis.3. A surveillance system, as in claim 1, wherein said image processing means is pre-programmed to request further analysis of any image having a feature taken from the group comprising certain pre-determined events, features, sequences of actions and images.4. A surveillance system, as in claim 3, wherein said image processing means is pre-programmed by said data-base.5. A surveillance system, as in claim 3, wherein said image analysis means is pre-programmed to determine a control function response to any image having one of said features.6. A surveillance system, as in claim 1, wherein said image analysis means includes a self-learning means for identifying at least one of a pattern of events and pattern of behavior in a previous sequence of said images that has already been analyzed and regarded as unimportant, and for eliminating said unimportant at least one pattern of events and pattern of behavior from further analysis by said image analysis means.7. A surveillance system, as in claim 1, wherein said image analysis means includes an artificial intelligence means for identifying one of a pattern of events and pattern of behavior in a previous sequence of said images that has already been analyzed and regarded as unimportant, and for eliminating said unimportant at least one pattern of events and pattern of behavior from further analysis by said image analysis means.8. A surveillance network comprising a plurality of surveillance systems, as in claim 1, with each of said first cameras positioned to have a field of view of a portion of a combined surveillance area whereby each of said first cameras provides images of said combined surveillance area, each of said second cameras provides more detailed images of at least a portion of said combined surveillance area thereby enabling an object to be monitored and tracked continuously within said combined surveillance area, and a control means for handing over tracking of said object from one of said cameras covering one portion of said combined surveillance area to another of said cameras covering an adjoining portion of said combined surveillance area thereby ensuring continuity in the surveillance of said object.9. A surveillance system, comprising: (a) a first camera, positioned to have a field of view of a surveillance area, for providing images of said surveillance area, (b) a second camera for providing a higher quality image of at least a portion of the surveillance area, (c) an image processing means for making a qualitative assessment of the images provided by said first camera and for causing, when the image quality is inadequate, operation of said second camera to provide a higher quality image of said portion of the surveillance area requiring further analysis, (d) a data-base comprising at least one data-base source from the group comprising human operators, rule based systems, knowledge based systems, artificial intelligence systems, data-bases and algorithms, (e) at least one additional imaging means from the group comprising conventional radar, synthetic aperture radar, infra-red imaging systems, milimetric wave imaging systems, acoustic systems and magnetic systems, and (f) an image analysis means for analyzing the content of said higher quality images from said second camera with reference to said data-base, assessing whether any further analysis of the content of said higher quality images from said second camera is required, controlling said second camera to provide further image information of any portion of said surveillance area requiring further analysis, and demanding further image information from said additional imaging means.10. A surveillance system comprising: (a) a first imaging means positioned to have a field of view of a surveillance area, for providing images of said surveillance area, (b) at least one second imaging means having the ability to provide more detailed information in relation to at least a portion of said surveillance area, (c) an image processing means for making a qualitative assessment of the images provided by the first imaging means and for causing, when the image is inadequate, operation of said second imaging means to provide more detailed information of said portion of the surveillance area, (d) a data-base comprising at least one data-base source from the group comprising human operators, rule based systems, knowledge based systems, artificial intelligence systems, data-bases and algorithms, and (e) an image analysis means for: analyzing the content of said more detailed information with reference to said data-base, assessing whether any further analysis of the content of said more detailed information from said second imaging means is required, and controlling said second imaging means to provide further information of any portion of said surveillance area requiring further analysis.11. A surveillance system, as in claim 10, wherein said second imaging means is selected from the group comprising conventional radar, synthetic aperture radar, infra-red imaging systems, millimetric way imaging systems, acoustic systems, magnetic systems and cameras providing a higher quality image.12. A vehicle surveillance system comprising: (a) a first camera positioned to view a passing vehicle and to produce an image thereof, (b) a second camera operable to produce a more detailed image of said passing vehicle, (c) an image processing means for making a qualitative assessment of said vehicle image provided by the first camera and for causing, when an image from the first camera is inadequate, operation of the second camera to provide a more detailed image of at least part of the vehicle, (d) a data-base comprising at least one data-base source from the group comprising human operators, rule based systems, knowledge-based systems, artificial intelligence systems, data-bases and algorithms, and (e) an image analysis means for; analyzing the content of said more detailed vehicle image with reference to said data-base, assessing whether any further analysis of said more detailed vehicle image is required, and controlling at least one of said cameras to provide a further image of at least part of the vehicle.13. A vehicle surveillance system comprising: (a) a first camera positioned to view passing vehicles and to produce images thereof, (b) an imaging means operable to produce a more detailed image of any part of a passing vehicle, said imaging means being selected from the group comprising conventional radar, synthetic aperture radar, infra-red imaging systems, millimetric way imaging systems, acoustic systems, magnetic systems and cameras producing a high quality image, (c) an image processing means for making a qualitative assessment of said vehicle images provided by said first camera and for causing, when an image from the first camera is inadequate, operation of said imaging means to provide a more detailed image of at least part of the vehicle, (d) a data-base comprising at least one data-base source from the group comprising human operators, rule based systems, knowledge based systems, artificial intelligence systems, data-bases and algorithms, and (e) an image analysis means for analyzing the content of said more detailed image with reference to said data-base.14. A vehicle surveillance system, as in claim 13, wherein said image analysis means is arranged to analyze the content of said more detailed image to identify at least one feature from the group comprising vehicle color, vehicle registration, vehicle speed, and image of its driver.15. A vehicle surveillance system, as in claim 13, wherein said image analysis means is also arranged for: assessing whether any further identification features of the vehicle are required, and controlling said imaging means to capture such further identification features of the said vehicle.16. A vehicle surveillance system, as in claim 13, wherein said image analysis means is arranged to assess whether at least one required feature from the group comprising vehicle color, vehicle registration, vehicle speed and image of its driver has been identified, and controlling said imaging means to identify said required feature.",US6970576_B1.txt,"G02B7/08,G03B15/00,G03B17/00,G03B17/56,G06T1/00,G08B13/196,G08B15/00,G08G1/017,G08G1/04,H04N7/18","{'electric communication technique', 'signalling', 'computing; calculating; counting', 'optics', 'photography; cinematography; analogous techniques using waves other than optical waves; electrography; holography'}","['optical elements, systems or apparatus', 'apparatus or arrangements for taking photographs or for projecting or viewing them; apparatus or arrangements employing analogous techniques using waves other than optical waves; accessories therefor (optical parts of such apparatus g02b; photosensitive materials or processes for photographic purposes g03c; apparatus for processing exposed photographic materials g03d)', 'apparatus or arrangements for taking photographs or for projecting or viewing them; apparatus or arrangements employing analogous techniques using waves other than optical waves; accessories therefor (optical parts of such apparatus g02b; photosensitive materials or processes for photographic purposes g03c; apparatus for processing exposed photographic materials g03d)', 'apparatus or arrangements for taking photographs or for projecting or viewing them; apparatus or arrangements employing analogous techniques using waves other than optical waves; accessories therefor (optical parts of such apparatus g02b; photosensitive materials or processes for photographic purposes g03c; apparatus for processing exposed photographic materials g03d)', 'image data processing or generation, in general', 'signalling or calling systems; order telegraphs; alarm systems', 'signalling or calling systems; order telegraphs; alarm systems', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'pictorial communication, e.g. television']","surveillance system with autonomic control A surveillance system which allows an operator to determine the type of objects or events which trigger image capture and analysis. Analysis of captured images is provided by access to various type of databases thereby providing for positive identification and monitoring of objects or events, and further provided for the system to issue control commands or prompt operations if required.",electric communication technique signalling computing; calculating; counting optics photography; cinematography; analogous techniques using waves other than optical waves; electrography; holography
US2014188350_A1,2012-07-12,2014-07-03,2011-07-13,"JAGUAR LAND ROVERGASZCZAK, ANNAPOPHAM, THOMAS","GASZCZAK, ANNAPOPHAM, THOMAS",44586491,road monitoring,vehicle control system and method,"The present invention provides a vehicle control system (110) and method operable to control at least one vehicle subsystem (120, 130, 140, 150, 160) to operate in a selected one of a plurality of subsystem configuration modes. The control system is operable in a plurality of driving modes in each of which it is arranged to select the subsystem configuration mode of the at least one vehicle subsystem in a manner suitable for a respective type of driving surface. The control system uses the output of an imaging device (115) capturing the driving surface on which the vehicle is driving or will likely be driving.","1. A control system for a vehicle, operable to assume one of a plurality of driving modes in each of which at least one vehicle subsystem is operated in a respective one of a plurality of subsystem configuration modes, the control system comprising a controller or processor operable to receive an image from an imaging device of a driving surface over which the vehicle is driving or may subsequently drive and to select a driving mode appropriate to the driving surface in dependence on a content of one or more images output by the imaging device.2. A system as claimed in claim 1 operable automatically to assume the appropriate driving mode for the type of driving surface determined by the controller or processor in dependence on the content of the one or more images.3. A system as claimed in claim 1 operable to provide an indication to a driver of the appropriate driving mode for the type of driving surface determined by the controller or processor in dependence on the content of the one or more images.4. A system as claimed in claim 3 wherein the indication of the appropriate driving mode is provided by one selected from amongst a visual indication and an audible indication.5. A system as claimed in claim 1 arranged to determine the type of the driving surface in dependence on at least one selected from amongst a texture and a composition of the driving surface as determined according to the output of the imaging device.6. A system as claimed in claim 1 arranged to determine the type of the driving surface in dependence on a colour of the driving surface as determined according to the output of the imaging device.7. A system as claimed in claim 6 arranged to determine the type of the driving surface in dependence on colour features normalised with respect to illumination.8. A system as claimed in claim 7 arranged to determine the type of the driving surface in dependence on a composition of the driving surface determined by reference to a texture and a colour of the driving surface as determined according to the output of the imaging device.9. A system as claimed in claim 1 arranged to determine the type of the driving surface in dependence on the output of the imaging device in combination with an output of location determining apparatus, the location determining apparatus being operable to provide an output in dependence on a geographical location of the vehicle.10. A system as claimed in claim 9 in combination with location determining apparatus, wherein the location determining apparatus is operable to provide an output indicative of a type of driving surface on which the vehicle is operating.11. A system as claimed in claim 10 arranged to compare the type of driving surface determined by a control unit in dependence on the output of the imaging device with a type of driving surface determined by reference to the location determining apparatus, in the event of a discrepancy the control unit being arranged to determine which of the driving surface types is the correct type by reference to data indicating a likely correct type of driving surface for a given combination of a type determined in dependence on the output of the imaging device and a type determined by reference to the location determining apparatus.12. A system as claimed in claim 1 operable automatically to switch on an illumination source of the vehicle in the event a level of ambient illumination is insufficient for the imaging device to capture a suitable image.13. A system as claimed in claim 12 operable automatically to switch on one or more headlamps of the vehicle.14. A system as claimed in claim 1 operable to trigger a window wash and/or wipe feature in order to clean a window through which the imaging device views the driving surface.15. A system as claimed in claim 14 operable automatically to switch on a wiper of the vehicle thereby to clean the window.16. A system as claimed in claim 1 wherein the imaging device is arranged to detect at least one selected from amongst visible light, infra-red light and ultraviolet light.17. A system as claimed in claim 1 operable to determine whether the vehicle is towing an object and to determine the appropriate driving mode in dependence on the determination whether the vehicle is towing and the type of driving surface.18. A system as claimed in claim 17 operable to determine whether the vehicle is towing by reference to the content of one or more images output by the imaging device.19. A system as claimed in claim 17 operable to determine whether the vehicle is towing by reference to whether the object under tow is electrically connected to the vehicle.20. A system as claimed in claim 1 wherein the driving modes include an off-road mode in which the subsystem configurations are controlled in a manner suitable for driving on rough terrain and an on-road mode in which the subsystem configurations are controlled in a manner suitable for driving on-road.21. A system as claimed in claim 1 wherein the driving modes include at least one low friction mode in which the subsystem configurations are controlled in a manner suitable for driving on low friction surfaces and a high friction mode in which the subsystem configurations are controlled in a manner suitable for driving on high friction surfaces.22. A system as claimed in claim 21 wherein the low friction mode is arranged to be suitable for driving on grass, gravel and snow.23. A system as claimed in claim 1 wherein the driving modes are selectable by a driver of the vehicle.24. A system as claimed in claim 23 wherein the driving modes may be selected by means of a terrain selection input arranged to influence the configuration mode selected on the basis of the terrain selected.25. A system as claimed in claim 23 wherein the driving modes may be selected by means of a mode of use input arranged to influence the configuration mode selected on the basis of a selected mode of use of the vehicle.26. A system as claimed in claim 25 wherein the mode of use input is arranged to allow selection between a plurality of driving styles.27. A system as claimed in claim 26 wherein the driving styles includes a sport style.28. A system as claimed in claim 1 operable to determine a likely path of a vehicle over the driving surface in dependence on one or more input signals and to determine the type of the driving surface over which the vehicle will likely drive in dependence on the content of one or more images output by the device.29. A system as claimed in claim 28 operable to determine a likely path of respective left and right wheels of the vehicle and to determine the type of the driving surface over which the respective wheels will drive in dependence on the content of one or more images output by the device.30. A system as claimed in claim 28 operable to determine a likely path of each respective wheel of the vehicle and to determine the type of the driving surface over which the respective wheels will drive in dependence on the content of one or more images output by the device.31. A system as claimed in claim 1 in the form of a vehicle control unit.32. A system as claimed in claim 1 in combination with the at least one vehicle subsystem.33. A vehicle having a system as claimed in claim 1.34. A method of controlling at least one vehicle subsystem, comprising: receiving an output of an imaging device arranged to output an image of a driving surface over which the vehicle is driving or may subsequently drive; and in dependence on a content of one or more images output by the imaging device, selecting a driving mode appropriate to the driving surface from a plurality of driving modes in each of which at least one vehicle subsystem is operated in a respective one of a plurality of subsystem configuration modes.35. A computer program product comprising a medium on which or in which is stored computer program code which, when executed in a computer system, will perform the method of claim 34.",US2014188350_A1.txt,"B60W10/06,B60W10/10,B60W10/18,B60W10/20,B60W10/22,B60W40/06,B60W40/076,B60W50/00,B60W50/08,B60W50/14",{'vehicles in general'},"['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit']","vehicle control system and method The present invention provides a vehicle control system (110) and method operable to control at least one vehicle subsystem (120, 130, 140, 150, 160) to operate in a selected one of a plurality of subsystem configuration modes. The control system is operable in a plurality of driving modes in each of which it is arranged to select the subsystem configuration mode of the at least one vehicle subsystem in a manner suitable for a respective type of driving surface. The control system uses the output of an imaging device (115) capturing the driving surface on which the vehicle is driving or will likely be driving.",vehicles in general
US2019378280_A1,2019-08-23,2019-12-12,2019-08-09,LG ELECTRONICS,"LEE, HYUN OKSIN, MAN-SOOSONG, SANGHOONCHO, CHANGSEOKPARK, WOONGHEEKANG, CHEOLJEON, SANGKUK",67775600,road monitoring,xr device and method for controlling the same,"An extended reality (XR) device and a method for controlling the same are disclosed. The XR device is applicable to 5G communication technology, robot technology, autonomous driving technology, and Artificial Intelligence (AI) technology. The XR device includes a communication module configured to communicate with a Head-Mounted Display (HMD) that is worn by a user to provide the user with Virtual Reality (VR) content, a camera configured to receive an image of a first space including the user, and a processor configured to transmit VR content to the HMD through the communication module. The processor recognizes a position of the user and positions of a plurality of real-world objects within an image of the first space. When the user moves closer to at least one of the recognized objects by a predetermined distance or less, the processor warns the user of possibility of collision with the at least one recognized object.","1. An extended reality (XR) device comprising: a communication module configured to communicate with a Head-Mounted Display (HMD) that is worn by a user to provide the user with Virtual Reality (VR) content; a camera configured to receive an image of a first space including the user; and a processor configured to transmit VR content to the HMD through the communication module, wherein the processor is further configured to: recognize a position of the user and positions of a plurality of real-world objects within an image of the first space, and when the user moves closer to at least one of the recognized objects by a predetermined distance or less, warn the user of possibility of collision with the at least one recognized object.2. The XR device according to claim 1, wherein the processor is further configured to: create a first tree structure composed of nodes corresponding to the objects, and store the first tree structure in a memory of the XR device, wherein the nodes include information about positions and sizes of the objects; and determine whether the user moves closer to at least one of the objects by a predetermined distance or less using the stored first tree structure.3. The XR device according to claim 2, wherein the processor is further configured to determine whether the user has moved closer to at least one object corresponding to at least one node from among the nodes by a predetermined distance or less.4. The XR device according to claim 2, wherein the processor is further configured to: recognize bounding-boxes of the objects within the image, and determine whether the user has moved closer to at least one of the bounding-boxes of the objects by a predetermined distance or less.5. The XR device according to claim 4, wherein the processor is further configured to: recognize a bounding-box of at least one body part of the user within the image; and determine whether the bounding-box of the at least one body part moves closer to at least one of the bounding-boxes of the objects by a predetermined distance or less.6. The XR device according to claim 2, wherein the objects include at least one object that does not move at all within the image, and at least one object that is not changed in size within the image.7. The XR device according to claim 2, wherein the processor is further configured to: when a new object, that has moved within the image of the first space or has been changed in size within the image of the first space, is recognized, add a new node corresponding to the new object to the first tree structure, and store the first tree structure added with the new node in the memory, wherein the new node includes information about a position and size of the new object.8. The XR device according to claim 2, wherein the processor is further configured to: after the user has moved from one position to another position, when a specific object, which has been covered by the user prior to the movement of the user, is recognized, add a specific node corresponding to the specific object to the first tree structure, and store the first tree structure added with the specific node in the memory, wherein the specific node includes information about a position and size of the specific object.9. The XR device according to claim 2, wherein: the memory stores the first tree structure and a second tree structure that is composed of respective nodes corresponding to objects included in a second space different from the first space, the processor is further configured to: when the user moves between the first space and the second space, create a third tree structure that is composed of nodes related to a movement direction of the user from among nodes included in the first and second tree structures, store the third tree structure in the memory, and determine whether the moving user moves closer to at least one of objects included in the third tree structure by a predetermined distance or less.10. The XR device according to claim 1, wherein the processor is further configured to transmits collision alert information to the HMD through the communication module such that the collision alert information including information about a direction of the at least one object located adjacent to the user and an estimated collision distance of the at least one adjacent object is displayed on an output screen image of the VR content of the HMD.11. A method for controlling an extended reality (XR) device comprising: transmitting Virtual Reality (VR) content to a Head-Mounted Display (HMD) worn by a user; receiving an image of a first space including the user through a camera; recognizing a position of the user and positions of a plurality of real-world objects within the image of the first space; determining whether the user moves closer to at least one of the recognized objects by a predetermined distance or less; and warning the user of possibility of collision with the at least one object located adjacent to the user according to the result of determination.12. The method according to claim 11, further comprising: creating a first tree structure composed of nodes corresponding to the objects, storing the first tree structure in a memory of the XR device, wherein the nodes include information about positions and sizes of the objects; and determining whether the user moves closer to at least one of the objects by a predetermined distance or less using the stored first tree structure.13. The method according to claim 12, wherein the determining includes: determining whether the user has moved closer to at least one object corresponding to at least one node from among the nodes by a predetermined distance or less.14. The method according to claim 12, wherein the determining includes: recognizing bounding-boxes of the objects within the image; and determining whether the user has moved closer to at least one of the bounding-boxes of the objects by a predetermined distance or less.15. The method according to claim 14, wherein: recognizing a bounding-box of at least one body part of the user within the image; and determining whether the bounding-box of the at least one body part moves closer to at least one of the bounding-boxes of the objects by a predetermined distance or less.16. The method according to claim 12, wherein the objects include at least one object that does not move at all within the image, and at least one object that is not changed in size within the image.17. The method according to claim 12, further comprising: when a new object, that has moved within the image of the first space or has been changed in size within the image of the first space, is recognized, adding a new node corresponding to the new object to the first tree structure, and storing the first tree structure added with the new node in the memory, wherein the new node includes information about a position and size of the new object.18. The method according to claim 12, wherein the determining includes: after the user has moved from one position to another position, when a specific object, which has been covered by the user prior to the movement of the user, is recognized, adding a specific node corresponding to the specific object to the first tree structure, and storing the first tree structure added in the specific node in the memory, wherein the specific node includes information about a position and size of the specific object.19. The method according to claim 12, wherein: the memory stores the first tree structure and a second tree structure that is composed of respective nodes corresponding to objects included in a second space different from the first space, wherein the determining includes: when the user moves between the first space and the second space, creating a third tree structure that is composed of nodes related to a movement direction of the user from among nodes included in the first and second tree structures, and storing the third tree structure in the memory, and determining whether the moving user moves closer to at least one of objects included in the third tree structure by a predetermined distance or less.20. The method according to claim 11, wherein the warning the user of the possibility of collision with the at least one object located adjacent to the user includes: transmitting collision alert information to the HMD such that the collision alert information including information about a direction of the at least one object located adjacent to the user and an estimated collision distance of the at least one adjacent object is displayed on an output screen image of the VR content of the HMD.",US2019378280_A1.txt,"G06T11/00,G06T7/246,G06T7/70",{'computing; calculating; counting'},"['image data processing or generation, in general', 'image data processing or generation, in general', 'image data processing or generation, in general']","xr device and method for controlling the same An extended reality (XR) device and a method for controlling the same are disclosed. The XR device is applicable to 5G communication technology, robot technology, autonomous driving technology, and Artificial Intelligence (AI) technology. The XR device includes a communication module configured to communicate with a Head-Mounted Display (HMD) that is worn by a user to provide the user with Virtual Reality (VR) content, a camera configured to receive an image of a first space including the user, and a processor configured to transmit VR content to the HMD through the communication module. The processor recognizes a position of the user and positions of a plurality of real-world objects within an image of the first space. When the user moves closer to at least one of the recognized objects by a predetermined distance or less, the processor warns the user of possibility of collision with the at least one recognized object.",computing; calculating; counting
CN110850109_A,2019-11-21,2020-02-28,2019-11-21,INTELLICLOUD TECHNOLOGY COMPANY,WANG FEI,69603359,speed & trajectory,method for measuring vehicle speed based on fuzzy image,"The invention discloses a method for measuring vehicle speed based on a fuzzy image. The method for measuring the vehicle speed based on the fuzzy image comprises the following steps that step 1, a single vehicle scene image is shot in real time; step 2, a calibration function between road surface distance and pixel distance is calculated according to the vehicle scene image; step 3, a vehicle image is extracted from the current vehicle scene image; step 4, fuzziness calculating is carried out on the vehicle image, and a fuzzy pixel value is output; and step 5, the vehicle speed is calculatedaccording to the calibration function, the fuzzy pixel value and shooting exposure time. According to the method for measuring the vehicle speed based on the fuzzy image, the single image is used forcalculating the vehicle speed in the image by a deep learning model evaluating the fuzziness, and the installation and measurement process of vehicle speed measuring equipment can be effectively simplified.","1. a method for measuring vehicle speed based on blurred images is characterized by comprising the following steps: step 1, shooting a single vehicle scene image in real time; step 2, calculating a calibration function between the road surface distance and the pixel distance according to the vehicle scene image; step 3, extracting a vehicle image in the current vehicle scene image; step 4, calculating the fuzziness of the vehicle image and outputting a fuzzy pixel value; and 5, calculating the vehicle running speed according to the calibration function, the fuzzy pixel value and the shooting exposure time. 2. the method for measuring the vehicle speed based on the blurred image as claimed in claim 1, wherein the vehicle running speed is calculated by adopting the following formula: wherein: v is the vehicle speed; h (x) is a calibration function; z is the minimum pixel distance between the vehicle image and the first calibration line segment;  z are blurred pixel values;  t is the shot exposure time. 3. the method for measuring vehicle speed based on blurred image as claimed in claim 2, wherein the step 2 comprises the following sub-steps: step 2.1, respectively selecting a first calibration line segment and a second calibration line segment which are perpendicular to the length direction of the road surface and parallel to each other in the vehicle scene image, and measuring the actual lengths of the first calibration line segment and the second calibration line segment as d respectively1and d2; step 2.2, calculating pixels of the first calibration line segment and the second calibration line segment according to the vehicle scene imageeach length is pd1and pd2calculating the pixel distance h between the first calibration line segment and the second calibration line segmentp; step 2.3, calculating a calibration function h (x) according to the following calculation formula: wherein: 4. the method for measuring the vehicle speed based on the blurred image as claimed in claim 3, wherein the distance between the first calibration line segment and the second calibration line segment is greater than or equal to the length of the vehicle. 5. the method for measuring the vehicle speed based on the blurred image as claimed in any one of claims 1 to 4, wherein a high-speed camera is used for shooting the vehicle image in the step 1, and a plurality of groups of fixed shutter exposure time are set. 6. the method of claim 5, wherein the fixed shutter exposure time is adjusted based on ambient light intensity. 7. the method for measuring vehicle speed based on blurred images as claimed in claim 1, wherein in step 3, a deep learning target detection method is adopted to extract the vehicle images in the current image scene. 8. the method for measuring vehicle speed based on blurred images as claimed in claim 1, wherein in step 4, a deep learning target detection method is adopted to perform blur degree calculation on the vehicle images and output blurred pixel values.",CN110850109_A.txt,"G01P3/38,G06K9/00,G06T7/00","{'measuring; testing', 'computing; calculating; counting'}","['measuring linear or angular speed, acceleration, deceleration, or shock; indicating presence, absence, or direction, of movement (measuring or recording blood flow a61b5/02, a61b8/06; monitoring speed or deceleration of electrically-propelled vehicles b60l3/00; vehicle lighting systems adapted to indicate speed b60q1/54; determining position or course in navigation, measuring ground distance in geodesy or surveying g01c; combined measuring devices for measuring two or more variables of movement g01c23/00; measuring velocity of sound g01h; measuring velocity of light g01j7/00; measuring direction or velocity of solid objects by reception or emission of radiowaves or other waves and based on propagation effects, e.g. doppler effect, propagation time, direction of propagation, g01s; measuring speed of nuclear radiation g01t; measuring acceleration of gravity g01v; {measuring or recording the speed of trains b61l23/00; speed indicators incorporated in motor vehicles b60k35/00; measuring frequency or phase g01r; traffic control g08g})', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general']","method for measuring vehicle speed based on fuzzy image The invention discloses a method for measuring vehicle speed based on a fuzzy image. The method for measuring the vehicle speed based on the fuzzy image comprises the following steps that step 1, a single vehicle scene image is shot in real time; step 2, a calibration function between road surface distance and pixel distance is calculated according to the vehicle scene image; step 3, a vehicle image is extracted from the current vehicle scene image; step 4, fuzziness calculating is carried out on the vehicle image, and a fuzzy pixel value is output; and step 5, the vehicle speed is calculatedaccording to the calibration function, the fuzzy pixel value and shooting exposure time. According to the method for measuring the vehicle speed based on the fuzzy image, the single image is used forcalculating the vehicle speed in the image by a deep learning model evaluating the fuzziness, and the installation and measurement process of vehicle speed measuring equipment can be effectively simplified.",measuring; testing computing; calculating; counting
US9950700_B2,2016-03-30,2018-04-24,2016-03-30,GM GLOBAL TECHNOLOGY OPERATIONS,"ZENG, SHUQINGLITKOUHI, BAKHTIAR B.ZHAO, QINGRONGTONG, WEI",59885756,road monitoring,road surface condition detection with multi-scale fusion,"A method of determining a surface condition of a path of travel. A plurality of images is captured of a surface of the path of travel by an image capture device. The image capture device captures images at varying scales. A feature extraction technique is applied by a feature extraction module to each of the scaled images. A fusion technique is applied, by the processor, to the extracted features for identifying the surface condition of the path of travel. A road surface condition signal provide to a control device. The control device applies the road surface condition signal to mitigate the wet road surface condition.","1. A method of determining a surface condition of a path of travel comprising the steps of: obtaining data of a surface of the path of travel by sensor-based devices, the sensor-based devices capturing data at different sized scales; applying a feature extraction technique, by a feature extraction module, to each of the scaled data; wherein each feature extracted by the feature extraction module is independently input to a classifier, wherein the classifier outputs a respective classified result for each extracted feature input to the processor; wherein the classifier provides the classified results for each extracted feature to a decision level fusion module of the processor, the decision level fusion module determining the surface condition of the path of travel based on fused classified results; wherein the fused classified results are determined based on the following formula:description=""In-line Formulae"" end=""lead""?C=i=1, . . . , kwiCi description=""In-line Formulae"" end=""tail""? wherein Ci is the classified result of each extracted feature by the classifier, and wi is a weighting function; applying a fusion technique, by the processor, to the extracted features for identifying the surface condition of the path of travel; and providing a road surface condition signal to a control device, the control device applying the road surface condition signal to mitigate the wet road surface condition.2. The method of claim 1 wherein each of the extracted features from each scale data are concatenated in a feature level fusion module of the processor.3. The method of claim 2 wherein an output from the feature level fusion module is input to a classifier for identifying the surface condition of the road.4. The method of claim 1 wherein the weighting function is determined by a classification confidence of the classified result.5. The method of claim 4 wherein the weighting function is determined by prior knowledge of a scaling size of the captured image.6. The method of claim 1 wherein each of the extracted features are input to a feature level fusion module of the processor to generate a feature level fusion result and a decision level fusion module of the processor to generate a decision level fusion result.7. The method of claim 6 wherein the feature level fusion result is determined as a function of extracting features from each respective scaled data, wherein each extracted feature is input to the feature level fusion module for generating the feature level fusion result.8. The method of claim 7 wherein the feature level fusion result is input to a classifier for identifying the surface condition of the road as a function of the extracted features.9. The method of claim 8 wherein the decision level fusion result is determined as a function of extracting features from each respective scaled data by the decision level fusion module, wherein each feature extracted by the feature extraction module is independently input to the classifier, wherein the classifier outputs a respective classified result for each extracted feature, and wherein the classifier provides the classified results to the decision level fusion module for generating a road surface condition based on decision level fusion result.10. The method of claim 9 wherein the classified feature level fusion result and the decision level fusion result are input to a hybrid fusion module of the processor to generate a hybrid fusion result identifying the surface condition of the path of travel.11. The method of claim 10 wherein fusion module determines the hybrid fusion result based on the following formula:description=""In-line Formulae"" end=""lead""?Ch=wC+wdCd description=""In-line Formulae"" end=""tail""? where C is the feature level fusion result, w is a classification confidence weight, Ca is decision level fusion result, and wd is a classification confidence result.12. The method of claim 1 wherein fusion results are determined as a function of extracting features from each respective scaled data, wherein each feature extracted by the feature extraction module for determining the fusion results is independently determined using a deep learning technique.13. The method of claim 1 wherein the road surface condition signal is provided to a vehicle controller, the controller using the road surface condition signal to determine an autonomous actuating vehicle braking strategy.14. The method of claim 1 wherein the road surface condition signal is provided to a vehicle controller, the controller using the road surface condition signal to determine a traction control strategy for autonomously actuating a traction control system.15. The method of claim 1 wherein the identified road surface condition signal is provided to a wireless communication system for alerting other vehicles of the identified surface condition on the path of travel.16. The method of claim 1 wherein the road surface condition signal alerts a driver of a potential reduced traction between vehicle tires and the road surface.17. The method of claim 1 wherein the identified road surface condition signal alerts a driver of the vehicle against a use of automated features.18. The method of claim 1 wherein the identified road surface condition signal is provided to a vehicle controller, the controller autonomously modifying a control setting of an automated control feature in response to the road surface condition signal.19. The method of claim 1 wherein the identified road surface condition signal is provided to a vehicle controller, the controller autonomously disabling cruise control in response to the road surface condition signal.20. The method of claim 1 wherein the identified road surface condition signal alerts a driver to reduce a vehicle speed.21. The method of claim 1 the data is obtained by active capture devices.",US9950700_B2.txt,"B60Q9/00,B60T7/12,B60T8/17,B60T8/171,B60T8/175",{'vehicles in general'},"['arrangement of signalling or lighting devices, the mounting or supporting thereof or circuits therefor, for vehicles in general', 'vehicle brake control systems or parts thereof; brake control systems or parts thereof, in general (electrodynamic brake systems for vehicle, in general b60l; brakes per se, i.e. devices where braking effect occurs, including ultimate brake actuators, f16d); arrangement of braking elements on vehicles in general; portable devices for preventing unwanted movement of vehicles; vehicle modifications to facilitate cooling of brakes', 'vehicle brake control systems or parts thereof; brake control systems or parts thereof, in general (electrodynamic brake systems for vehicle, in general b60l; brakes per se, i.e. devices where braking effect occurs, including ultimate brake actuators, f16d); arrangement of braking elements on vehicles in general; portable devices for preventing unwanted movement of vehicles; vehicle modifications to facilitate cooling of brakes', 'vehicle brake control systems or parts thereof; brake control systems or parts thereof, in general (electrodynamic brake systems for vehicle, in general b60l; brakes per se, i.e. devices where braking effect occurs, including ultimate brake actuators, f16d); arrangement of braking elements on vehicles in general; portable devices for preventing unwanted movement of vehicles; vehicle modifications to facilitate cooling of brakes', 'vehicle brake control systems or parts thereof; brake control systems or parts thereof, in general (electrodynamic brake systems for vehicle, in general b60l; brakes per se, i.e. devices where braking effect occurs, including ultimate brake actuators, f16d); arrangement of braking elements on vehicles in general; portable devices for preventing unwanted movement of vehicles; vehicle modifications to facilitate cooling of brakes']","road surface condition detection with multi-scale fusion A method of determining a surface condition of a path of travel. A plurality of images is captured of a surface of the path of travel by an image capture device. The image capture device captures images at varying scales. A feature extraction technique is applied by a feature extraction module to each of the scaled images. A fusion technique is applied, by the processor, to the extracted features for identifying the surface condition of the path of travel. A road surface condition signal provide to a control device. The control device applies the road surface condition signal to mitigate the wet road surface condition.",vehicles in general
US9928427_B2,2015-12-03,2018-03-27,2015-12-03,GM GLOBAL TECHNOLOGY OPERATIONS,"LITKOUHI, BAKHTIAR B.WANG JINSONGZHANG, QIZHAO, QINGRONG",58722911,road monitoring,vision-based wet road surface condition detection using tire rearward splash,A method for determining a wet surface condition of a road. An image of a road surface is captured by an image capture device of the host vehicle. The image capture device is mounted on a side of the host vehicle and captures an image in a downward direction. A region of interest rearward of the wheel of the host vehicle is identified in the captured image by a processor. The region of interest is representative of where rearward splash as generated by the wheel occurs. A determination is made whether precipitation is present in the region of interest by applying a filter to the image. A wet road surface signal is generated in response to the identification of precipitation in the region of interest.,"1. A method for determining a wet surface condition of a road, the method comprising the steps of: capturing an image of a road surface by an image capture device of the host vehicle, the image capture device mounted on a side of the host vehicle and capturing an image in a downward direction; identifying in the captured image, by a processor, a region of interest rearward of the wheel of the host vehicle, the region of interest representative of where rearward splash as generated by the wheel occurs; applying a filter to the region of interest in the image for identifying precipitation, including applying an edge filter for identifying an edge between an area of splash and an area of no splash in the region of interest, wherein applying the edge filter includes applying a texture classification filter, wherein the texture classification filter utilizes an LM filter bank including a plurality of filters having various orientations, wherein the plurality of filters applied to the region of interest assists in identifying edges within the region of interest, wherein a feature analysis is applied to responses from the LM filter bank to determine whether precipitation is present, and wherein a response from the LM filter bank having a magnitude of 0 indicates a no-splash condition; determining whether precipitation is present in the region of interest; and generating a wet road surface signal in response to the identification of precipitation in the region of interest.2. The method of claim 1 wherein the LM filter bank are first and second derivatives at predetermined orientations and predetermined sized scales.3. The method of claim 1 wherein a response from an LM filter having a magnitude with an absolute value that is greater than 0 indicates precipitation.4. The method of claim 3 wherein a machine learning technique is applied to train an offline classifier utilizing extracted features from a plurality of sample images, wherein the classifier is implemented online within the vehicle to detect edges based on extracted features from feature analysis in determining whether precipitation is present as a function of responses from the edge filter.5. The method of claim 4 wherein an intensity value of a feature is determined using the following formula: where N is the total number of pixels in the region of interest of the filtered image that make up the k-th filter, and Ik(i) is a pixel value of the i-th pixel of the filter image k-th LM filter.6. The method of claim 1 wherein capturing an image in a downward direction includes a real downward image of the road surface.7. The method of claim 1 wherein capturing an image in a downward direction includes generating a virtual image in a downward direction based on the real image, wherein a virtual image is generated by reorienting the image so that the virtual image is generated as if a camera pose is facing downward.8. The method of claim 7 wherein reorienting the image to generate the virtual image comprises the steps of: identifying the virtual pose; mapping of each virtual point on the virtual image to a corresponding point on the real image.9. The method of claim 1 wherein the wet road surface signal is provided to a vehicle controller, the controller autonomously actuating vehicle braking for mitigating condensation build-up on vehicle brakes.10. The method of claim 1 wherein the wet road surface signal is provided to a vehicle controller, the controller autonomously actuating a traction control system for mitigating condensation build-up on vehicle brakes.11. The method of claim 1 wherein the wet road surface signal is provided to a wireless communication system for alerting other vehicles of the wet road surface condition.12. The method of claim 1 wherein the wet road surface signal alerts a driver of a potential reduced traction between vehicle tires and the road surface.13. The method of claim 1 wherein the wet road surface signal alerts a driver of the vehicle against a use of cruise control.14. The method of claim 1 wherein the wet road surface signal alerts a driver of the vehicle against a use of automated features.15. The method of claim 14 wherein the wet road surface signal is provided to a vehicle controller, the controller autonomously disabling cruise control.16. The method of claim 1 wherein the wet road surface signal alerts a driver to reduce a vehicle speed.17. The method of claim 1 wherein the wet road surface signal is provided to a vehicle controller for shutting baffles on an air intake scoop of a vehicle for preventing water ingestion.18. The method of claim 1 wherein the wet road surface signal is provided to a vehicle controller, the controller autonomously modifying a control setting of an automated control feature in response to the wet road surface condition.",US9928427_B2.txt,"B60W40/06,G06K9/00,G06K9/32,G06K9/46,G06K9/62,G06T7/00,G06T7/40,H04N7/18","{'electric communication technique', 'computing; calculating; counting', 'vehicles in general'}","['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general', 'image data processing or generation, in general', 'pictorial communication, e.g. television']",vision-based wet road surface condition detection using tire rearward splash A method for determining a wet surface condition of a road. An image of a road surface is captured by an image capture device of the host vehicle. The image capture device is mounted on a side of the host vehicle and captures an image in a downward direction. A region of interest rearward of the wheel of the host vehicle is identified in the captured image by a processor. The region of interest is representative of where rearward splash as generated by the wheel occurs. A determination is made whether precipitation is present in the region of interest by applying a filter to the image. A wet road surface signal is generated in response to the identification of precipitation in the region of interest.,electric communication technique computing; calculating; counting vehicles in general
US2020215970_A1,2019-11-23,2020-07-09,2019-01-07,HYUNDAI MOTOR COMPANYKIA MOTORS CORPORATION,"YOU, JUNG KEUNYEE, KAANGDOKLEE, JINMOKIM, JOONGKWAN",71403755,road monitoring,vehicle and control method for the same,"A vehicle is provided to include a bio-signal sensor that detects a bio-signal of a user, a display device that displays an image and a controller that determines at least one of a positivity of the user or change amount of the positivity of the user based on the detected bio-signal. The controller accumulates a positivity index when at least one of the positivity or change amount of the positivity is equal to or greater than a predetermined positivity or a predetermined change amount of the positivity and transmit a control signal to the display device to display the accumulated positivity index.","1. A vehicle, comprising: a bio-signal sensor configured to detect a bio-signal of a user; a display device configured to display an image; and a controller configured to determine at least one of a positivity of the user or change amount of the positivity of the user based on the detected bio-signal, accumulate a positivity index when at least one of the positivity or change amount of the positivity is equal to or greater than a predetermined positivity or a predetermined change amount of the positivity, and transmit a control signal to the display device to display the accumulated positivity index.2. The vehicle according to claim 1, wherein the display device is configured to display a cymatics image which is transformed according to a frequency, and the controller is configured to determine at least one of a frequency or size of the cymatics image in proportion to the positivity index, and transmit a control signal to the display device to display the cymatics image having at least one of the determined frequency or size.3. The vehicle according to claim 1, wherein: the controller is configured to transmit a control signal to the display device to display a predetermined image when the positivity index reaches a target positivity index.4. The vehicle according to claim 1, further comprising: a feedback device configured to operate to increase the positivity of the user; wherein the controller is configured to transmit a signal to the feedback device to operate when the positivity index reaches a target positivity index.5. The vehicle according to claim 4, wherein the feedback device includes vibration elements disposed on a seat of the vehicle, and the controller is configured to transmit at least one of a control signal for the vibration elements to vibrate at a predetermined frequency or a control signal for the vibration elements to vibrate at a predetermined intensity or more.6. The vehicle according to claim 4, wherein the feedback device includes a speaker disposed within the vehicle, and the controller is configured to transmit a control signal for the speaker to output a predetermined sound.7. The vehicle according to claim 4, wherein the feedback device includes a lighting device disposed within the vehicle, and the controller is configured to transmit at least one of a control signal for the lighting device to emit light at a predetermined frequency, a control signal for the lighting device to emit light at a predetermined brightness, or a control signal for the lighting device to emit light in a predetermined color.8. The vehicle according to claim 4, wherein the feedback device includes an air-conditioner disposed within the vehicle, and the controller is configured to transmit at least one of a control signal for the air-conditioner to output a predetermined scent or a control signal for the air-conditioner to output wind of a predetermined mode.9. The vehicle according to claim 1, wherein the controller is configured to reset the positivity index to an initial value when the positivity index reaches a target positivity index.10. The vehicle according to claim 1, further comprising: a camera configured to obtain image data for the user; wherein the controller is configured to determine the positivity of the user based on at least one of the image data for the user or the bio-signal of the user.11. The vehicle according to claim 1, further comprising: an inputter configured to receive information of at least one of an initial value of the positivity index or a cumulative condition of the positivity index.12. The vehicle according to claim 1, wherein the controller is configured to transmit the control signal for the display device to display the accumulated positivity index in a form of at least one of a numeral, an emoticon indicating an emotional state, gauge bar, or a letter.13. A vehicle control method, comprising: detecting, by a controller, a bio-signal of a user using a sensor; determining, by the controller, at least one of a positivity of the user or change amount of the positivity of the user based on the detected bio-signal; accumulating, by the controller, a positivity index when at least one of the positivity or change amount of the positivity is equal to or greater than a predetermined positivity or a predetermined change amount of the positivity; and displaying, by the controller, the accumulated positivity index.14. The vehicle control method according to claim 13, further comprising: determining, by the controller, at least one of a frequency or size of a cymatics image in proportion to the positivity index; and displaying, by the controller, the cymatics image having at least one of the determined frequency or size.15. The vehicle control method according to claim 13, further comprising: displaying, by the controller, a predetermined image when the positivity index reaches a target positivity index.16. The vehicle control method according to claim 13, further comprising: transmitting, by the controller, a control signal to operate a feedback device when the positivity index reaches a target positivity index.17. The vehicle control method according to claim 16, wherein the feedback device includes vibration elements disposed on a seat of a vehicle, and the transmitting of the control signal for operating the feedback device when the positivity index reaches the target positivity index includes: transmitting, by the controller, at least one of a control signal for the vibration elements to vibrate at a predetermined frequency or a control signal for the vibration elements to vibrate at a predetermined intensity or more.18. The vehicle control method according to claim 16, wherein the feedback device includes a speaker disposed within a vehicle, and the transmitting of the control signal for operating the feedback device when the positivity index reaches the target positivity index includes: transmitting, by the controller, a control signal for the speaker to output a predetermined sound.19. The vehicle control method according to claim 16, wherein the feedback device includes a lighting device disposed within a vehicle, and the transmitting the control signal for operating the feedback device when the positivity index reaches the target positivity index includes: transmitting, by the controller, at least one of a control signal for the lighting device to emit light at a predetermined frequency, a control signal for the lighting device to emit light at a predetermined brightness, or a control signal for the lighting device to emit light in a predetermined color.20. The vehicle control method according to claim 16, wherein the feedback device includes an air-conditioner disposed in a vehicle, and the transmitting of the control signal for operating the feedback device when the positivity index reaches the target positivity index includes: transmitting, by the controller, at least one of a control signal for the air-conditioner to output a predetermined scent or a control signal for the air-conditioner to output wind of a predetermined mode.",US2020215970_A1.txt,"A61B5/00,A61B5/16,A61B5/18,B60H1/00,B60H3/00,B60N2/90,B60Q9/00,B60R1/00,G06F3/01,G06K9/00","{'computing; calculating; counting', 'medical or veterinary science; hygiene', 'vehicles in general'}","['diagnosis; surgery; identification (analysing biological material g01n, e.g. g01n33/48; obtaining records using waves other than optical waves, in general g03b42/00)', 'diagnosis; surgery; identification (analysing biological material g01n, e.g. g01n33/48; obtaining records using waves other than optical waves, in general g03b42/00)', 'diagnosis; surgery; identification (analysing biological material g01n, e.g. g01n33/48; obtaining records using waves other than optical waves, in general g03b42/00)', 'arrangements of heating, cooling, ventilating or other air-treating devices specially adapted for passenger or goods spaces of vehicles', 'arrangements of heating, cooling, ventilating or other air-treating devices specially adapted for passenger or goods spaces of vehicles', 'seats specially adapted for vehicles; vehicle passenger accommodation not otherwise provided for', 'arrangement of signalling or lighting devices, the mounting or supporting thereof or circuits therefor, for vehicles in general', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'electric digital data processing (computer systems based on specific computational models g06n)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']","vehicle and control method for the same A vehicle is provided to include a bio-signal sensor that detects a bio-signal of a user, a display device that displays an image and a controller that determines at least one of a positivity of the user or change amount of the positivity of the user based on the detected bio-signal. The controller accumulates a positivity index when at least one of the positivity or change amount of the positivity is equal to or greater than a predetermined positivity or a predetermined change amount of the positivity and transmit a control signal to the display device to display the accumulated positivity index.",computing; calculating; counting medical or veterinary science; hygiene vehicles in general
US2012229647_A1,2012-01-01,2012-09-13,2011-03-08,"BANK OF AMERICA CORPORATIONCALMAN, MATTHEW A.ROSS, ERIK, STEPHEN","CALMAN, MATTHEW A.ROSS, ERIK, STEPHEN",46795219,survillance,real-time video image analysis for providing security,"System, method, and computer program product are provided for using real-time video analysis to provide the user of mobile devices with security, no matter the user's location. Through the use of real-time vision object recognition objects, logos, artwork, products, locations, and other features that can be recognized in the real-time video stream and frequented locations of the user can be established. In this way, a baseline layout of the frequented location is determined, such that the objects and individuals typically in the frequented area are recognized. The system may continue to take real-time video images of the frequented location, such that if a variation in the baseline layout occurs, the change may be alerted. In this way, a security compromise, such as a break-in to a user's home may be detected by the system and thus the system may notify the user or the appropriate authorities.","1. A method for providing security to a user, the method comprising: receiving a captured first image of an area frequented by the user; building a directory of data relating to the area frequented by the user, wherein the directory comprises information regarding objects and individuals within the area frequented by the user, wherein the directory of data is built using the captured first image of the area frequented by the user; receiving a captured second image of the area frequented by the user subsequent to the directory of data being built; recognizing variations between the directory of data and the captured second images with respect to the objects and the individuals within the area frequented by the user, through the use of a processor device; and presenting indicators associated with the variations, via the mobile device of the user, wherein the indicators provide a recommended security action based on the variations between the directory of data and the captured second image.2. The method of claim 1, wherein presenting the indicators associated with the variations further comprises superimposing the indicators associated with the variations over real-time video that is capture by the mobile device of an area frequented by the user, wherein the location of the objects and the individuals within the area frequented by the user, other than the variations, correspond to the directory of data relating to the area frequented by the user.3. The method of claim 1, wherein the directory of data comprises information regarding the objects that are normally found within the area frequented by the user.4. The method of claim 1, wherein the directory of data comprises information regarding the individuals that are normally found within the area frequented by the user.5. The method of claim 1, wherein the receiving the captured second image of the area frequented by the user comprises real-time imaging of the area frequented by the user.6. The method of claim 1, wherein the receiving the captured second image of the area frequented by the user comprises receiving real-time images of the area frequented by the user continuously while the mobile device is in the area frequented by the user.7. The method of claim 1, wherein presenting an indicator associated with the variation comprises displaying the indicator on a display of the mobile device.8. The method of claim 1, wherein presenting an indicator associated with the variation comprises superimposing the indicator over real-time video that is captured by the mobile device.9. The method of claim 1, wherein the indicator is selectable by the user.10. The method of claim 1, wherein the indicator, upon being selected, provides information regarding the variation based on security information related to the area frequented by the user.11. A system for providing security to a user, comprising: a memory device; a communication device; a processing device operatively coupled to the memory device and the communication device, wherein the processing device is configured to execute computer-readable program code to: receive a captured first image of an area frequented by the user; build a directory of data relating to the area frequented by the user, wherein the directory comprises information regarding objects and individuals within the area frequented by the user, wherein the directory of data is built using the captured first image of the area frequented by the user; receive, via a mobile device, a captured second image of the area frequented by the user subsequent to the directory of data being built; recognize variations between the directory and the captured second images with respect to the objects and the individuals within the area frequented by the user, through the use of a processor device; and present indicators associated with the variations a recommended security action based on the variations between the directory of data and the captured second image.12. The system of claim 11, wherein presenting the indicators associated with the variations comprises superimposing the indicators associated with the variations over real-time video that is capture by the mobile device of an area frequented by the user, wherein the location of the objects and the individuals within the area frequented by the user, other than the variations, correspond to the directory of data relating to the area frequented by the user.13. The system of claim 11, wherein the directory comprises information regarding the objects that are normally found within the area frequented by the user.14. The system of claim 11, wherein the directory comprises information regarding the individuals that are normally found within the area frequented by the user.15. The system of claim 11, wherein the receiving the captured second image of the area frequented by the user comprises real-time imaging of the area frequented by the user.16. The system of claim 11, wherein the receiving the captured second image of the area frequented by the user comprises receiving real-time images of the area frequented by the user continuously while the mobile device is in the area frequented by the user.17. The system of claim 11, wherein presenting an indicator associated with the variation comprises displaying the indicator on a display of the mobile device.18. The system of claim 11, wherein presenting an indicator associated with the variation comprises superimposing the indicator over real-time video that is captured by the mobile device.19. The system of claim 11, wherein the indicator is selectable by the user.20. The system of claim 11, wherein the indicator, upon being selected, provides information regarding the variation based on security information related to the area frequented by the user.21. A computer program product for providing landscaping design recommendations, the computer program product comprising at least one non-transitory computer-readable medium having computer-readable program code portions embodied therein, the computer-readable program code portions comprising: an executable portion configured for receiving a captured first image of an area frequented by the user; an executable portion configured for building a directory of data relating to the area frequented by the user, wherein the directory comprises information regarding objects and individuals within the area frequented by the user, wherein the directory of data is built using the captured first image of the area frequented by the user; an executable portion configured for receiving, via a mobile device a captured second image of the area frequented by the user subsequent to the directory of data being built; an executable portion configured for recognizing variations between the directory and the captured second images with respect to the objects and the individuals within the area frequented by the user, through the use of a processor device; and an executable portion configured for presenting indicators associated with the variations, via the mobile device of the user, a recommended security action based on the variations between the directory of data and the captured second image.22. The computer program product of claim 21, wherein presenting the indicators associated with the variations comprises superimposing the indicators associated with the variations over real-time video that is capture by the mobile device of an area frequented by the user, wherein the location of the objects and the individuals within the area frequented by the user, other than the variations, correspond to the directory of data relating to the area frequented by the user.23. The computer program product of claim 21, wherein the directory comprises information regarding the objects that are normally found within the area frequented by the user.24. The computer program product of claim 21, wherein the directory comprises information regarding the individuals that are normally found within the area frequented by the user.25. The computer program product of claim 21, wherein the receiving the captured second image of the area frequented by the user comprises real-time imaging of the area frequented by the user.26. The computer program product of claim 21, wherein the receiving the captured second image of the area frequented by the user comprises receiving real-time images of the area frequented by the user continuously while the mobile device is in the area frequented by the user.27. The computer program product of claim 21, wherein presenting an indicator associated with the variation comprises displaying the indicator on a display of the mobile device.28. The computer program product of claim 21, wherein presenting an indicator associated with the variation comprises superimposing the indicator over real-time video that is captured by the mobile device.29. The computer program product of claim 21, wherein the indicator is selectable by the user.30. The computer program product of claim 21, wherein the indicator, upon being selected, provides information regarding the variation based on security information related to the area frequented by the user.",US2012229647_A1.txt,"G06F3/048,G08B1/08,G08B13/196,G08B17/10,G08B21/04,G08B21/24,G08B25/00,G08B29/18,H04N7/18","{'electric communication technique', 'signalling', 'computing; calculating; counting'}","['electric digital data processing (computer systems based on specific computational models g06n)', 'signalling or calling systems; order telegraphs; alarm systems', 'signalling or calling systems; order telegraphs; alarm systems', 'signalling or calling systems; order telegraphs; alarm systems', 'signalling or calling systems; order telegraphs; alarm systems', 'signalling or calling systems; order telegraphs; alarm systems', 'signalling or calling systems; order telegraphs; alarm systems', 'signalling or calling systems; order telegraphs; alarm systems', 'pictorial communication, e.g. television']","real-time video image analysis for providing security System, method, and computer program product are provided for using real-time video analysis to provide the user of mobile devices with security, no matter the user's location. Through the use of real-time vision object recognition objects, logos, artwork, products, locations, and other features that can be recognized in the real-time video stream and frequented locations of the user can be established. In this way, a baseline layout of the frequented location is determined, such that the objects and individuals typically in the frequented area are recognized. The system may continue to take real-time video images of the frequented location, such that if a variation in the baseline layout occurs, the change may be alerted. In this way, a security compromise, such as a break-in to a user's home may be detected by the system and thus the system may notify the user or the appropriate authorities.",electric communication technique signalling computing; calculating; counting
US2020294092_A1,2019-03-12,2020-09-17,2019-03-12,XEVO,"TONG, RICHARD CHIA TSING",72423336,road monitoring,system and method for providing content to a user based on a predicted route identified from audio or images,"Embodiments are directed towards providing a system that presents content to a user of a vehicle based on where the vehicle is going. A microphone captures audio signals within the vehicle, which are analyzed for route information. These audible commands may be said by a person in the vehicle, such as a passenger telling the driver where to turn, or they may be received from a mobile computing device, such as a smartphone executing a map application that is providing audible directions. An anticipated route of the vehicle is determined based on the audible route information. Content is selected and presented to the user of the vehicle based on the anticipated route. Images of a display screen of the mobile computing device may also be analyzed to identify the route information.","1. A system, comprising: a memory configured to store data and computer instructions; an output interface coupled to the memory, the output interface configured to present content to a user of a vehicle; a GPS unit configured to receive information regarding a current location of the vehicle; a microphone configured to capture audio from within the vehicle; a camera configured to capture images of a display screen of a mobile computing device of the user; and a processor configured to execute the computer instructions to: receive audio data via at least one of either the microphone or an image via the camera or both; analyze the received audio data or image or both to identify route information; and in response to identification of the route information: determine, via the GPS unit, a current location of the vehicle; determine an anticipated route of the vehicle based on the current location of the vehicle and the route information; receive content to present to the user of the vehicle based on the current location of the vehicle and the anticipated route of the vehicle; and present the content to the user via the output interface.2. The system of claim 1, wherein the received audio data is received from a voice of the user of the vehicle.3. The system of claim 1, wherein the received audio data is received from a speaker of an electronic device providing audible driving directions to a driver of the vehicle.4. The system of claim 1, wherein the processor is configured to analyze the received image by executing further computer instructions to: perform image recognition on the received image to identify textual route information on the display screen of the mobile computing device; and determine the textual route information from the identified textual route information.5. The system of claim 1, wherein the route information includes a graphical map with a highlighted route.6. The system of claim 1, wherein the route information includes a textual instruction for a driver of the vehicle.7. The system of claim 1, wherein the processor is configured to analyze the received image by executing further computer instructions to: perform image recognition on the received image to identify text on the display screen of the mobile computing device; and compare the identified text with known data sets corresponding to characters; and in response to a match between the identified text and the known data sets, identify the route information.8. The system of claim 1, wherein the processor is configured to execute the computer instructions to further: receive a plurality of location data for the vehicle based on the information regarding the current location of the vehicle received via the GPS unit during a time period; generate a current route of the vehicle for the time period based on the plurality of location data; select a previous route from a plurality of previous routes that includes a first segment that matches the current route and a second segment that matches the anticipated route; select additional content based on the selected previous route without input or interaction from the user; and present the additional content to the user via the output interface.9. The system of claim 1, wherein the processor is configured to select the content by executing further computer instructions to: select an advertisement for a store that is within a select proximity to the route information.10. A method, comprising: receiving indirect audible route information from within a vehicle; receiving an estimated route of the vehicle based at least in part on the indirect audible route information; receiving content to present to a user of the vehicle based on the estimated route of the vehicle; and providing the content to the user.11. The method of claim 10, wherein receiving audible route information includes: receiving the audible route information from a voice of a person within the vehicle.12. The method of claim 10, wherein receiving the audible route information includes: receiving audio signals from an electronic speaker of an electronic device providing audible driving directions to a driver of the vehicle.13. The method of claim 11, further comprising: receiving an image of a display screen of a mobile computing device of a person in the vehicle; analyzing the received image to identify route information on the display screen of the mobile computing device; determining visual route information commands from the route information; and updating the estimated route based on the visual route information.14. The method of claim 11, wherein selecting the content further comprises: identifying a current route of the vehicle; selecting a previous route from a plurality of previous routes that includes a first segment that matches the current route and a second segment that matches the estimated route; and selecting the content based on the selected previous route without input or interaction from the user.",US2020294092_A1.txt,"G01S19/13,G06F3/16,G06K9/00,G06K9/62,G06Q30/02","{'measuring; testing', 'computing; calculating; counting'}","['radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'electric digital data processing (computer systems based on specific computational models g06n)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for']","system and method for providing content to a user based on a predicted route identified from audio or images Embodiments are directed towards providing a system that presents content to a user of a vehicle based on where the vehicle is going. A microphone captures audio signals within the vehicle, which are analyzed for route information. These audible commands may be said by a person in the vehicle, such as a passenger telling the driver where to turn, or they may be received from a mobile computing device, such as a smartphone executing a map application that is providing audible directions. An anticipated route of the vehicle is determined based on the audible route information. Content is selected and presented to the user of the vehicle based on the anticipated route. Images of a display screen of the mobile computing device may also be analyzed to identify the route information.",measuring; testing computing; calculating; counting
US10817729_B2,2018-09-26,2020-10-27,2018-09-26,ALLSTATE INSURANCE COMPANY,"ARAGON, JUAN CARLOSMADIGAN, REGINA",69884892,speed & trajectory,dynamic driving metric output generation using computer vision methods,"Aspects of the disclosure relate to dynamic driving metric output platforms that utilize improved computer vision methods to determine vehicle metrics from video footage. A computing platform may receive video footage from a vehicle camera. The computing platform may determine that a reference marker in the video footage has reached a beginning and an end of a road marker based on brightness transitions, and may insert time stamps into the video accordingly. Based on the time stamps, the computing platform may determine an amount of time during which the reference marker covered the road marking. Based on a known length of the road marking and the amount of time during which the reference marker covered the road marking, the computing platform may determine a vehicle speed. The computing platform may generate driving metric output information, based on the vehicle speed, which may be displayed by an accident analysis platform. Based on known dimensions of pavement markings the computing platform may obtain the parameters of the camera (e.g., focal length, camera height above ground plane and camera tilt angle) used to generate the video footage and use the camera parameters to determine the distance between the camera and any object in the video footage.","1. A computing platform, comprising: at least one processor; a communication interface communicatively coupled to the at least one processor; and memory storing computer-readable instructions that, when executed by the at least one processor, cause the computing platform to: receive video footage from a vehicle camera, wherein the vehicle camera is located inside a vehicle; insert, into the video footage, a reference marker comprising a horizontal line located in a center of a frame of the vehicle camera, wherein the reference marker remains centered in the frame throughout a duration of the video footage; determine that the reference marker in the video footage has reached a beginning of a road marking by determining that a first brightness transition in the video footage exceeds a predetermined threshold; insert, into the video footage and in response to determining that the reference marker has reached the beginning of the road marking, a first time stamp indicating a time at which the reference marker reached the beginning of the road marking; determine that the reference marker has reached an end of the road marking by determining that a second brightness transition in the video footage exceeds the predetermined threshold; insert, into the video footage and in response to determining that the reference marker has reached the end of the road marking, a second time stamp indicating a time at which the reference marker reached the end of the road marking; determine, based on the first time stamp and the second time stamp, an amount of time during which the reference marker covered the road marking; determine, based on a known length of the road marking and the amount of time during which the reference marker covered the road marking, a vehicle speed of the vehicle, wherein determining the vehicle speed comprises diving the known length of the road marking by the amount of time during which the reference marker covered the road marking; generate driving metric output information based on the vehicle speed; generate one or more commands directing an accident analysis platform to generate and cause display of a driving metric interface based on the driving metric output information; establish a first wireless data connection with the accident analysis platform; and send, while the first wireless data connection is established and to the accident analysis platform, the driving metric output information and the one or more commands directing the accident analysis platform to generate and cause display of the driving metric interface based on the driving metric output information, wherein: sending the driving metric output information and the one or more commands directing the accident analysis platform to generate and cause display of the driving metric interface based on the driving metric output information causes the accident analysis platform to display the driving metric interface, and the driving metric interface includes a distance between the vehicle and another vehicle.2. The computing platform of claim 1, wherein the memory stores additional computer-readable instructions that, when executed by the at least one processor, further cause the computing platform to: establish a second wireless data connection with a vehicle camera, wherein the video footage is received while the second wireless data connection is established.3. The computing platform of claim 1, wherein the memory stores additional computer-readable instructions that, when executed by the at least one processor, further cause the computing platform to determine that the video footage contains a road marking associated with a standard length.4. The computing platform of claim 1, wherein the memory stores additional computer-readable instructions that, when executed by the at least one processor, further cause the computing platform to insert, into the video footage, a reference marker, wherein the reference marker corresponds to a fixed position in the video footage.5. The computing platform of claim 1, wherein the memory stores additional computer-readable instructions that, when executed by the at least one processor, further cause the computing platform to convert the video footage to greyscale prior to determining that the reference marker in the video footage has reached the beginning of the road marking.6. The computing platform of claim 1, wherein the memory stores additional computer-readable instructions that, when executed by the at least one processor, further cause the computing platform to determine camera parameters such as focal length, camera height above a ground plane, and camera tilt angle based on information provided by pavement markings with pre-determined dimensions, and to determine based on these camera parameters a distance between the vehicle camera and an object in the video footage.7. The computing platform of claim 6, wherein the memory stores additional computer-readable instructions that, when executed by the at least one processor, further cause the computing platform to: generate one or more commands directing a vehicle attribute database to provide vehicle parameters for a vehicle corresponding to the vehicle camera; establish a third wireless data connection with the vehicle attribute database; and send, while the third wireless data connection is established, the one or more commands directing the vehicle attribute database to provide the vehicle parameters.8. The computing platform of claim 7, wherein the memory stores additional computer-readable instructions that, when executed by the at least one processor, further cause the computing platform to: receive a vehicle parameter output corresponding to the vehicle parameters; and determine, based on the vehicle parameters and the distance between the vehicle camera and an object in the video footage, a distance between the vehicle and the object in the video footage.9. The computing platform of claim 8, wherein the memory stores additional computer-readable instructions that, when executed by the at least one processor, further cause the computing platform to: update the driving metric output information based on the distance between the vehicle and the object in the video footage; generate one or more commands directing the accident analysis platform to generate and cause display of an updated driving metric interface based on the updated driving metric output information; and send, while the first wireless data connection is established and to the accident analysis platform, the updated driving metric output information and the one or more commands directing the accident analysis platform to generate and cause display of the updated driving metric interface based on the updated driving metric output information.10. A method comprising: at a computing platform comprising at least one processor, a communication interface, and memory: receiving, by the at least one processor and via the communication interface, video footage from a vehicle camera, wherein the vehicle camera is located inside a vehicle; inserting, into the video footage, a reference marker comprising a horizontal line located in a center of a frame of the vehicle camera, wherein the reference marker remains centered in the frame throughout a duration of the video footage; determining, by the at least one processor, that the reference marker in the video footage has reached a beginning of a road marking by determining that a first brightness transition in the video footage exceeds a predetermined threshold; inserting, by the at least one processor, into the video footage, and in response to determining that the reference marker has reached the beginning of the road marking, a first time stamp indicating a time at which the reference marker reached the beginning of the road marking; determining, by the at least one processor, that the reference marker has reached an end of the road marking by determining that a second brightness transition in the video footage exceeds the predetermined threshold; inserting, by the at least one processor, into the video footage, and in response to determining that the reference marker has reached the end of the road marking, a second time stamp indicating a time at which the reference marker reached the end of the road marking; determining, by the at least one processor and based on the first time stamp and the second time stamp, an amount of time during which the reference marker covered the road marking; determining, by the at least one processor and based on a known length of the road marking and the amount of time during which the reference marker covered the road marking, a vehicle speed of the vehicle, wherein determining the vehicle speed comprises diving the known length of the road marking by the amount of time during which the reference marker covered the road marking; generating, by the at least one processor, driving metric output information based on the vehicle speed; generating, by the at least one processor, one or more commands directing an accident analysis platform to generate and cause display of a driving metric interface based on the driving metric output information; establishing a first wireless data connection with the accident analysis platform; and sending, by the at least one processor, via the communication interface while the first wireless data connection is established and to the accident analysis platform, the driving metric output information and the one or more commands directing the accident analysis platform to generate and cause display of the driving metric interface based on the driving metric output information, wherein: sending the driving metric output information and the one or more commands directing the accident analysis platform to generate and cause display of the driving metric interface based on the driving metric output information causes the accident analysis platform to display the driving metric interface, and the driving metric interface includes a distance between the vehicle and another vehicle.11. The method of claim 10, further comprising: establishing a second wireless data connection with a vehicle camera, wherein the video footage is received while the second wireless data connection is established.12. The method of claim 10, further comprising: determining, by the at least one processor, that the video footage contains a road marking associated with a standard length.13. The method of claim 10, further comprising: inserting, by the at least one processor and into the video footage, a reference marker, wherein the reference marker corresponds to a fixed position in the video footage.14. The method of claim 10, further comprising: converting, by the at least one processor, the video footage to greyscale prior to determining that the reference marker in the video footage has reached the beginning of the road marking.15. The method of claim 10, further comprising: determining, by the at least one processor, a distance between the vehicle camera and an object in the video footage.16. The method of claim 15, further comprising: generating, by the at least one processor, one or more commands directing a vehicle attribute database to provide vehicle parameters for a vehicle corresponding to the vehicle camera; establishing a third wireless data connection with the vehicle attribute database; and sending, while the third wireless data connection is established, the one or more commands directing the vehicle attribute database to provide the vehicle parameters.17. The method of claim 16, further comprising: receiving a vehicle parameter output corresponding to the vehicle parameters; and determining, based on the vehicle parameters and the distance between the vehicle camera and an object in the video footage, a distance between the vehicle and the object in the video footage.18. The method of claim 17, further comprising: updating the driving metric output information based on the distance between the vehicle and the object in the video footage; generating one or more commands directing the accident analysis platform to generate and cause display of an updated driving metric interface based on the updated driving metric output information; and sending, while the first wireless data connection is established and to the accident analysis platform, the updated driving metric output information and the one or more commands directing the accident analysis platform to generate and cause display of the updated driving metric interface based on the updated driving metric output information.19. One or more non-transitory computer-readable media storing instructions that, when executed by a computing platform comprising at least one processor, a communication interface, and memory, cause the computing platform to: receive video footage from a vehicle camera, wherein the vehicle camera is located inside a vehicle; insert, into the video footage, a reference marker comprising a horizontal line located in a center of a frame of the vehicle camera, wherein the reference marker remains centered in the frame throughout a duration of the video footage; determine that the reference marker in the video footage has reached a beginning of a road marking by determining that a first brightness transition in the video footage exceeds a predetermined threshold; insert, into the video footage and in response to determining that the reference marker has reached the beginning of the road marking, a first time stamp indicating a time at which the reference marker reached the beginning of the road marking; determine that the reference marker has reached an end of the road marking by determining that a second brightness transition in the video footage exceeds the predetermined threshold; insert, into the video footage and in response to determining that the reference marker has reached the end of the road marking, a second time stamp indicating a time at which the reference marker reached the end of the road marking; determine, based on the first time stamp and the second time stamp, an amount of time during which the reference marker covered the road marking; determine, based on a known length of the road marking and the amount of time during which the reference marker covered the road marking, a vehicle speed of the vehicle, wherein determining the vehicle speed comprises diving the known length of the road marking by the amount of time during which the reference marker covered the road marking; generate driving metric output information based on the vehicle speed; generate one or more commands directing an accident analysis platform to generate and cause display of a driving metric interface based on the driving metric output information; establish a first wireless data connection with the accident analysis platform; and send, while the first wireless data connection is established and to the accident analysis platform, the driving metric output information and the one or more commands directing the accident analysis platform to generate and cause display of the driving metric interface based on the driving metric output information, wherein: sending the driving metric output information and the one or more commands directing the accident analysis platform to generate and cause display of the driving metric interface based on the driving metric output information causes the accident analysis platform to display the driving metric interface, and the driving metric interface includes a distance between the vehicle and another vehicle.20. The one or more non-transitory computer-readable media of claim 19, wherein the memory stores additional computer readable media storing instructions, that when executed by the at least one processor, cause the at least one processor to: establish a second wireless data connection with a vehicle camera, wherein the video footage is received while the second wireless data connection is established.21. The computing platform of claim 1, wherein the memory stores additional computer-readable instructions that, when executed by the at least one processor, further cause the computing platform to determine ego-vehicle speed using distances to specific objects in the video footage by obtaining distances to an object on two given frames, by computing a delta of the distances, and by dividing the delta of the distances by a time that elapses between such frames.22. The computing platform of claim 1, wherein the memory stores additional computer-readable instructions that, when executed by the at least one processor, further cause the computing platform to determine speed for a vehicle associated with the vehicle camera and distances to objects in video footage from pre-recorded datasets and video available on the Internet for all of which no camera parameters are available, resulting in estimation of one or more of: speed and acceleration for the vehicle, distance to objects, relative speed of other vehicles, relative and absolute speed of other vehicles, and relative and absolute acceleration of other vehicles, from the video footage.",US10817729_B2.txt,"G06K9/00,G06T7/246,G06T7/50,G06T7/80,G07C5/00,G07C5/08,H04N5/232,H04N5/265","{'electric communication technique', 'computing; calculating; counting', 'checking-devices'}","['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general', 'image data processing or generation, in general', 'image data processing or generation, in general', 'time or attendance registers; registering or indicating the working of machines; generating random numbers; voting or lottery apparatus; arrangements, systems or apparatus for checking not provided for elsewhere', 'time or attendance registers; registering or indicating the working of machines; generating random numbers; voting or lottery apparatus; arrangements, systems or apparatus for checking not provided for elsewhere', 'pictorial communication, e.g. television', 'pictorial communication, e.g. television']","dynamic driving metric output generation using computer vision methods Aspects of the disclosure relate to dynamic driving metric output platforms that utilize improved computer vision methods to determine vehicle metrics from video footage. A computing platform may receive video footage from a vehicle camera. The computing platform may determine that a reference marker in the video footage has reached a beginning and an end of a road marker based on brightness transitions, and may insert time stamps into the video accordingly. Based on the time stamps, the computing platform may determine an amount of time during which the reference marker covered the road marking. Based on a known length of the road marking and the amount of time during which the reference marker covered the road marking, the computing platform may determine a vehicle speed. The computing platform may generate driving metric output information, based on the vehicle speed, which may be displayed by an accident analysis platform. Based on known dimensions of pavement markings the computing platform may obtain the parameters of the camera (e.g., focal length, camera height above ground plane and camera tilt angle) used to generate the video footage and use the camera parameters to determine the distance between the camera and any object in the video footage.",electric communication technique computing; calculating; counting checking-devices
US10551199_B2,2017-12-29,2020-02-04,2017-12-29,LYFT,"MALEK, YUANYUANHAQUE, ASIFMURPHY, JAMES KEVIN",67058130,gps,utilizing artificial neural networks to evaluate routes based on generated route tiles,"This disclosure covers methods, non-transitory computer readable media, and systems that generate route tiles reflecting both GPS locations and map-matched locations for regions along a route traveled by a client device associated with a transportation vehicle. For example, in some implementations, the disclosed systems use an artificial neural network to analyze the route tiles and determine route-accuracy metrics indicating GPS locations or map-matched locations for particular regions along the route. The disclosed systems can then use the route-accuracy metrics to facilitate transport of requestors by, for example, determining a distance of the route or a location of a client device associated with a transportation vehicle.","We claim:1. A system comprising: at least one processor; and at least one non-transitory computer readable storage medium storing instructions that, when executed by the at least one processor, cause the system to: identify a set of GPS locations and a corresponding set of map-matched locations for a route traveled by a client device; segment the set of GPS locations and the corresponding set of map-matched locations into a set of regions along the route, wherein each region from the set of regions comprises a subset of GPS locations and a subset of map-matched locations; analyze a region from the set of regions utilizing a trained artificial neural network to select between a particular subset of GPS locations and a particular subset of map-matched locations for the region; and update the route to include the particular subset of GPS locations or the particular subset of map-matched locations selected for the region.2. The system of claim 1, further comprising instructions that, when executed by the at least one processor, cause the system to generate a route tile for each region from the set of regions based on the subset of GPS locations for each region and the subset of map-matched locations for each region.3. The system of claim 2, further comprising instructions that, when executed by the at least one processor, cause the system to utilize the trained artificial neural network to analyze the route tile for the region to generate a route-accuracy metric indicating the particular subset of GPS locations or the particular subset of map-matched locations for the region.4. The system of claim 3, further comprising instructions that, when executed by the at least one processor, cause the system to generate the route tile for the region by: generating a first image matrix for the region comprising a representation of the particular subset of GPS locations for the region; and generating a second image matrix for the region comprising a representation of the particular subset of map-matched locations for the region.5. The system of claim 4, further comprising instructions that, when executed by the at least one processor, cause the system to generate the route tile for the region by determining a first estimated path of the client device within the region based on the particular subset of GPS locations for the region and a second estimated path of the client device within the region based on the particular subset of map-matched locations for the region, wherein: the first image matrix comprises a first set of pixels that represent the first estimated path of the client device within the region and a second set of pixels that represent positions outside of the first estimated path within the region; and the second image matrix comprises a third set of pixels that represent the second estimated path of the client device within the region and a fourth set of pixels that represent positions outside of the second estimated path within the region.6. The system of claim 1, further comprising instructions that, when executed by the at least one processor, cause the system to update the route by determining a distance of the route based on the particular subset of GPS locations or the particular subset of map-matched locations selected for the region.7. The system of claim 1, further comprising instructions that, when executed by the at least one processor, cause the system to update the route by adjusting a digital map for the region to correspond to the particular subset of GPS locations or the particular subset of map-matched locations selected for the region.8. The system of claim 1, further comprising instructions that, when executed by the at least one processor, cause the system to generate the trained artificial neural network by: generating a training route tile for a training region along a training route based on training GPS locations and corresponding training map-matched locations; utilizing an artificial neural network to predict a route-accuracy metric for the training region based on the training route tile; and generating the trained artificial neural network by comparing the route-accuracy metric to a ground-truth-route-accuracy metric for the training region.9. The system of claim 8, wherein the training GPS locations comprise simulated training GPS locations, the system further comprising instructions that, when executed by the at least one processor, cause the system to generate the simulated training GPS locations by: creating simulated route locations for the training route within a road network based on standard-traveling patterns of transportation vehicles; and generating the simulated training GPS locations by transforming the simulated route locations based on a GPS-noise model.10. The system of claim 9, further comprising instructions that, when executed by the at least one processor, cause the system to generate the ground-truth-route-accuracy metric for the training region by: determining an error value between the simulated training GPS locations and the simulated route locations; and comparing the determined error value to a noise threshold.11. A method comprising: identifying a set of GPS locations and a corresponding set of map-matched locations for a route traveled by a client device; segmenting the set of GPS locations and the corresponding set of map-matched locations into a set of regions along the route, wherein each region from the set of regions comprises a subset of GPS locations and a subset of map-matched locations; analyzing a region from the set of regions utilizing a trained artificial neural network to select between a particular subset of GPS locations or a particular subset of map-matched locations for the region; and updating the route to include the particular subset of GPS locations or the particular subset of map-matched locations selected for the region.12. The method of claim 11, further comprising generating a route tile for each region from the set of regions based on the subset of GPS locations for each region and the subset of map-matched locations for each region.13. The method of claim 12, further comprising utilizing the trained artificial neural network to analyze the route tile for the region to generate a route-accuracy metric indicating the particular subset of GPS locations or the particular subset of map-matched locations for the region.14. The method of claim 12, wherein generating the route tile for the region comprises: generating a first image matrix for the region comprising a representation of the particular subset of GPS locations for the region; and generating a second image matrix for the region comprising a representation of the particular subset of map-matched locations for the region.15. The method of claim 14, wherein: generating the route tile for the region comprises determining a first estimated path of the client device within the region based on the particular subset of GPS locations for the region and a second estimated path of the client device within the region based on the particular subset of map-matched locations for the region; the first image matrix comprises a first set of pixels that represent the first estimated path of the client device within the region and a second set of pixels that represent positions outside of the first estimated path within the region; and the second image matrix comprises a third set of pixels that represent the second estimated path of the client device within the region and a fourth set of pixels that represent positions outside of the second estimated path within the region.16. A non-transitory computer readable medium storing instructions thereon that, when executed by at least one processor, cause a system to: identify a set of GPS locations and a corresponding set of map-matched locations for a route traveled by a client device; segment the set of GPS locations and the corresponding set of map-matched locations into a set of regions along the route, wherein each region from the set of regions comprises a subset of GPS locations and a subset of map-matched locations; analyze a region from the set of regions utilizing a trained artificial neural network to select between a particular subset of GPS locations and a particular subset of map-matched locations for the region; and update the route to include the particular subset of GPS locations or the particular subset of map-matched locations selected for the region.17. The non-transitory computer readable medium of claim 16, further comprising instructions that, when executed by the at least one processor, cause the system to generate a route tile for each region from the set of regions based on the subset of GPS locations for each region and the subset of map-matched locations for each region.18. The non-transitory computer readable medium of claim 17, further comprising instructions that, when executed by the at least one processor, cause the system to utilize the trained artificial neural network to analyze the route tile for the region to generate a route-accuracy metric indicating the particular subset of GPS locations or the particular subset of map-matched locations for the region.19. The non-transitory computer readable medium of claim 17, further comprising instructions that, when executed by the at least one processor, cause the system to generate the route tile for the region by: generating a first image matrix for the region comprising a representation of the particular subset of GPS locations for the region; and generating a second image matrix for the region comprising a representation of the particular subset of map-matched locations for the region.20. The non-transitory computer readable medium of claim 19, further comprising instructions that, when executed by the at least one processor, cause the system to generate the route tile for the region by determining a first estimated path of the client device within the region based on the particular subset of GPS locations for the region and a second estimated path of the client device within the region based on the particular subset of map-matched locations for the region, wherein: the first image matrix comprises a first set of pixels that represent the first estimated path of the client device within the region and a second set of pixels that represent positions outside of the first estimated path within the region; and the second image matrix comprises a third set of pixels that represent the second estimated path of the client device within the region and a fourth set of pixels that represent positions outside of the second estimated path within the region.",US10551199_B2.txt,"G01C21/00,G01C21/30,G06N3/02,G06N5/02","{'measuring; testing', 'computing; calculating; counting'}","['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models']","utilizing artificial neural networks to evaluate routes based on generated route tiles This disclosure covers methods, non-transitory computer readable media, and systems that generate route tiles reflecting both GPS locations and map-matched locations for regions along a route traveled by a client device associated with a transportation vehicle. For example, in some implementations, the disclosed systems use an artificial neural network to analyze the route tiles and determine route-accuracy metrics indicating GPS locations or map-matched locations for particular regions along the route. The disclosed systems can then use the route-accuracy metrics to facilitate transport of requestors by, for example, determining a distance of the route or a location of a client device associated with a transportation vehicle.",measuring; testing computing; calculating; counting
US2020056897_A1,2018-08-14,2020-02-20,2018-08-14,IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION),"ANGLIN, HOWARD N.ZACHARIAS, SHINOJWILLIAMSON, LEIGHSNOOK, ROGEREKAMBARAM, VIJAY",69523126,gps,spatio-temporal re-routing of navigation,"A method, computer program product, and a system where a processor(s) monitors activities of a user operating a vehicle with a navigational device providing route guidance, via a first route, to a geographic destination. The processor(s) identifies driving patterns of the user to generate a driver profile for the user, including a baseline activity pattern. The processor(s) determines, at a given time, that the monitored activities of the user are outside of the baseline activity pattern and enables an integrated image capture device in the vehicle and captures images of an environment outside of a the vehicle. The processor(s) utilizes the image capture device, to capture images and derives data describing environmental conditions, based on performing a semantic analysis of the images. The processor(s) transmits the data to a repository and generates a second route to the destination, based on the driver profile and relevant data from the repository data.","1. A computer-implemented method, comprising: monitoring, by one or more processors, activities of a user operating a vehicle comprising a navigational device, wherein the navigational device is providing route guidance to the user, via a first route, to a geographic destination, and wherein the vehicle is proceeding to the destination; based on the monitoring, identifying, by the one or more processors, driving patterns of the user to generate a driver profile for the user, wherein the driver profile comprises a baseline activity pattern of the user, when the user is operating the vehicle; determining, by the one or more processors, at a given time, that the monitored activities of the user are outside of the baseline activity level; enabling, by the one or more processors, based on determining that the monitored activities of the user are outside of the baseline activity level, an image capture device, wherein the image capture device is an integrated device of the vehicle, and wherein the image capture device is oriented to capture images of an environment outside of a the vehicle; utilizing, by the one or more processors, the image capture device, to capture images of the environment; deriving, by the one or more processors, data describing environmental conditions of the environment, from the images captured by the image capture device, based on performing a semantic analysis of the images captured by the image capture device; transmitting, by the one or more processors, the data to a repository, wherein the repository indexes the data by a location of the vehicle during the capturing, wherein the repository comprises additional data from additional vehicles, the additional data describing additional environmental conditions, the additional data indexed in the repository, by locations of the additional vehicles when the additional data was obtained, and by times when the additional data was obtained, and wherein the data and the additional data comprise repository data; generating, by the one or more processors, a second route to the destination, based on the driver profile and relevant data selected from the repository data, wherein the second route to the destination avoids one or more locations en route to the destination, wherein the relevant data indicates a given environmental condition at the one or more locations, and wherein the driver profile indicates a departure from the baseline activity pattern of the user, when the user experiences the given environmental condition, while operating the vehicle; and facilitating, by the one or more processors, an action in the navigational device related to changing the first route to the second route.2. The computer-implemented method of claim 1, wherein the action is selected from the group consisting of: automatically changing the route guidance to provide revised route guidance to the user, via the second route, through the navigational device, and prompting the user, through an interface of the navigational device, to confirm the changing the route guidance to the revised route guidance.3. The computer-implemented method of claim 2, wherein the action consists of prompting the user to confirm the changing the route guidance to the revised route guidance, the method further comprising: obtaining, by the one or more processors, a response to the prompting; updating, by the one or more processors, the driver profile based on the response.4. The computer-implemented method of claim 3, wherein the action consists of prompting the user to confirm the changing the route guidance to the revised route guidance, wherein the prompting comprises a description of the given environmental condition avoided in the second route.5. The computer-implemented method of claim 1, wherein the monitoring comprises obtaining data describing behaviors of the user, when operating the vehicle, from one or more sources selected from the group consisting of: sensor technologies instrumented in the vehicle, computing resources of the vehicle, and personal computing devices of the user.6. The computer-implemented method of claim 5, wherein the one or more sources comprise a sensor technology of the sensor technologies, wherein the one or more processors obtain a portion of the data describing behaviors of the user from the sensor technology, based on the sensor technology obtaining the data from monitoring a component selected from the group consisting of: brake, accelerator, and gear.7. The computer-implemented method of claim 4, wherein the one or more sources comprise a personal device of the personal devices, wherein the one or more processors obtain a portion of the data describing behaviors of the user from the personal device, based on the personal device comprising at least one component selected from the group consisting of: an accelerometer, and a gyroscope.8. The computer-implemented method of claim 7, further comprising: disabling, by the one or more processors, the image capture device, based on a factor selected from the group consisting of: passage of a pre-determined interval and completion of the deriving.9. The computer-implemented method of claim 1, the generating the second route to the destination further comprising: determining, by the one or more processors, based on accessing the repository, one or more locations to avoid based on the repository data indicating various environmental conditions at the one or more locations to avoid; analyzing, by the one or more processors, the driver profile, to identify the given environmental condition, among the various environmental conditions; identifying, by the one or more processors, the one or more locations en route to the destination from the one or more locations to avoid based; and generating, by the one or more processors, the second route, wherein the second route comprises a shortest route to the destination avoiding the one or more locations en route to the destination.10. The computer implemented method of claim 1, wherein the image capture device is selected from the group consisting of: a headlight camera, a review view camera, a side view camera, and a front view camera.11. The computer implemented method of claim 1, wherein the images of the environment comprise video.12. A computer program product comprising: a computer readable storage medium readable by one or more processors and storing instructions for execution by the one or more processors for performing a method comprising: monitoring, by the one or more processors, activities of a user operating a vehicle comprising a navigational device, wherein the navigational device is providing route guidance to the user, via a first route, to a geographic destination, and wherein the vehicle is proceeding to the destination; based on the monitoring, identifying, by the one or more processors, driving patterns of the user to generate a driver profile for the user, wherein the driver profile comprises a baseline activity pattern of the user, when the user is operating the vehicle; determining, by the one or more processors, at a given time, that the monitored activities of the user are outside of the baseline activity level; enabling, by the one or more processors, based on determining that the monitored activities of the user are outside of the baseline activity level, an image capture device, wherein the image capture device is an integrated device of the vehicle, and wherein the image capture device is oriented to capture images of an environment outside of a the vehicle; utilizing, by the one or more processors, the image capture device, to capture images of the environment; deriving, by the one or more processors, data describing environmental conditions of the environment, from the images captured by the image capture device, based on performing a semantic analysis of the images captured by the image capture device; transmitting, by the one or more processors, the data to a repository, wherein the repository indexes the data by a location of the vehicle during the capturing, wherein the repository comprises additional data from additional vehicles, the additional data describing additional environmental conditions, the additional data indexed in the repository, by locations of the additional vehicles when the additional data was obtained, and by times when the additional data was obtained, and wherein the data and the additional data comprise repository data; generating, by the one or more processors, a second route to the destination, based on the driver profile and relevant data selected from the repository data, wherein the second route to the destination avoids one or more locations en route to the destination, wherein the relevant data indicates a given environmental condition at the one or more locations, and wherein the driver profile indicates a departure from the baseline activity pattern of the user, when the user experiences the given environmental condition, while operating the vehicle; and facilitating, by the one or more processors, an action in the navigational device related to changing the first route to the second route.13. The computer program product of claim 12, wherein the action is selected from the group consisting of: automatically changing the route guidance to provide revised route guidance to the user, via the second route, through the navigational device, and prompting the user, through an interface of the navigational device, to confirm the changing the route guidance to the revised route guidance.14. The computer program product of claim 13, wherein the action consists of prompting the user to confirm the changing the route guidance to the revised route guidance, the method further comprising: obtaining, by the one or more processors, a response to the prompting; updating, by the one or more processors, the driver profile based on the response.15. The computer program product of claim 13, wherein the action consists of prompting the user to confirm the changing the route guidance to the revised route guidance, wherein the prompting comprises a description of the given environmental condition avoided in the second route.16. The computer program product of claim 12, wherein the monitoring comprises obtaining data describing behaviors of the user, when operating the vehicle, from one or more sources selected from the group consisting of: sensor technologies instrumented in the vehicle, computing resources of the vehicle, and personal computing devices of the user.17. The computer program product of claim 16, wherein the one or more sources comprise a sensor technology of the sensor technologies, wherein the one or more processors obtain a portion of the data describing behaviors of the user from the sensor technology, based on the sensor technology obtaining the data from monitoring a component selected from the group consisting of: brake, accelerator, and gear.18. The computer program product of claim 16, wherein the one or more sources comprise a personal device of the personal devices, wherein the one or more processors obtain a portion of the data describing behaviors of the user from the personal device, based on the personal device comprising at least one component selected from the group consisting of: an accelerometer, and a gyroscope.19. A system comprising: a memory; one or more processors in communication with the memory; program instructions executable by the one or more processors via the memory to perform a method, the method comprising: monitoring, by the one or more processors, activities of a user operating a vehicle comprising a navigational device, wherein the navigational device is providing route guidance to the user, via a first route, to a geographic destination, and wherein the vehicle is proceeding to the destination; based on the monitoring, identifying, by the one or more processors, driving patterns of the user to generate a driver profile for the user, wherein the driver profile comprises a baseline activity pattern of the user, when the user is operating the vehicle; determining, by the one or more processors, at a given time, that the monitored activities of the user are outside of the baseline activity level; enabling, by the one or more processors, based on determining that the monitored activities of the user are outside of the baseline activity level, an image capture device, wherein the image capture device is an integrated device of the vehicle, and wherein the image capture device is oriented to capture images of an environment outside of a the vehicle; utilizing, by the one or more processors, the image capture device, to capture images of the environment; deriving, by the one or more processors, data describing environmental conditions of the environment, from the images captured by the image capture device, based on performing a semantic analysis of the images captured by the image capture device; transmitting, by the one or more processors, the data to a repository, wherein the repository indexes the data by a location of the vehicle during the capturing, wherein the repository comprises additional data from additional vehicles, the additional data describing additional environmental conditions, the additional data indexed in the repository, by locations of the additional vehicles when the additional data was obtained, and by times when the additional data was obtained, and wherein the data and the additional data comprise repository data; generating, by the one or more processors, a second route to the destination, based on the driver profile and relevant data selected from the repository data, wherein the second route to the destination avoids one or more locations en route to the destination, wherein the relevant data indicates a given environmental condition at the one or more locations, and wherein the driver profile indicates a departure from the baseline activity pattern of the user, when the user experiences the given environmental condition, while operating the vehicle; and facilitating, by the one or more processors, an action in the navigational device related to changing the first route to the second route.20. The system of claim 19, computer program product, wherein the action is selected from the group consisting of: automatically changing the route guidance to provide revised route guidance to the user, via the second route, through the navigational device, and prompting the user, through an interface of the navigational device, to confirm the changing the route guidance to the revised route guidance",US2020056897_A1.txt,"G01C21/34,G01C21/36,G06K9/00","{'measuring; testing', 'computing; calculating; counting'}","['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']","spatio-temporal re-routing of navigation A method, computer program product, and a system where a processor(s) monitors activities of a user operating a vehicle with a navigational device providing route guidance, via a first route, to a geographic destination. The processor(s) identifies driving patterns of the user to generate a driver profile for the user, including a baseline activity pattern. The processor(s) determines, at a given time, that the monitored activities of the user are outside of the baseline activity pattern and enables an integrated image capture device in the vehicle and captures images of an environment outside of a the vehicle. The processor(s) utilizes the image capture device, to capture images and derives data describing environmental conditions, based on performing a semantic analysis of the images. The processor(s) transmits the data to a repository and generates a second route to the destination, based on the driver profile and relevant data from the repository data.",measuring; testing computing; calculating; counting
US10078791_B2,2015-01-09,2018-09-18,2014-01-09,IRVINE SENSORS CORPORATION,"LUDWIG, DAVIDJUSTICE, JAMESVILLACORTA, VIRGILIOWEAVER, ERICKNUTSON, FREDRIKASADI, OMARYAM, MANNCHUOY",60675040,road monitoring,methods and devices for cognitive-based image data analytics in real time,"An invention is disclosed for a real time video analytic processor that embodies algorithms and processing architectures that process a wide variety of sensor images in a fashion that emulates how the human visual path processes and interprets image content. spatial, temporal, and color content of images are analyzed and the salient features of the images determined. These salient features are then compared to the salient features of objects of user interest in order to detect, track, classify, and characterize the activities of the objects. Objects or activities of interest are annotated in the image streams and alerts of critical events are provided to users. Instantiation of the cognitive processing can be accomplished on multi-FPGA and multi-GPU processing hardwares.","We claim:1. A real time video analytic image processor which emulates the human visual path comprising: an imaging sensor configured for capturing and outputting a sequence of image frames to define a video stream; an edge processor configured to receive the video stream to define an edge processor video stream; a core processor configured to receive the video stream to define a core processor video stream; the edge processor and the core processor comprising a hash synchronization function whereby the image frames are synchronized to the edge processor and the core processor using a unique hash identifier; the edge processor configured to perform a video stream pre-processing function, an analytic function, an analytic metadata output function and a post-processing video compression function to provide an analytic metadata output and a compressed video output; the core processor configured to receive the analytic metadata output and the compressed video output from the edge processor; and; the core processor configured to perform a salient feature extraction function by means of a convolution, an object classification and an event detection from the core processor video stream, the analytic metadata output and the compressed video output based on a correlation between an observed object and an object of interest.2. The image processor of claim 1 comprising a plurality of imaging sensors, each imaging sensor configured for capturing and outputting a sequence of image frames to define a plurality of independent video streams.3. The image processor of claim 1 configured for processing still imagery.4. The image processor of claim 1 configured for processing high definition (HD) video or full motion video (FMV) imagery.5. The image processor of claim 1 configured for processing thermal imagery.6. The image processor of claim 1 configured for processing multispectral imagery.7. The image processor of claim 1 configured for processing hyperspectral imagery.8. The image processor of claim 1 configured for processing LIDAR imagery.9. The image processor of claim 1 configured for processing radar imagery including synthetic aperture array (SAR) and ground moving target indicator (GMTI) imagery.10. The image processor of claim 1 wherein the salient feature extraction, classification and annotation function is performed in real time at the same rate as the sensor is producing image data.",US10078791_B2.txt,"G06K9/00,G06K9/46,G06K9/62,G06K9/64,G06T7/277",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'image data processing or generation, in general']","methods and devices for cognitive-based image data analytics in real time An invention is disclosed for a real time video analytic processor that embodies algorithms and processing architectures that process a wide variety of sensor images in a fashion that emulates how the human visual path processes and interprets image content. spatial, temporal, and color content of images are analyzed and the salient features of the images determined. These salient features are then compared to the salient features of objects of user interest in order to detect, track, classify, and characterize the activities of the objects. Objects or activities of interest are annotated in the image streams and alerts of critical events are provided to users. Instantiation of the cognitive processing can be accomplished on multi-FPGA and multi-GPU processing hardwares.",computing; calculating; counting
US9972206_B2,2015-12-03,2018-05-15,2015-12-03,GM GLOBAL TECHNOLOGY OPERATIONS,"LITKOUHI, BAKHTIAR B.WANG JINSONGZHAO, QINGRONGZHANG, QI",58722924,road monitoring,wet road surface condition detection,"A method for determining a wet surface condition of a road. Capturing an image of a wheel of a remote vehicle traveling in an adjacent lane by an image capture device of a host vehicle. Identifying in the captured image, by processor of a host vehicle, a region of interest relative to the wheel where the region of interest is representative of where precipitation dispersion occurs. A determination is made whether precipitation is present in the region of interest. A wet road surface signal is generated in response to the identification of precipitation in the adjacent lane.","1. A method for determining a wet surface condition of a road with adjacent first and second lanes, the method comprising: capturing an image of a road wheel of a remote vehicle traveling in the first lane by an image capture device of a host vehicle traveling in the second lane; identifying, in the captured image by a processor of the host vehicle, a region of interest relative to the road wheel of the remote vehicle, wherein the region of interest has a location, a width, and a height, the width and height determined as a function of a tire radius of a tire of the road wheel, and the location determined as a function of the tire radius and a rim radius of a rim of the road wheel, wherein the tire radius and the rim radius are determined by: determining a location of the road wheel in the captured image; applying an edge detection technique to the road wheel within the captured image; applying an image transform analysis to identify respective positions of one or more shapes associated with the road wheel in the captured image; identifying contours of the tire and the rim in the captured image; identifying a center of the road wheel; and determining the tire radius and the rim radius each as a function of a respective distance from the center of the road wheel to the identified contours of the tire or the rim; determining whether or not precipitation is present in the region of interest; and generating a wet road surface signal in response to a determination that precipitation is present in the region of interest.2. The method of claim 1, wherein the region of interest is located in a region where precipitation is dispersed by the road wheel of the remote vehicle traveling in the first lane.3. The method of claim 1, wherein the region of interest is a rectangular region, the width is substantially equal to the tire radius, and the height is substantially equal to one half of the tire radius.4. The method of claim 1, wherein the region of interest extends in a direction rearward of the center of the road wheel, wherein a corner of the region of interest is located at a coordinate relative to the center of the road wheel, the coordinate being located a lateral length from the center of the road wheel substantially equal to the rim radius of the rim of the road wheel and a longitudinal length from the center of the road wheel substantially equal to the tire radius of the tire.5. The method of claim 1, wherein the edge detection technique detects horizontal, vertical, and diagonal edges of the road wheel within the captured image.6. The method of claim 1, wherein the image transform analysis includes a Hough transformation analysis applied to the edge-detected image to identify lines and positions of shapes of the road wheel.7. A method for determining a wet surface condition of a road with adjacent first and second lanes, the method comprising: capturing an image of a road wheel of a remote vehicle traveling in the first lane by an image capture device of a host vehicle traveling in the second lane; identifying, in the captured image by a processor of the host vehicle, a region of interest relative to the road wheel of the remote vehicle, the region of interest having a location determined as a function of a location of the road wheel, wherein determining the location of the road wheel in the captured image includes applying a wheel zone localization, wherein application of the wheel zone localization comprises: determining a real-world wheel center position of the road wheel based on signals generated by a sensing-based device of the host vehicle; mapping the real-world center position of the road wheel to a wheel center position of the road wheel in the captured image; regenerating the captured image so that an optical axis of the captured image is perpendicular to a plane of a face of the road wheel; identifying a diameter of a tire of the road wheel in the regenerated image; and generating a localized wheel zone, the localized wheel zone being sized as a function of the diameter of the tire; determining whether or not precipitation is present in the region of interest; and generating a wet road surface signal in response to a determination that precipitation is present in the region of interest.8. The method of claim 7, wherein regenerating the captured image so that the optical axis of the captured image is perpendicular to the plane of the face of the road wheel comprises regenerating the captured image until the road wheel in the captured image displays a shape of a circle.9. The method of claim 8, wherein regenerating the captured image until the road wheel in the captured image displays the shape of a circle includes generating a synthetic image as taken from a virtual camera having an optical axis that is perpendicular to a planar face of the road wheel.10. The method of claim 9, wherein generating the synthetic image comprises: identifying a plurality of pixels in the captured image; determining a respective correlation between each of the pixels in the captured image and a respective one of a plurality of pixels in the virtual image as viewed by the optical axis perpendicular to the planar face of the road wheel; and mapping each of the pixels in the captured image to the correlated respective one of the pixels in the virtual image.11. The method of claim 7, wherein the sizing the localized wheel zone as a function of the diameter of the tire includes sizing the localized wheel zone as a square, wherein each side of the square is sized to a predetermined value times the diameter of the tire, wherein the square is centered at the wheel center position of the wheel.12. A method for determining a wet surface condition of a road with first and second lanes, the method comprising: capturing an image of a road wheel of a remote vehicle traveling in the first lane by an image capture device of a host vehicle traveling in the second lane; identifying, in the captured image by a processor of the host vehicle, a region of interest relative to the road wheel of the remote vehicle; determining whether or not precipitation is present in the region of interest in the captured image, including: analyzing the region of interest as a gray-level image; applying a filter to the captured image to identify noise in the gray-level image, the noise representing precipitation in the region of interest; and determining whether or not precipitation is present in the region of interest based on a non-uniformity of color in the filtered image; and generating a wet road surface signal in response to a determination that precipitation is present in the region of interest.13. The method of claim 12, wherein the filter includes a median filter that generates a filtered image of the captured image that includes no noise, and wherein a noise image is generated as function of a difference between an unfiltered image of the captured image and the filtered image.14. The method of claim 12, further comprising applying a binary conversion to the filtered image to determine whether or not precipitation is present, the binary conversion converting precipitation to a white color.15. The method of claim 1, wherein the wet road surface signal includes an alert to a driver of the host vehicle of a potential hydroplaning.16. The method of claim 1, wherein the wet road surface signal includes an alert to a driver of the host vehicle of a potential reduced traction between one or more vehicle tires of the host vehicle and the road surface.17. The method of claim 1, wherein the host vehicle includes a vehicle braking system and a vehicle controller, the method further comprising transmitting the wet road surface signal to the vehicle controller, the vehicle controller autonomously actuating the vehicle braking system in response to the wet road surface signal.18. The method of claim 1, wherein the wet road surface signal includes an alert to a driver of the host vehicle to reduce a vehicle speed.19. The method of claim 1, wherein the host vehicle includes a traction control system and a vehicle controller, the method further comprising transmitting the wet road surface signal to the vehicle controller, the vehicle controller autonomously actuating the traction control system in response to the wet road surface signal.20. The method of claim 1, further comprising transmitting the wet road surface signal to a wireless communication system with an indicator to alert other vehicles of the wet surface condition.21. The method of claim 1, wherein the wet road surface signal includes an alert to a driver of the host vehicle against a use of cruise control.22. The method of claim 1, wherein the host vehicle includes a cruise control system and a vehicle controller, the method further comprising transmitting the wet road surface signal to the vehicle controller, the vehicle controller autonomously disabling the cruise control system in response to the wet road surface signal.23. The method of claim 1, further comprising transmitting the wet road surface signal to a vehicle controller of the host vehicle, the vehicle controller autonomously shutting baffles on an air intake scoop of the host vehicle in response to the wet road surface signal.24. The method of claim 1 wherein the wet road surface signal includes an alert to a driver of the host vehicle against a use of automated vehicle features.",US9972206_B2.txt,"B60K31/16,B60R16/023,B60T8/171,B60T8/172,B60T8/175,G01S13/95,G06K9/00,G08G1/04,G08G1/0962,G08G1/0965,G08G1/0967","{'measuring; testing', 'signalling', 'computing; calculating; counting', 'vehicles in general'}","['arrangement or mounting of propulsion units or of transmissions in vehicles; arrangement or mounting of plural diverse prime-movers in vehicles; auxiliary drives for vehicles; instrumentation or dashboards for vehicles; arrangements in connection with cooling, air intake, gas exhaust or fuel supply of propulsion units in vehicles', 'vehicles, vehicle fittings, or vehicle parts, not otherwise provided for', 'vehicle brake control systems or parts thereof; brake control systems or parts thereof, in general (electrodynamic brake systems for vehicle, in general b60l; brakes per se, i.e. devices where braking effect occurs, including ultimate brake actuators, f16d); arrangement of braking elements on vehicles in general; portable devices for preventing unwanted movement of vehicles; vehicle modifications to facilitate cooling of brakes', 'vehicle brake control systems or parts thereof; brake control systems or parts thereof, in general (electrodynamic brake systems for vehicle, in general b60l; brakes per se, i.e. devices where braking effect occurs, including ultimate brake actuators, f16d); arrangement of braking elements on vehicles in general; portable devices for preventing unwanted movement of vehicles; vehicle modifications to facilitate cooling of brakes', 'vehicle brake control systems or parts thereof; brake control systems or parts thereof, in general (electrodynamic brake systems for vehicle, in general b60l; brakes per se, i.e. devices where braking effect occurs, including ultimate brake actuators, f16d); arrangement of braking elements on vehicles in general; portable devices for preventing unwanted movement of vehicles; vehicle modifications to facilitate cooling of brakes', 'radio direction-finding; radio navigation; determining distance or velocity by use of radio waves; locating or presence-detecting by use of the reflection or reradiation of radio waves; analogous arrangements using other waves', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})', 'traffic control systems (guiding railway traffic, ensuring the safety of railway traffic b61l; arrangement of road signs or traffic signals e01f9/00; radar or analogous systems, sonar systems, lidar systems specially adapted for traffic control g01s13/91, g01s15/88, g01s17/88; {radar or analogous systems, sonar systems, lidar systems specially adapted for anti-collision purposes g01s13/93, g01s15/93, g01s17/93})']","wet road surface condition detection A method for determining a wet surface condition of a road. Capturing an image of a wheel of a remote vehicle traveling in an adjacent lane by an image capture device of a host vehicle. Identifying in the captured image, by processor of a host vehicle, a region of interest relative to the wheel where the region of interest is representative of where precipitation dispersion occurs. A determination is made whether precipitation is present in the region of interest. A wet road surface signal is generated in response to the identification of precipitation in the adjacent lane.",measuring; testing signalling computing; calculating; counting vehicles in general
EP2107504_A1,2008-03-31,2009-10-07,2008-03-31,HARMAN BECKER AUTOMOTIVE SYSTEMS,"SCHAUFLER, ROLAND",40227891,road monitoring,method and device for generating a real time environment model for vehicles,"The present invention provides a vehicle environment monitoring device and method that is based on transmitting a three-dimensional vector model generated at a vehicle to an external location. The three-dimensional vector model of the vehicle's environment is generated on the basis of the image data captured by at least a three-dimensional camera. Out of the image data, particular data are extracted for generating the three-dimensional vector model in order to reduce the data volume and thus the bandwidth requirements for transmission. Possible applications of the transmitted vector model include, but are not limited to driver assistance, external monitoring and vehicle control, as well as overall traffic monitoring and control. Preferably, a sequence of three-dimensional vector models, representing a three-dimensional space-and-time model, is generated and transmitted.","1. A method of monitoring the environment of a camera-equipped vehicle (1201), comprising the steps of: capturing (S500) a 3D-image of the environment of the vehicle (1201), the 3D-image representing a predetermined area of the vehicle environment, reducing the amount of information acquired in said image capturing step by extracting (S520) data from said 3D-image employing a data extraction algorithm; generating (S530) a 3D-vector model (403) of the vehicle environment from said data extracted from said 3D-image, and transmitting (S621) the 3D-vector model (403) to an external location with respect to said vehicle (1201).2. A method according to claim 1, wherein said 3D-vector model (403) is transmitted by radio.3. A method according to claim 1, wherein said 3D-vector model (403) is transmitted via a cellular telephone line.4. A method according to any of claims 1 to 3, wherein said 3D-vector model (403) being transmitted for visualizing on an external computer.5. A method according to any of claims 1 to 4, further comprising the step of evaluating (S623) said 3D-vector model (403) at an external computer.6. A method according to claim 5, further comprising the step of retransmitting (S625) results of said evaluation (S623) back to the vehicle (1201).7. A method according to claim 5 or 6, further comprising the step of remotely controlling (S627) the vehicle (1201) by employing the evaluation results.8. A method according to any of claims 1 to 7, wherein said external location being a stationary location.9. A method according to any of claims 1 to 8, wherein said transmitting step (S621) transmitting a sequence of 3D-vector models (403) representing a 3D-space-and-time model.10. A method according to any of claims 1 to 9, further comprising the step of determining (S510) the data extraction algorithm to be employed in said reducing step based on at least one parameter characterizing the vehicle situation.11. A method according to claim 10, wherein said extraction algorithm determining step (S510) determining the data extraction algorithm to be adapted to the processing time available for generating the 3D-vector model (403).12. A method according to claim 10 or 11, wherein said extraction algorithm determining step (S510) determining the data extraction algorithm to be adapted to the required information content of the 3D-vector model (403).13. A method according to any of claims 1 to 12, wherein said data extraction algorithm including object recognition.14. A method according to any of claims 1 to 13, wherein said data extraction algorithm including a step of filtering of the captured 3D-image.15. A method according to any of claims 1 to 14, wherein said data extraction algorithm extracts data only from a part of the 3D-image corresponding to a certain visual field.16. A method according to any of claims 1 to 15, further comprising the step of further processing (S601, S623) said 3D-vector model (403) for assisting (S603, S627) a driver in controlling the vehicle (1201) based on the processing result.17. A method according to claim 16, wherein said extraction algorithm determining step (S510) includes the step of determining a maximum available amount of time for performing said steps (S530, S601) of generating and further processing said 3D-vector model (403) based on the vehicle situation.18. A method according to claim 17, wherein said extraction algorithm determining step (S510) further taking into account said maximum available amount of time for determining said data extraction algorithm.19. A method according to claim 18, wherein said extraction algorithm determining step (S51 0) including the step of determining a maximum amount of data to be extracted in correspondence with said maximum amount of time.20. A method according to claim 19, wherein said extraction algorithm determining step (S510) further including the step of determining priorities for different kinds of data that can be extracted from said 3D-image based on the particular relevance of different kinds of data in the vehicle situation characterized by said at least one parameter.21. A method according to any of claims 10 to 20, wherein said extraction algorithm determining step (S510) further including the step of analyzing the vehicle situation based on the captured 3D-image, wherein a value of said at least one parameter is obtained from said analyzing step.22. A method according to any of claims 10 to 21,further comprising the step (S507) of detecting vehicle status information,wherein the detected vehicle status information including a value of said at least one parameter.23. A method according to claim 22, whereinsaid vehicle status information including velocity information of said vehicle (1201), andthe size of the visual field from which said data extraction algorithm extracts data is the smaller the higher the velocity of the vehicle (1201) is.24. A method according to any of claims 10 to 23, further comprising the step of receiving (S505) a value of said at least one parameter that has been input manually.25. A method according to any of claims 10 to 24, wherein said extraction algorithm determining step (S51 0) including defining variables of an algorithm.26. A method according to any of claims 10 to 25, wherein said extraction algorithm determining step (S510) including the step of selecting a particular data extraction algorithm out of a plurality of data extraction algorithms.27. A method according to any of claims 1 to 26, wherein said 3D-vector model (403) represents objects in the environment of the vehicle (1201) by vectors and object classification information, said object classification information associating an object with standard object classes of an object model (801).28. A method according to claim 27, wherein said object model (801) being pre-stored in a database (411).29. A method according to claim 27 or 28, wherein said extraction algorithm including the step of obtaining object classification information from the 3D-image.30. A method according to any of claims 1 to 19, further comprising the step ofgenerating a 3D-space-and-time model, whereinsaid 3D-space-and-time model comprising a sequence of 3D-vector models (403).31. A method according to claim 30, wherein said sequence of 3D-vector models (403) employing a co-ordinate system fixed to the vehicle (1201) as a reference system.32. A method according to claim 30 or 31, further comprising the step of generating (S601) a response (413) to the information of the vehicle environment represented by said 3D-space-and-time model.33. A method according to claim 32, wherein said response (413) being directed towards avoiding an emergency situation such as a threatening collision or a loss of the driver's control over the vehicle (1201).34. A method according to claim 33, wherein said response (413) comprising notification (S907) of an indication of an emergency situation to be expected.35. A method according to claim 33, wherein said response (413) comprising notification (S909) of an advice for assisting a driver to avoid an emergency situation.36. A method according to claim 33, wherein said response (413) comprising an automatic interference (S911) with the vehicle control.37. A method according to claim 32, wherein said response (413) being directed towards assisting a driver in parking the vehicle (1201).38. A method according to claim 32, wherein said response (413) comprising taking countermeasures to minimize the consequences of a threatening accident.39. A method according to claim 38, wherein said countermeasures comprising status control of vehicle safety equipment, such as an air bag or a seat belt.40. A method according to any of claims 1 to 39, wherein said image capturing step (S500) capturing a plurality of 3D-images from plural camera positions.41. A method according to claim 40, wherein said plurality of 3D-images representing an environment area including a predetermined range around the vehicle (1201).42. A method according to any of claims 1 to 41, wherein said 3D-vector model generating step (S530) further employing data from sensors (1202b, 1202c) other than cameras (1202a).43. A method according to claim 42, further comprising the step of sensor fusion (S803) to transform sensor data (s1, s2, s3, sn), including extracted image data into a unique format, and to remove inconsistencies and redundancy.44. A method according to claim 42 or 43, wherein said other sensors (1202b, 1202c) including at least one of a motion sensor, an accelerometer, a temperature sensor, a radar sensor, and an infrared sensor.45. A method according to any of claims 1 to 44, further comprising the step of saving (S611) the 3D-vector model (403) in an Event Data Recorder (405).46. A method according to claim 45, further comprising the steps ofgenerating a sequence of 3D-vector models (403) during a predetermined period of time, andsaving (S611) said sequence of 3D-vector models (403) into the Event Data Recorder (405).47. A method according to claim 46, further comprising the step of associating the 3D-vector models (403) of said sequence before saving with time stamps (1003).48. A method according to claim 47, wherein the 3D-vector models (403) are saved for a predetermined storing time (tn-t0),the method further comprising the step of detecting an impact caused by an extraordinary event, such as an accident, wherein the 3D-vector models (403) remain permanently saved upon said detection of an impact.49. A method according to claim 48,wherein said 3D-vector models are stored in a cyclic storage (1000) having a predetermined storage capacity (1005),further comprising the step of cyclically overwriting the stored 3D-vector model (403) having the earliest time stamp (1003), until said detection of an impact.50. An environment monitoring device for monitoring the environment of a vehicle (1201), comprising: a 3D-camera (1202a) for capturing an image of the environment of the vehicle (1201), the image representing a predetermined area of the vehicle environment; and first processing means (301) for processing the information acquired by said 3D-camera (1202a), said first processing means (301) including: extracting means (503) for extracting data from said image by employing a data extraction algorithm; and model generating means (505) for generating a 3D-vector model (403) of the vehicle environment from said data extracted from said image; wherein said environment monitoring device further comprising a transmitter (1203) for transmitting the 3D-vector model (403) to an external location with respect to said vehicle (1201).51. An environment monitoring device according to claim 50, wherein said transmitter (1203) transmitting said 3D-vector model (403) by radio.52. An environment monitoring device according to claim 50, wherein said transmitter (1203) transmitting said 3D-vector model (403) via a cellular telephone line.53. An environment monitoring device according to any of claims 50 to 52, wherein said transmitter (1203) transmitting said 3D-vector model (403) for visualizing on an external computer.54. An environment monitoring device according to any of claims 50 to 53, wherein said transmitter (1203) transmitting said 3D-vector model (403) for evaluating at an external computer.55. An environment monitoring device according to claim 54, further comprising receiving means (1203) for receiving results of said evaluation transmitted back to the vehicle (1201).56. An environment monitoring device according to claim 54 or 55, further comprising means for controlling the vehicle by employing the evaluation results received from remote.57. An environment monitoring device according to any of claims 50 to 56, wherein said external location being a stationary location.58. An environment monitoring device according to any of claims 50 to 57, wherein said transmitter (1203) transmitting a sequence of 3D-vector models (403) representing a 3D-space-and-time model.59. An environment monitoring device according to any of claims 50 to 58, wherein said first processing means (301) further including determining means (501) for determining the data extraction algorithm to be employed by said extracting means (503) based on at least one parameter characterizing the vehicle situation.60. An environment monitoring device according to claim 59, wherein said determining means (501) determining the data extraction algorithm to be adapted to the processing time available for generating the 3D-vector model (403).61. An environment monitoring device according to claim 59 or 60, wherein said determining means (501) determining the data extraction algorithm to be adapted to the required information content of the 3D-vector model (403).62. An environment monitoring device according to any of claims 50 to 61, wherein said data extraction algorithm including object recognition.63. An environment monitoring device according to any of claims 50 to 62, wherein said data extraction algorithm including a step of filtering of the captured image.64. An environment monitoring device according to any of claims 50 to 64, wherein said data extraction algorithm extracts data only from a part of the image corresponding to a certain visual field.65. An environment monitoring device according to any of claims 50 to 55, further comprising second processing means (303) for further processing said 3D-vector model (403) for assisting a driver in controlling the vehicle (1201) based on the processing result.66. An environment monitoring device according to claim 65, wherein said determining means (501) determining a maximum available amount of time for generating and further processing said 3D-vector model (403) based on the vehicle situation.67. An environment monitoring device according to claim 66, wherein said determining means (501) further taking into account said maximum available amount of time for determining said data extraction algorithm.68. An environment monitoring device according to claim 67, wherein said determining means (501) determining a maximum amount of data to be extracted in correspondence with said maximum amount of time.69. An environment monitoring device according to claim 68, wherein said determining means (501) further determining priorities for different kinds of data that can be extracted from said image based on the particular relevance of different kinds of data in the vehicle situation characterized by said at least one parameter.70. An environment monitoring device according to any of claims 59 to 69, wherein said determining means (501) further analyzing the vehicle situation based on the captured image for obtaining a value of said at least one parameter.71. An environment monitoring device according to any of claims 59 to 62,further comprising a detector (415) for detecting vehicle status information,wherein the detected vehicle status information including a value of said at least one parameter.72. An environment monitoring device according to claim 71, whereinsaid vehicle status information including velocity information of said vehicle (1201), andthe size of the visual field from which said data extraction algorithm extracts data is the smaller the higher the velocity of the vehicle (1201) is.73. An environment monitoring device according to any of claims 59 to 72, further comprising an input interface (401) for receiving a value of said at least one parameter.74. An environment monitoring device according to any of claims 59 to 73, wherein said determining means (501) defining variables of an algorithm.75. An environment monitoring device according to any of claims 59 to 74, wherein said determining means (501) selecting a particular data extraction algorithm out of a plurality of data extraction algorithms.76. An environment monitoring device according to any of claims 50 to 75, wherein said 3D-vector model (403) represents objects in the environment of the vehicle (1201) by vectors and object classification information, said object classification information associating an object with standard object classes of an object model (801).77. An environment monitoring device according to claim 76, further comprising a database (411) for pre-storing said object model (801).78. An environment monitoring device according to claim 76 or 77, wherein said extraction algorithm obtaining object classification information from the image.79. An environment monitoring device according to any of claims 50 to 78, wherein said model generating means generating a 3D-space-and-time model, said 3D-space-and-time model comprising a sequence of 3D-vector models (403).80. An environment monitoring device according to claim 79, wherein said sequence of 3D-vector models (403) employing a co-ordinate system fixed to the vehicle (1201) as a reference system.81. An environment monitoring device according to claim 79 or 80, further comprising second processing means (303) for generating a response (413) to the information of the vehicle environment represented by said 3D-space-and-time model.82. An environment monitoring device according to claim 81, wherein said response (413) being directed towards avoiding an emergency situation such as a threatening collision or a loss of the driver's control over the vehicle (1201).83. An environment monitoring device according to claim 82, further comprising notifying means for notifying an emergency situation to be expected based on the response (413) generated by said second processing means (303).84. An environment monitoring device according to claim 82, further comprising notifying means for notifying an advice for assisting a driver to avoid an emergency situation based on the response (413) generated by said second processing means (303).85. An environment monitoring device according to claim 82, further comprising a controller for automatically interfering with the vehicle control based on the response (413) generated by said second processing means (303).86. An environment monitoring device according to claim 81, wherein said response (413) being directed towards assisting a driver in parking the vehicle (1201).87. An environment monitoring device according to claim 81, further comprising a controller for taking countermeasures to minimize the consequences of a threatening accident based on the response (413) generated by said second processing means (303).88. An environment monitoring device according to claim 87, wherein said controller performing status control of vehicle safety equipment, such as an air bag or a seat belt.89. An environment monitoring device according to any of claims 50 to 88, comprising a plurality of 3D cameras (1202a) for capturing images from plural positions.90. An environment monitoring device according to claim 89, wherein the images captured by said plurality of 3D-cameras (1202a) representing an environment area including a predetermined range around the vehicle (1201).91. An environment monitoring device according to any of claims 50 to 90,further comprising sensors (1202b, 1202c) other than 3D-cameras,wherein said model generating means (505) further employing data from said other sensors (1202b, 1202c) for generating said 3D-vector model (403).92. An environment monitoring device according to claim 91, further comprising sensor fusion means (407) for transforming sensor data (s1, s2, s3, sn,), including extracted image data into a unique format, and for removing inconsistencies and redundancy.93. An environment monitoring device according to claim 91 or 92, wherein said other sensors (1202b, 1202c) including at least one of a motion sensor, an accelerometer, a temperature sensor, a radar sensor, and an infrared sensor.94. An environment monitoring device according to any of claims 50 to 93, further comprising history recording means (409) for saving the 3D-vector model (403) in an Event Data Recorder (405).95. An environment monitoring device according to claim 94, whereinsaid model generating means (505) generating a sequence of 3D-vector models (403) during a predetermined period of time, andsaid history recording means (409) saving said sequence of 3D-vector models (403) into the Event Data Recorder (405).96. An environment monitoring device according to claim 95, wherein said history recording means (409) further associating the 3D-vector models (403) of said sequence before saving with time stamps (1003).97. An environment monitoring device according to claim 96, wherein the 3D-vector models are saved in the Event Data Recorder (405) for a predetermined storing time (tn-t0), andsaid history recording means (409) further comprising an impact detector for detecting an impact caused by an extraordinary event, such as an accident, and the 3D-vector models (403) remain permanently saved upon an impact is detected by said impact detector.98. An environment monitoring device according to claim 96 or 97,wherein said 3D-vector models (403) are stored in a cyclic storage (1000) having a predetermined storage capacity (1005), andsaid history recording means (409) cyclically overwriting the stored 3D-vector model (403) having the earliest time stamp (1003), until an impact is detected by said impact detector.",EP2107504_A1.txt,"G05D1/02,G06K9/00","{'controlling; regulating', 'computing; calculating; counting'}","['systems for controlling or regulating non-electric variables (for continuous casting of metals b22d11/16; valves per se f16k; sensing non-electric variables, see the relevant subclasses of g01; for regulating electric or magnetic variables g05f)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']","method and device for generating a real time environment model for vehicles The present invention provides a vehicle environment monitoring device and method that is based on transmitting a three-dimensional vector model generated at a vehicle to an external location. The three-dimensional vector model of the vehicle's environment is generated on the basis of the image data captured by at least a three-dimensional camera. Out of the image data, particular data are extracted for generating the three-dimensional vector model in order to reduce the data volume and thus the bandwidth requirements for transmission. Possible applications of the transmitted vector model include, but are not limited to driver assistance, external monitoring and vehicle control, as well as overall traffic monitoring and control. Preferably, a sequence of three-dimensional vector models, representing a three-dimensional space-and-time model, is generated and transmitted.",controlling; regulating computing; calculating; counting
US2019257666_A1,2019-02-15,2019-08-22,2018-02-17,ITERIS,"WHITING, MICHAEL, T.KRETER, TODD W.",67617674,road monitoring,augmented reality system for visualization of traffic information in a transportation environment,"An augmented reality visualization framework for precision traffic analysis combines traffic data with content representative of physical-world characteristics, such as conditions in an intersection or roadway in which traffic activity occurs, and images of the intersection and roadway itself. The framework includes a visualization platform that enables display of the combined traffic data and content representing physical-world characteristics, and continual augmentations of such information in response to adjustments directed by users or automatically detected from actions such as user movement, other user manipulation, or movement of the visualization platform itself. The framework enables traffic applications such that a user can, when the platform is interfaced with intersection or roadway equipment, adjust machine activity or signal timing based upon the visualizations or data presented via the visualization platform. The visualization platform may also be configured to present the traffic data, and the content representative of physical-world characteristics, in a virtual reality setting.","1. A method, comprising: capturing input data representing a physical-world characteristic relative to a transportation environment that includes at least one of an intersection, a roadway, and conditions in the transportation environment where traffic activity occurs, the input data including at least one of a video image, a video stream, or a map of the physical-world characteristic; rendering the input data on a visualization platform for display of the at least one of a video image, a video stream, or a map of the physical-world characteristic; detecting a geographical position, and one or more of a movement, tilt or a change in direction of the visualization platform; requesting traffic data for the physical-world characteristic based on the geographical position and the one or more of a movement, tilt or a change in direction of the visualization platform; blending the input data and traffic data by overlaying the traffic data onto the at least one of a video image, a video stream, or a map of the physical-world characteristic to augment the input data with the traffic data on the visualization platform; and continually augmenting the input data by modifying the traffic data based on additional input generated by user manipulation of the visualization platform, to generate an augmented reality visualization of the physical-world characteristic; and initiating one or more responsive actions from the augmented reality visualization of the physical-world characteristic, the one or more responsive actions including at least one of modification of traffic data detection, and an adjustment of a traffic signal timing.2. The method of claim 1, wherein the visualization platform is a display apparatus worn by a user.3. The method of claim 1, wherein the one or more of a movement, tilt or a change in direction of the visualization platform results from the user manipulation of the visualization platform.4. The method of claim 1, wherein the user manipulation of the visualization platform includes one or more of tactile, verbal, and orientational manipulation.5. The method of claim 5, wherein the one or more responsive actions from the continual augmented reality visualization of the physical world characteristic are initiated by the user manipulation of the visualization platform.6. The method of claim 1, wherein the traffic data includes one or more of traffic activity information, an identification of one or more vulnerable road users, environmental or situational areas of interest, points of interest in the physical world characteristic, scouting reports, objects located in the transportation environment, sensor data, satellite data, vehicle data that includes one or more of a classification of a vehicle and a speed of a vehicle, equipment located within the transportation environment, and tracks of past movement of equipment in the transportation environment.7. The method of claim 1, wherein the visualization platform is a display apparatus that includes at least one of a mobile device, a wearable device, a display inside a piece of traffic equipment, a holo-glass image displayed inside a piece of traffic equipment, a heads-up display, and a three-dimension projection.8. The method of claim 1, wherein the input data is further augmented on the visualization platform with one or more of flags, markers, icons, floating images, or colorized three-dimensional overlays.9. The method of claim 1, wherein the conditions in the transportation environment where traffic activity occurs include one or more of vehicle-related conditions, pavement-related conditions, and weather-related conditions.10. The method of claim 1, wherein the continually augmenting the input data further comprises creating a virtual reality visualization of the physical-world characteristic with which the user can interact using the visualization platform, wherein the augmented reality visualization is blended with the virtual reality visualization.11. The method of claim 1, further comprising generating a multi-period traffic performance map for the transportation environment encompassed by the physical-world characteristic.12. A method, comprising: ingesting at least one of a video image, a video stream, or a map as input data representing a physical-world characteristic relative to a transportation environment that includes at least one of an intersection, a roadway, and conditions in the transportation environment where traffic activity occurs; analyzing the input data in a plurality of data processing modules within a computing environment in which the plurality of data processing modules are executed in conjunction with at least one specifically-configured processor, the data processing modules configured to generate an augmented reality visualization of the physical-world characteristic, by configuring the input data on a visualization platform for display of the at least one a video image, a video stream, or a map; determining a geographical position of the visualization platform; identifying a manipulation of the visualization platform from one or more of a movement, tilt or a change in direction; requesting traffic data for the physical-world characteristic based on the geographical position and the manipulation of the visualization platform; overlaying the traffic data onto the at least one a video image, a video stream, or a map of the physical-world characteristic to augment the input data with the traffic data on the visualization platform; and continually augmenting the input data by modifying the traffic data based on additional input generated by the manipulation of the visualization platform; and generating, as output data, instructions for one or more responsive actions from the augmented reality visualization of the physical-world characteristic, the one or more responsive actions including at least one of modification of traffic data detection, and an adjustment of a traffic signal timing.13. The method of claim 12, wherein the visualization platform is a display apparatus worn by a user.14. The method of claim 12, wherein the manipulation of the visualization platform includes one or more of tactile, verbal, and orientational manipulation.15. The method of claim 14, wherein the one or more responsive actions from the continual augmented reality visualization of the physical-world characteristic are initiated by the manipulation of the visualization platform.16. The method of claim 12, wherein the traffic data includes one or more of traffic activity information, an identification of one or more vulnerable road users, environmental or situational areas of interest, points of interest in the physical world characteristic, scouting reports, objects located in the transportation environment, sensor data, satellite data, vehicle data that includes one or more of a classification of a vehicle and a speed of a vehicle, equipment located within the transportation environment, and tracks of past movement of equipment in the transportation environment.17. The method of claim 12, wherein the visualization platform is a display apparatus that includes at least one of a mobile device, a wearable device, a display inside a piece of traffic equipment, a holo-glass image displayed inside a piece of traffic equipment, a heads-up display, and a three-dimension projection.18. The method of claim 12, wherein the input data is further augmented on the visualization platform with one or more of flags, markers, icons, floating images, or colorized three-dimensional overlays.19. The method of claim 12, wherein the conditions in the transportation environment where traffic activity occurs include one or more of vehicle-related conditions, pavement-related conditions, and weather-related conditions.20. The method of claim 12, wherein the continually augmenting the input data further comprises creating a virtual reality visualization of the physical-world characteristic with which the user can interact using the visualization platform, wherein the augmented reality visualization is blended with the virtual reality visualization.21. The method of claim 12, further comprising generating a multi-period traffic performance map for the transportation environment encompassed by the physical-world characteristic.22. An augmented reality system for traffic applications, comprising: a computing environment including at least one computer-readable storage medium having program instructions stored therein and a computer processor operable to execute the program instructions to generate an augmented reality visualization of the physical-world characteristic, the plurality of data processing modules including: a data ingest module configured to ingest at least one of a video image, a video stream, or a map as input data representing a physical-world characteristic relative to a transportation environment that includes at least one of an intersection, a roadway, and conditions in the transportation environment where traffic activity occurs a visualization platform configured to display the input data and traffic for the physical-world characteristic, the visualization platform including a display apparatus that includes at least one of a mobile device, a wearable device, a display inside a piece of traffic equipment, a holo-glass image displayed inside a piece of traffic equipment, a heads-up display, and a three-dimension projection; one or more modules configured to continually augment the input data with the traffic data in the visualization platform, by configuring the input data on a visualization platform for display of the at least one a video image, a video stream, or a map; determining a geographical position of the visualization platform; identifying a manipulation of the visualization platform from one or more of a movement, tilt or a change in direction; requesting the traffic data for the physical-world characteristic based on the geographical position and the manipulation of the visualization platform; overlaying the traffic data onto the at least one a video image, a video stream, or a map of the physical-world characteristic to augment the input data with the traffic data on the visualization platform; and modifying the traffic data based on additional input generated by the manipulation of the visualization platform, an output module configured to initiate one or more responsive actions from the augmented reality visualization of the physical-world characteristic, the one or more responsive actions including at least one of modification of traffic data detection, and an adjustment of a traffic signal timing.23. The system of claim 22, wherein the manipulation of the visualization platform includes one or more of tactile, verbal, and orientational manipulation.24. The system of claim 23, wherein the one or more responsive actions from the continual augmented reality visualization of the physical-world characteristic are initiated by the manipulation of the visualization platform.25. The system of claim 22, wherein the traffic data includes one or more of traffic activity information, an identification of one or more vulnerable road users, environmental or situational areas of interest, points of interest in the physical-world characteristic, scouting reports, objects located in the transportation environment, sensor data, satellite data, vehicle data that includes one or more of a classification of a vehicle and a speed of a vehicle, equipment located within the transportation environment, and tracks of past movement of equipment in the transportation environment.26. The system of claim 22, wherein the input data is further augmented on the visualization platform with one or more of flags, markers, icons, floating images, or colorized three-dimensional overlays.27. The system of claim 22, wherein the conditions in the transportation environment where traffic activity occurs include one or more of vehicle-related conditions, pavement-related conditions, and weather-related conditions.28. The system of claim 22, further comprising creating a virtual reality visualization of the physical-world characteristic with which the user can interact using the visualization platform, wherein the augmented reality visualization is blended with the virtual reality visualization.29. The system of claim 22, wherein the output module is further configured to generate a multi-period traffic performance map for the transportation environment encompassed by the physical-world characteristic.",US2019257666_A1.txt,"G01C21/36,G06F3/0481,G06T19/00,H04W28/02","{'measuring; testing', 'electric communication technique', 'computing; calculating; counting'}","['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'electric digital data processing (computer systems based on specific computational models g06n)', 'image data processing or generation, in general', 'wireless communication networks (broadcast communication h04h; communication systems using wireless links for non-selective communication, e.g. wireless extensions h04m1/72)']","augmented reality system for visualization of traffic information in a transportation environment An augmented reality visualization framework for precision traffic analysis combines traffic data with content representative of physical-world characteristics, such as conditions in an intersection or roadway in which traffic activity occurs, and images of the intersection and roadway itself. The framework includes a visualization platform that enables display of the combined traffic data and content representing physical-world characteristics, and continual augmentations of such information in response to adjustments directed by users or automatically detected from actions such as user movement, other user manipulation, or movement of the visualization platform itself. The framework enables traffic applications such that a user can, when the platform is interfaced with intersection or roadway equipment, adjust machine activity or signal timing based upon the visualizations or data presented via the visualization platform. The visualization platform may also be configured to present the traffic data, and the content representative of physical-world characteristics, in a virtual reality setting.",measuring; testing electric communication technique computing; calculating; counting
US10065614_B2,2015-12-04,2018-09-04,2015-05-29,HYUNDAI MOTOR COMPANY,"HYUN, BA ROKIM, SUNG JAE",57397955,speed & trajectory,system and method for variably controlling regenerative braking,"A system and method for variably controlling regenerative braking are provided. The system includes an imaging device that is configured to generate image information by photographing a front vehicle and a road situation and an accelerometer that is configured to sense acceleration of the front vehicle or a subject vehicle. A radar sensor is configured to sense a distance between the front vehicle and the subject vehicle. A vehicle controller is configured to recognize the acceleration of the front vehicle or the subject vehicle, the distance between the front vehicle and the subject vehicle, and whether a brake light of the front vehicle is turned on, to determine whether to actively decelerate the subject vehicle, and execute active deceleration.","1. A system for variably controlling regenerative braking, comprising: an imaging device configured to generate image information by photographing a front vehicle and a road situation; an accelerometer configured to sense acceleration of the front vehicle or a subject vehicle; a radar sensor configured to sense a distance between the front vehicle and the subject vehicle; and a vehicle controller configured to recognize the acceleration of the front vehicle or the subject vehicle, the distance between the front vehicle and the subject vehicle, and whether a brake light of the front vehicle is turned on, to determine whether to actively decelerate the subject vehicle, and execute active deceleration, wherein a decision boundary setting method is used to determine whether to actively decelerate the subject vehicle, and execute active deceleration using the acceleration of the front vehicle or the subject vehicle, and the distance between the front vehicle and the subject vehicle, and wherein the vehicle controller is further configured to: a vehicle load model unit configured to calculate a current speed of the subject vehicle using the acceleration of the front vehicle of the subject vehicle received via the accelerometer; a coasting distance model unit configured to compare a costing enable distance calculated based on the current speed of the subject vehicle with the distance between the front vehicle and the subject vehicle; a coasting enable unit configured to determine whether coasting is controllable using information from the comparison between the coasting enable distance and the distance between the front vehicle and the subject vehicle; and a front vehicle model unit configured to determine whether the break light of the front vehicle is turned on to determine whether the front vehicle is decelerated, and determine whether to actively decelerate the subject vehicle.2. The system according to claim 1, wherein the coasting enable unit is connected to a clutch controller of the subject vehicle to determine whether to actively decelerate the subject vehicle.3. The system according to claim 1, wherein the front vehicle model unit is connected to a motor controller, a transmission controller, a brake controller, or an engine controller to determine whether to actively decelerate the subject vehicle.4. The system according to claim 1, wherein the front vehicle model unit includes a brake light tracking unit configured to track the brake light of the front vehicle from the image information obtained by the imaging device to determine whether the brake light is turned on.5. A method for variably controlling regenerative braking, comprising: calculating, by a controller, a current speed of a subject vehicle using acceleration information of a front vehicle or the subject vehicle received via an accelerometer; comparing, by the controller, a coasting enable distance calculated based on the current speed of the subject vehicle with a distance between the front vehicle and the subject vehicle; determining, by the controller, whether coasting is controllable using received information from the comparison between the coasting enable distance and the distance between the front vehicle and the subject vehicle; and determining, by the controller, whether a brake light of the front vehicle is turned on to determine whether the front vehicle is decelerated and determine whether to actively decelerate the subject vehicle, wherein a decision boundary setting method is used to determine whether to actively decelerate the subject vehicle, and execute active deceleration using the acceleration of the front vehicle or the subject vehicle, and the distance between the front vehicle and the subject vehicle.6. The method according to claim 5, wherein, in the comparison of the coasting enable distance calculated based on the current speed of the subject vehicle with the distance between the front vehicle and the subject vehicle, when the calculated coasting enable distance is equal to or greater than the distance between the front vehicle and the subject vehicle, the subject vehicle is informed of the coating enable distance.7. The method according to claim 5, wherein the determination of whether the coasting is controllable includes: comparing, by the controller, the coasting enable distance calculated based on the current speed of the subject vehicle, the distance between the front vehicle and the subject vehicle, and a preset safe distance between the front vehicle and the subject vehicle, and determining whether an accelerator pedal of the subject vehicle is in an ON or OFF state when the coasting enable distance is greater than the distance between the front vehicle and the subject vehicle by the safe distance.8. A non-transitory computer readable medium containing program instructions executed by a controller, the computer readable medium comprising: program instructions that calculate a current speed of a subject vehicle using acceleration information of a front vehicle or the subject vehicle received via an accelerometer; program instructions that compare a coasting enable distance calculated based on the current speed of the subject vehicle with a distance between the front vehicle and the subject vehicle; program instructions that determine whether coasting is controllable using received information from the comparison between the coasting enable distance and the distance between the front vehicle and the subject vehicle; and program instructions that determine whether a brake light of the front vehicle is turned on to determine whether the front vehicle is decelerated and determine whether to actively decelerate the subject vehicle, wherein a decision boundary setting method is used to determine whether to actively decelerate the subject vehicle, and execute active deceleration using the acceleration of the front vehicle or the subject vehicle, and the distance between the front vehicle and the subject vehicle.9. The non-transitory computer readable medium of claim 8, wherein, in the comparison of the coasting enable distance calculated based on the current speed of the subject vehicle with the distance between the front vehicle and the subject vehicle, when the calculated coasting enable distance is equal to or greater than the distance between the front vehicle and the subject vehicle, the subject vehicle is informed of the coating enable distance.10. The non-transitory computer readable medium of claim 8, further comprising: program instructions that compare the coasting enable distance calculated based on the current speed of the subject vehicle, the distance between the front vehicle and the subject vehicle, and a preset safe distance between the front vehicle and the subject vehicle, and determine whether an accelerator pedal of the subject vehicle is in an ON or OFF state when the coasting enable distance is greater than the distance between the front vehicle and the subject vehicle by the safe distance.",US10065614_B2.txt,"B60T1/10,B60T7/22,B60T8/171,F16D61/00","{'engineering elements and units; general measures for producing and maintaining effective functioning of machines or installations; thermal insulation in general', 'vehicles in general'}","['vehicle brake control systems or parts thereof; brake control systems or parts thereof, in general (electrodynamic brake systems for vehicle, in general b60l; brakes per se, i.e. devices where braking effect occurs, including ultimate brake actuators, f16d); arrangement of braking elements on vehicles in general; portable devices for preventing unwanted movement of vehicles; vehicle modifications to facilitate cooling of brakes', 'vehicle brake control systems or parts thereof; brake control systems or parts thereof, in general (electrodynamic brake systems for vehicle, in general b60l; brakes per se, i.e. devices where braking effect occurs, including ultimate brake actuators, f16d); arrangement of braking elements on vehicles in general; portable devices for preventing unwanted movement of vehicles; vehicle modifications to facilitate cooling of brakes', 'vehicle brake control systems or parts thereof; brake control systems or parts thereof, in general (electrodynamic brake systems for vehicle, in general b60l; brakes per se, i.e. devices where braking effect occurs, including ultimate brake actuators, f16d); arrangement of braking elements on vehicles in general; portable devices for preventing unwanted movement of vehicles; vehicle modifications to facilitate cooling of brakes', 'couplings for transmitting rotation; clutches; brakes']","system and method for variably controlling regenerative braking A system and method for variably controlling regenerative braking are provided. The system includes an imaging device that is configured to generate image information by photographing a front vehicle and a road situation and an accelerometer that is configured to sense acceleration of the front vehicle or a subject vehicle. A radar sensor is configured to sense a distance between the front vehicle and the subject vehicle. A vehicle controller is configured to recognize the acceleration of the front vehicle or the subject vehicle, the distance between the front vehicle and the subject vehicle, and whether a brake light of the front vehicle is turned on, to determine whether to actively decelerate the subject vehicle, and execute active deceleration.",engineering elements and units; general measures for producing and maintaining effective functioning of machines or installations; thermal insulation in general vehicles in general
US10643378_B2,2015-12-07,2020-05-05,2015-08-03,BAIDU ON-LINE NETWORK TECHNOLOGY (BEIJING) COMPANY,"YAN, YANGWANG, RUISUOJIA, XIANGFEI",54499198,road monitoring,"method and device for modelling three-dimensional road model, and storage medium","The present disclosure provides a method and device for modelling a three-dimensional road model, and a storage medium. The method comprises: parsing two-dimensional road network data to establish a rudimentary road model; parsing panoramic image data to obtain three-dimensional attribute data of a traffic element; and combining the rudimentary model and the three-dimensional attribute data to obtain a three-dimensional road model. The method, device and storage medium can economically and efficiently acquire a three-dimensional road model based on a relatively easily obtained data source.","1. A method for modelling a three-dimensional road model, comprising: parsing two-dimensional road network data to establish a two-dimensional rudimentary road model; parsing panoramic image data to obtain three-dimensional attribute data of a traffic element, wherein the three-dimensional attribute data comprises three-dimensional position coordinates; and combining the rudimentary model and the three-dimensional attribute data to obtain a three-dimensional road model; wherein the combining the rudimentary model and the three-dimensional attribute data to obtain a three-dimensional road model comprises: transforming two-dimensional position data in the rudimentary model into three-dimensional data by using the three-dimensional attribute data, by adding height data to each position point in the rudimentary model; correcting the two-dimensional position data in the rudimentary model by using position data in the three-dimensional attribute data; and performing a three-dimensional model rebuilding based on the three-dimensional data to obtain the three-dimensional road model.2. The method according to claim 1, wherein the parsing two-dimensional road network data to establish a rudimentary road model comprises: obtaining a road boundary line, a median divider boundary line, a road center line and a lane line by parsing the two-dimensional road network data.3. The method according to claim 1, wherein the parsing panoramic image data to obtain three-dimensional attribute data of a traffic element comprises: obtaining the traffic element in the panoramic image data by using a pre-trained deep neural network, the traffic element comprising a lane line, a traffic sign, an indicating line and a traffic light; and obtaining position information of the traffic element in the three-dimensional space by determining position information of a set of boundary points of the traffic element in the three-dimensional space.4. The method according to claim 1, wherein the method further comprises: converting the three-dimensional model data into a pre-set model data format for navigation after combining the rudimentary model and the three-dimensional attribute data to obtain a three-dimensional road model.5. A device for modelling a three-dimensional road model, the device comprising: at least one processor; and a memory storing instructions, the instructions when executed by the at least one processor, cause the at least one processor to perform operations, the operations comprising: parsing two-dimensional road network data to establish a two-dimensional rudimentary road model; parsing panoramic image data to obtain three-dimensional attribute data of a traffic element, wherein the three-dimensional attribute data comprises three-dimensional position coordinates; and combining the rudimentary model and the three-dimensional attribute data to obtain a three-dimensional road model; wherein the combining the rudimentary model and the three-dimensional attribute data to obtain a three-dimensional road model comprises: transforming two-dimensional position data in the rudimentary model into three-dimensional data by using the three-dimensional attribute data, by adding height data to each position point in the rudimentary model; correcting the two-dimensional position data in the rudimentary model by using position data in the three-dimensional attribute data; and performing a three-dimensional model rebuilding based on the three-dimensional data to obtain the three-dimensional road model.6. The device according to claim 5, wherein the parsing two-dimensional road network data to establish a rudimentary road model comprises: obtaining a road boundary line, a median divider boundary line, a road center line and a lane line by parsing the two-dimensional road network data.7. The device according to claim 5, wherein the parsing panoramic image data to obtain three-dimensional attribute data of a traffic element comprises: obtaining the traffic element in the panoramic image data by using a pre-trained deep neural network, the traffic element comprising a lane line, a traffic sign, an indicating line and a traffic light; and obtaining position information of the traffic element in the three-dimensional space by determining position information of a set of boundary points of the traffic element in the three-dimensional space.8. The device according to claim 5, wherein the operations further comprise: converting the three-dimensional model data into a pre-set model data format for navigation after combining the rudimentary model and the three-dimensional attribute data to obtain a three-dimensional road model.9. A non-transitory storage medium comprising computer-executable instructions executing a method for modelling a three-dimensional road model when executed by a computer processor, the method comprising: parsing two-dimensional road network data to establish a two-dimensional rudimentary road model; parsing panoramic image data to obtain three-dimensional attribute data of a traffic element, wherein the three-dimensional attribute data comprises three-dimensional position coordinates; and combining the rudimentary model and the three-dimensional attribute data to obtain a three-dimensional road model; wherein the combining the rudimentary model and the three-dimensional attribute data to obtain a three-dimensional road model comprises: transforming two-dimensional position data in the rudimentary model into three-dimensional data by using the three-dimensional attribute data, by adding height data to each position point in the rudimentary model; correcting the two-dimensional position data in the rudimentary model by using position data in the three-dimensional attribute data; and performing a three-dimensional model rebuilding based on the three-dimensional data to obtain the three-dimensional road model.",US10643378_B2.txt,"G01C11/02,G01C21/32,G01C21/36,G06F15/18,G06F16/29,G06F17/30,G06K9/00,G06K9/62,G06N20/00,G06N3/08,G06T17/05,G06T7/73","{'measuring; testing', 'computing; calculating; counting'}","['measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'measuring distances, levels or bearings; surveying; navigation; gyroscopic instruments; photogrammetry or videogrammetry (measuring liquid level g01f; radio navigation, determining distance or velocity by use of propagation effects, e.g. doppler effects, propagation time, of radio waves, analogous arrangements using other waves g01s)', 'electric digital data processing (computer systems based on specific computational models g06n)', 'electric digital data processing (computer systems based on specific computational models g06n)', 'electric digital data processing (computer systems based on specific computational models g06n)', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'computing arrangements based on specific computational models', 'image data processing or generation, in general', 'image data processing or generation, in general']","method and device for modelling three-dimensional road model, and storage medium The present disclosure provides a method and device for modelling a three-dimensional road model, and a storage medium. The method comprises: parsing two-dimensional road network data to establish a rudimentary road model; parsing panoramic image data to obtain three-dimensional attribute data of a traffic element; and combining the rudimentary model and the three-dimensional attribute data to obtain a three-dimensional road model. The method, device and storage medium can economically and efficiently acquire a three-dimensional road model based on a relatively easily obtained data source.",measuring; testing computing; calculating; counting
US2020394476_A1,2019-10-14,2020-12-17,2019-06-17,HYUNDAI MOTOR COMPANYKAIST (KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY)KIA MOTORS CORPORATION,"KIM, DO YEONKIM, JUN MOKIM, YANG SHINKIM, YOUNG HYUNJung, Hae ChangLee, Si HaengLee, Jang HyeonPark, Min Woo",73546934,road monitoring,apparatus and method for recognizing object using image,An apparatus for recognizing an object using an image includes a depth map generator that generates a depth map using a feature map of the image based on a dilated convolutional neural network (DCNN) and an object recognition device that recognizes the object using the depth map generated by the depth map generator and the image.,"1. An apparatus for recognizing an object using an image, the apparatus comprising: a depth map generator configured to generate a depth map using a feature map of the image based on a dilated convolutional neural network (DCNN); and an object recognition device configured to recognize the object using the depth map generated by the depth map generator and the image.2. The apparatus of claim 1, further comprising: an input device configured to input a feature map of a red-green-blue (RGB) image to the depth map generator and input the RGB image to the object recognition device.3. The apparatus of claim 2, wherein the input device includes: a first convolution module configured to generate a 16-channel feature map using the RGB image; a second convolution module configured to generate a 16-channel feature map using a gray image of the RGB image; and a concatenation module configured to generate a 32-channel feature map by concatenating the 16-channel feature map generated by the first convolution module and the 16-channel feature map generated by the second convolution module.4. The apparatus of claim 3, wherein each of the first convolution module and the second convolution module uses a 33 filter.5. The apparatus of claim 2, wherein the input device includes: a first convolution module configured to generate a 16-channel feature map using the RGB image; a second convolution module configured to generate an 8-channel feature map using a gray image of the RGB image; a third convolution module configured to generate an 8-channel feature map using a light detection and ranging (LiDAR) image; and a concatenation module configured to generate a 32-channel feature map by concatenating the 16-channel feature map generated by the first convolution module, the 8-channel feature map generated by the second convolution module, and the 8-channel feature map generated by the third convolution module.6. The apparatus of claim 5, wherein each of the first convolution module, the second convolution module, and the third convolution module uses a 33 filter.7. The apparatus of claim 1, wherein the depth map generator generates the depth map in a manner to gradually reduce resolution of the feature map and return the reduced resolution of the feature map.8. The apparatus of claim 7, wherein the depth map generator applies a dilation rate corresponding to the resolution of the feature map.9. The apparatus of claim 7, wherein the depth map generator reduces the resolution of the feature map by half.10. The apparatus of claim 7, wherein the depth map generator includes a plurality of concatenation modules, each of which concatenates feature maps of the same channel in a process of gradually reducing the resolution of the feature map and returning the reduced resolution of the feature map.11. A method for recognizing an object using an image, the method comprising: generating, by a depth map generator, a depth map using a feature map of the image based on a dilated convolutional neural network (DCNN); and recognizing, by an object recognition device, the object using the generated depth map and the image.12. The method of claim 11, further comprising: inputting, by an input device, a feature map of a red-green-blue (RGB) image to the depth map generator; and inputting, by the input device, the RGB image to the object recognition device.13. The method of claim 12, wherein the inputting includes: generating, by a first convolution module, a 16-channel feature map using the RGB image; generating, by a second convolution module, a 16-channel feature map using a gray image of the RGB image; and generating, by a concatenation module, a 32-channel feature map by concatenating the 16-channel feature map generated by the first convolution module and the 16-channel feature map generated by the second convolution module.14. The method of claim 13, wherein each of the first convolution module and the second convolution module uses a 33 filter.15. The method of claim 12, wherein the inputting includes: generating, by a first convolution module, a 16-channel feature map using the RGB image; generating, by a second convolution module, an 8-channel feature map using a gray image of the RGB image; generating, by a third convolution module, an 8-channel feature map using a light detection and ranging (LiDAR) image; and generating, by a concatenation module, a 32-channel feature map by concatenating the 16-channel feature map generated by the first convolution module, the 8-channel feature map generated by the second convolution module, and the 8-channel feature map generated by the third convolution module.16. The method of claim 15, wherein each of the first convolution module, the second convolution module, and the third convolution module uses a 33 filter.17. The method of claim 11, wherein the generating of the depth map includes: generating the depth map in a manner to gradually reduce resolution of the feature map and return the reduced resolution of the feature map.18. The method of claim 17, wherein the generating of the depth map includes: applying a dilation rate corresponding to the resolution of the feature map.19. The method of claim 17, wherein the generating of the depth map includes: reducing the resolution of the feature map by half.20. The method of claim 17, wherein the generating of the depth map includes: concatenating feature maps of the same channel in the process of gradually reducing the resolution of the feature map and returning the reduced resolution of the feature map.",US2020394476_A1.txt,"G06K9/54,G06K9/62,G06N3/04,G06T7/50",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'computing arrangements based on specific computational models', 'image data processing or generation, in general']",apparatus and method for recognizing object using image An apparatus for recognizing an object using an image includes a depth map generator that generates a depth map using a feature map of the image based on a dilated convolutional neural network (DCNN) and an object recognition device that recognizes the object using the depth map generated by the depth map generator and the image.,computing; calculating; counting
US2012230548_A1,2012-01-01,2012-09-13,2011-03-08,"BANK OF AMERICA CORPORATIONCALMAN, MATTHEW A.HAMILTON, ALFREDROSS, ERIK, STEPHEN","CALMAN, MATTHEW A.HAMILTON, ALFREDROSS, ERIK, STEPHEN",46795631,vehicle classification,vehicle recognition,"System, method, and computer program product are provided for using real-time video analysis to provide information about vehicles to a user. Through the user of real-time vision object recognition an image of a vehicle VIN number or a portion of a vehicle may be captured using an image capture device. The VIN number or the portion the vehicle that was captured via the real-time video analysis may be analyzed to determine information about the vehicle. The information may include information about the vehicle, such as the make, model, year, price, vehicle history, and the like. Furthermore, information about the individual's finances, such that an individual may know budgeting of purchasing a vehicle. The information about the vehicle and financial information about purchasing the vehicle is presented to the user.","1. A method for vehicle recognition, the method comprising: identifying an image of the at least a portion of a vehicle in proximate location to a mobile device of a user; receiving, via the mobile device, the image of the at least a portion of the vehicle; determining, via a computer device processor, information about the vehicle based on the image of the at least a portion of the vehicle; and presenting the information about the vehicle to a user, via the mobile device.2. The method of claim 1, wherein the information about the vehicle comprises information about the make, model, and price of the vehicle.3. The method of claim 1, wherein the information about the vehicle comprises information about the maintenance history of the vehicle.4. The method of claim 1, wherein the information about the vehicle further comprises information relating to user financial data.5. The method of claim 1, further comprising analyzing the user financial data compared to the information about the price of the vehicle to determine the user's ability to afford purchasing the vehicle.6. The method of claim 1, wherein identifying an image of the at least a portion of the vehicle further comprises capturing real-time video stream of the at least a portion of the vehicle.7. The method of claim 6, wherein presenting the information about the vehicle to the user comprises superimposing the information about the vehicle over the vehicle over real-time video that is capture by the mobile device.8. The method of claim 1, wherein identifying the image of the at least a portion of the vehicle further comprises capturing an image of the VIN number of the vehicle.9. The method of claim 1, wherein identifying the image of the at least a portion of the vehicle further comprises capturing an image of a portion of the exterior of the vehicle.10. The method of claim 1, wherein identifying the image of the at least a portion of the vehicle further comprises capturing an image of a portion of the interior of the vehicle.11. A system for vehicle recognition, the system comprising: a memory device; a communication device; and a processing device operatively coupled to the memory device and the communication device, wherein the processing device is configured to execute computer-readable program code to: identify an image of the at least a portion of a vehicle in proximate location to a mobile device of a user; receive, via the mobile device, the image of the at least a portion of the vehicle; determine information about the vehicle based on the image of the at least a portion of the vehicle; and present the information about the vehicle to a user, via the mobile device.12. The system of claim 11, wherein the information about the vehicle comprises information about the make, model, and price of the vehicle.13. The system of claim 11, wherein the information about the vehicle comprises information about the maintenance history of the vehicle.14. The system of claim 11, wherein the information about the vehicle further comprises information relating to user financial data.15. The system of claim 11, further comprising analyzing the user financial data compared to the information about the price of the vehicle to determine the user's ability to afford purchasing the vehicle.16. The system of claim 11, wherein identifying an image of the at least a portion of the vehicle further comprises capturing real-time video stream of the at least a portion of the vehicle.17. The system of claim 16, wherein presenting the information about the vehicle to the user comprises superimposing the information about the vehicle over the vehicle over real-time video that is capture by the mobile device.18. The system of claim 11, wherein identifying the image of the at least a portion of the vehicle further comprises capturing an image of the VIN number of the vehicle.19. The system of claim 11, wherein identifying the image of the at least a portion of the vehicle further comprises capturing an image of a portion of the exterior of the vehicle.20. The system of claim 11, wherein identifying the image of the at least a portion of the vehicle further comprises capturing an image of a portion of the interior of the vehicle.21. A computer program product for vehicle recognition, the computer program product comprising at least one non-transitory computer-readable medium having computer-readable program code portions embodied therein, the computer-readable program code portions comprising: An executable portion configured for identifying an image of the at least a portion of a vehicle in proximate location to a mobile device of a user; An executable portion configured for receiving, via the mobile device, the image of the at least a portion of the vehicle; An executable portion configured for determining information about the vehicle based on the image of the at least a portion of the vehicle; and An executable portion configured for presenting the information about the vehicle to a user, via the mobile device.22. The computer program product of claim 21, wherein the information about the vehicle comprises information about the make, model, and price of the vehicle.23. The computer program product of claim 21, wherein the information about the vehicle comprises information about the maintenance history of the vehicle.24. The computer program product of claim 21, wherein the information about the vehicle further comprises information relating to user financial data.25. The computer program product of claim 21, further comprising analyzing the user financial data compared to the information about the price of the vehicle to determine the user's ability to afford purchasing the vehicle.26. The computer program product of claim 21, wherein identifying an image of the at least a portion of the vehicle further comprises capturing real-time video stream of the at least a portion of the vehicle.27. The computer program product of claim 26, wherein presenting the information about the vehicle to the user comprises superimposing the information about the vehicle over the vehicle over real-time video that is capture by the mobile device.28. The computer program product of claim 21, wherein identifying the image of the at least a portion of the vehicle further comprises capturing an image of the VIN number of the vehicle.29. The computer program product of claim 21, wherein identifying the image of the at least a portion of the vehicle further comprises capturing an image of a portion of the exterior of the vehicle.30. The computer program product of claim 21, wherein identifying the image of the at least a portion of the vehicle further comprises capturing an image of a portion of the interior of the vehicle.",US2012230548_A1.txt,"G06K9/00,G06K9/62,G06Q30/06,G06Q40/02",{'computing; calculating; counting'},"['graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for', 'data processing systems or methods, specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes; systems or methods specially adapted for administrative, commercial, financial, managerial, supervisory or forecasting purposes, not otherwise provided for']","vehicle recognition System, method, and computer program product are provided for using real-time video analysis to provide information about vehicles to a user. Through the user of real-time vision object recognition an image of a vehicle VIN number or a portion of a vehicle may be captured using an image capture device. The VIN number or the portion the vehicle that was captured via the real-time video analysis may be analyzed to determine information about the vehicle. The information may include information about the vehicle, such as the make, model, year, price, vehicle history, and the like. Furthermore, information about the individual's finances, such that an individual may know budgeting of purchasing a vehicle. The information about the vehicle and financial information about purchasing the vehicle is presented to the user.",computing; calculating; counting
US2019232964_A1,2018-01-30,2019-08-01,2018-01-30,TOYOTA MOTOR ENGINEERING & MANUFACTURING NORTH AMERICA,"LINDHOLM, TRENTON B.",67224483,road monitoring,fusion of front vehicle sensor data for detection and ranging of preceding objects,"The present disclosure relates to a method for determining inter-vehicle distance during adaptive cruise control. Moreover, the method is directed to the integration of multiple sensing modalities to accurately determine inter-vehicle distance at close ranges. This approach exploits standard, vehicle-based sensors and processing circuitry to improve the selection of sensing modalities during inter-vehicle distance determination.","1. A method for determining a distance between a vehicle and a preceding object, comprising: acquiring, via a first sensing modality, a first sensed parameter related to the distance between the vehicle and the preceding object; determining, via a processing circuitry, a magnitude of the first sensed parameter acquired via the first sensing modality; acquiring, via a second sensing modality, a second sensed parameter related to the distance between the vehicle and the preceding object; determining, via a processing circuitry, a magnitude of the second sensed parameter acquired via the second sensing modality; and selecting the first sensed parameter, the second sensed parameter, or a combination thereof based upon the determination of the magnitude of the first sensed parameter and the magnitude of the second sensed parameter relative to pre-determined thresholds.2. The method according to claim 1, further comprising: acquiring, via camera, an image related to the preceding object; classifying the image related to the preceding object; and selecting the first sensed parameter, the second sensed parameter, or a combination thereof based upon the classification of the image related to the preceding object.3. The method according to claim 2, wherein the classifying comprises: classifying the image based upon a classifier, wherein the classifier is trained on a database, the database including a corpus of reference images, each reference image comprising an identifiable reference object and being associated in the database with a corresponding text description of the reference object, and a plurality of image query resolution data structures, each comprising a collection of records wherein each record includes an image descriptor of one of the reference images, and wherein each data structure is searchable using a corresponding one of a set of predetermined search processes to identify a closest match record within the data structure based upon the image descriptors.4. The method according to claim 1, wherein the first sensing modality or the second sensing modality is one of a plurality of sensing modalities including radar and sonar.5. The method according to claim 1, wherein the first sensed parameter or the second sensed parameter is one of a plurality of sensed parameters including radar reflective intensity and sonar resolution.6. The method according to claim 2, wherein the camera is a visible light camera, an infrared camera, or a combination thereof.7. The method according to claim 1, wherein the preceding object is a vehicle.8. A non-transitory computer-readable medium comprising executable program instructions which, when executed by a processing circuitry, cause the processing circuitry to implement a method comprising: acquiring, via a first sensing modality, a first sensed parameter related to the distance between the vehicle and the preceding object; determining a magnitude of the first sensed parameter acquired via the first sensing modality; acquiring, via a second sensing modality, a second sensed parameter related to the distance between the vehicle and the preceding object; determining a magnitude of the second sensed parameter acquired via the second sensing modality; and selecting the first sensed parameter, the second sensed parameter, or a combination thereof based upon the determination of the magnitude of the first sensed parameter and the magnitude of the second sensed parameter relative to pre-determined thresholds.9. The non-transitory computer-readable medium according to claim 8, wherein the method comprises: acquiring, via camera, an image related to the preceding object; classifying the image related to the preceding object; and selecting the first sensed parameter, the second sensed parameter, or a combination thereof based upon the classification of the image related to the preceding object.10. The non-transitory computer-readable medium according to claim 9, wherein the classifying comprises: classifying the image based upon a classifier, wherein the classifier is trained on a database, the database including a corpus of reference images, each reference image comprising an identifiable reference object and being associated in the database with a corresponding text description of the reference object, and a plurality of image query resolution data structures, each comprising a collection of records wherein each record includes an image descriptor of one of the reference images, and wherein each data structure is searchable using a corresponding one of a set of predetermined search processes to identify a closest match record within the data structure based upon the image descriptors.11. The non-transitory computer-readable medium according to claim 8, wherein the first sensing modality or the second sensing modality is one of a plurality of sensing modalities including radar and sonar.12. The non-transitory computer-readable medium according to claim 8, wherein the first sensed parameter or the second sensed parameter is one of a plurality of sensed parameters including radar reflective intensity and sonar resolution.13. The non-transitory computer-readable medium according to claim 9, wherein the camera is a visible light camera, an infrared camera, or a combination thereof.14. The non-transitory computer-readable medium according to claim 8, wherein the preceding object is a vehicle.15. An apparatus for determining a distance between a vehicle and a preceding object, comprising: processing circuitry configured to: acquire, via a first sensing modality, a first sensed parameter related to the distance between the vehicle and the preceding object; determine a magnitude of the first sensed parameter acquired via the first sensing modality; acquire, via a second sensing modality, a second sensed parameter related to the distance between the vehicle and the preceding object; determine a magnitude of the second sensed parameter acquired via the second sensing modality; and select the first sensed parameter, the second sensed parameter, or a combination thereof based upon the determination of the magnitude of the first sensed parameter and the magnitude of the second sensed parameter relative to pre-determined thresholds.16. The apparatus according to claim 15, wherein the processing circuitry is further configured to: acquire, via camera, an image related to the preceding object; classify the image related to the preceding object; and select the first sensed parameter, the second sensed parameter, or a combination thereof based upon the classification of the image related to the preceding object.17. The apparatus according to claim 16, wherein the processing circuitry is further configured to: classify the image based upon a classifier, wherein the classifier is trained on a database, the database including a corpus of reference images, each reference image comprising an identifiable reference object and being associated in the database with a corresponding text description of the reference object, and a plurality of image query resolution data structures, each comprising a collection of records wherein each record includes an image descriptor of one of the reference images, and wherein each data structure is searchable using a corresponding one of a set of predetermined search processes to identify a closest match record within the data structure based upon the image descriptors.18. The apparatus according to claim 15, wherein the first sensing modality or the second sensing modality is one of a plurality of sensing modalities including radar and sonar.19. The apparatus according to claim 15, wherein the first sensed parameter or the second sensed parameter is one of a plurality of sensed parameters including radar reflective intensity and sonar resolution.20. The apparatus according to claim 16, wherein the camera is a visible light camera, an infrared camera, or a combination thereof.",US2019232964_A1.txt,"B60W40/04,G06K9/00,G06K9/62","{'computing; calculating; counting', 'vehicles in general'}","['conjoint control of vehicle sub-units of different type or different function; control systems specially adapted for hybrid vehicles; road vehicle drive control systems for purposes not related to the control of a particular sub-unit', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers', 'graphical data reading (image or video recognition or understanding g06v); presentation of data; record carriers; handling record carriers']","fusion of front vehicle sensor data for detection and ranging of preceding objects The present disclosure relates to a method for determining inter-vehicle distance during adaptive cruise control. Moreover, the method is directed to the integration of multiple sensing modalities to accurately determine inter-vehicle distance at close ranges. This approach exploits standard, vehicle-based sensors and processing circuitry to improve the selection of sensing modalities during inter-vehicle distance determination.",computing; calculating; counting vehicles in general
KR102089230_B1,2019-12-30,2020-03-13,2019-12-30,AGILESODA,JUNG AHN JAEKIM TEA YOUNNGUYEN TUAN ANH,69938526,speed & trajectory,apparatus and method for predicting vehicle speed using image,"Disclosed are an apparatus and method for predicting the speed of a vehicle using an image. The present invention uses only one driving image of a black box installed in a vehicle, and in particular, can predict the speed of the vehicle using only the driving image from a first-person perspective. According to the present invention, the method for predicting the speed of a vehicle using an image comprises the steps of: a) receiving, by a speed prediction analysis unit (200), speed prediction target image data input from a prediction image input unit (100); b) converting, by the speed prediction analysis unit (200), the received speed prediction target image data to have a resolution of a predetermined size and adjusting the size; and c) extracting consecutive n image frames in an arbitrary time interval of the adjusted speed prediction target image data, extracting arbitrary features for prediction of speed through machine learning based on 3D convolutional networks (3DCNN) from the extracted n image frames, and predicting the speed of an (n+1)^th image frame from the output of a linear function based on the extracted features.","1. receiving velocity prediction target image data input from a prediction image input unit (100), adjusting image transformation and size to have a resolution of a predetermined size, extracting n consecutive image frames from an arbitrary time interval of the adjusted velocity prediction target image data, and a rate prediction analysis unit (200) that extracts arbitrary features for prediction of rate through machine learning based on 3 d convolutional networks (3dcnn) from the extracted n image frames, and predicts the rate of the (n+ 1) th image frame from an output of a linear function based on the extracted features, wherein the velocity prediction analysis unit (200) superimposes features for prediction of velocity with n/2 image frames among n image frames to next n/2 image frames, wherein the superimposed clips are transmitted as 3dcnn to generate a plurality of dimension image descriptors by using rectified linear units (relus) as an activation function, and wherein the superimposed clips are extracted through l2 normalization of the generated dimension image descriptors. 2. the speed prediction device for a vehicle using video according to claim 1, wherein the speed prediction target image data includes a lane indicated as a dotted line on a road. 3. the apparatus of claim 2, wherein the 3dcnn is based on a c3d regression model or a restnet 18 regression model. 4. the velocity prediction device according to claim 1, wherein the velocity prediction analysis unit (200) displays the predicted velocity value in the velocity prediction target image data and outputs it through the prediction image output unit (300). 5. a) an image processing method comprising the steps of: a) receiving, by a velocity prediction analysis unit (200), velocity prediction target image data input from a prediction image input unit (100); b) adjusting, by the velocity prediction analysis unit (200), the image conversion and size of the received velocity prediction target image data to have a predetermined size of resolution; and c) the speed prediction analysis unit (200) extracts n consecutive image frames from an arbitrary time interval of the adjusted speed prediction target image data, extracts arbitrary features for predicting speed from the extracted n image frames through machine learning based on 3 d convolutional networks (3dcnn), predicting a velocity of an (n+ 1) th image frame from an output of a linear function based on the extracted features, wherein in step c), features for predicting velocity overlap n/2 image frames among the n image frames with next n/2 image frames, wherein the superimposed clips are transferred to 3dcnn to generate a plurality of dimension image descriptors by using a rectified linear unit (relu) as an activation function, and are extracted through l2 normalization of the generated dimension image descriptors. 6. the method according to claim 5, further comprising: d) displaying, by the speed prediction analysis unit (200), the predicted speed value in the speed prediction target image data, and outputting the displayed speed value through a prediction image output unit (300). 7. the method according to claim 5, wherein, in the step b), the image frame of the received velocity prediction target image data is converted into rgb, the converted image frame is cut into a screen area of a predetermined size so as to have a resolution of a predetermined size, a method for predicting a speed of a vehicle using an image, the method comprising: applying a dummy screen to adjust a screen size; and readjusting the size to have a resolution of a predetermined size. 8. the method according to claim 5, wherein the 3dcnn is based on a c3d regression model or a restnet 18 regression model.",KR102089230_B1.txt,"G01P3/38,G06F17/10,G06T7/20","{'measuring; testing', 'computing; calculating; counting'}","['measuring linear or angular speed, acceleration, deceleration, or shock; indicating presence, absence, or direction, of movement (measuring or recording blood flow a61b5/02, a61b8/06; monitoring speed or deceleration of electrically-propelled vehicles b60l3/00; vehicle lighting systems adapted to indicate speed b60q1/54; determining position or course in navigation, measuring ground distance in geodesy or surveying g01c; combined measuring devices for measuring two or more variables of movement g01c23/00; measuring velocity of sound g01h; measuring velocity of light g01j7/00; measuring direction or velocity of solid objects by reception or emission of radiowaves or other waves and based on propagation effects, e.g. doppler effect, propagation time, direction of propagation, g01s; measuring speed of nuclear radiation g01t; measuring acceleration of gravity g01v; {measuring or recording the speed of trains b61l23/00; speed indicators incorporated in motor vehicles b60k35/00; measuring frequency or phase g01r; traffic control g08g})', 'electric digital data processing (computer systems based on specific computational models g06n)', 'image data processing or generation, in general']","apparatus and method for predicting vehicle speed using image Disclosed are an apparatus and method for predicting the speed of a vehicle using an image. The present invention uses only one driving image of a black box installed in a vehicle, and in particular, can predict the speed of the vehicle using only the driving image from a first-person perspective. According to the present invention, the method for predicting the speed of a vehicle using an image comprises the steps of: a) receiving, by a speed prediction analysis unit (200), speed prediction target image data input from a prediction image input unit (100); b) converting, by the speed prediction analysis unit (200), the received speed prediction target image data to have a resolution of a predetermined size and adjusting the size; and c) extracting consecutive n image frames in an arbitrary time interval of the adjusted speed prediction target image data, extracting arbitrary features for prediction of speed through machine learning based on 3D convolutional networks (3DCNN) from the extracted n image frames, and predicting the speed of an (n+1)^th image frame from the output of a linear function based on the extracted features.",measuring; testing computing; calculating; counting
